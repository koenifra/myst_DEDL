{"version":"1","records":[{"hierarchy":{"lvl1":"Destination Earth - ERA5 hourly data on single levels from 1940 to present - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level","position":0},{"hierarchy":{"lvl1":"Destination Earth - ERA5 hourly data on single levels from 1940 to present - Data Access using DEDL HDA"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT ","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level","position":1},{"hierarchy":{"lvl1":"Destination Earth - ERA5 hourly data on single levels from 1940 to present - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#destination-earth-era5-hourly-data-on-single-levels-from-1940-to-present-data-access-using-dedl-hda","position":2},{"hierarchy":{"lvl1":"Destination Earth - ERA5 hourly data on single levels from 1940 to present - Data Access using DEDL HDA"},"content":"Documentation DestinE Data Lake HDA\n\nDocumentation ERA5\n\nAuthor: EUMETSAT\n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example.\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#destination-earth-era5-hourly-data-on-single-levels-from-1940-to-present-data-access-using-dedl-hda","position":3},{"hierarchy":{"lvl1":"Obtain Authentication Token"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#obtain-authentication-token","position":4},{"hierarchy":{"lvl1":"Obtain Authentication Token"},"content":"\n\nimport requests\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nFirst, we get an access token for the API\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#obtain-authentication-token","position":5},{"hierarchy":{"lvl1":"Query using the DEDL HDA API"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#query-using-the-dedl-hda-api","position":6},{"hierarchy":{"lvl1":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#query-using-the-dedl-hda-api","position":7},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Filter"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#filter","position":8},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Filter"},"content":"We have to setup up a filter and define which data to obtain.\n\nFilter Options HDA\n\ndatechoice = \"2020-06-10T10:00:00Z\"\nfilters = {\n    key: {\"eq\": value}\n    for key, value in {\n        \"format\": \"grib\",\n        \"variable\": \"2m_temperature\",\n        \"time\": \"12:00\"\n    }.items()\n}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#filter","position":9},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Make Data Request"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#make-data-request","position":10},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Make Data Request"},"content":"\n\nresponse = requests.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n \"collections\": [\"EO.ECMWF.DAT.REANALYSIS_ERA5_SINGLE_LEVELS\"],\n    \"datetime\": datechoice,\n    \"query\": filters\n})\n\nif(response.status_code!= 200):\n    (print(response.text))\n# Requests to EO.ECMWF.DAT.REANALYSIS_ERA5_SINGLE_LEVELS always return a single item containing all the requested data\n#print(response.json())\nproduct = response.json()[\"features\"][0]\n#product id\nproduct[\"id\"]\n\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#make-data-request","position":11},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Once our product found, we download the data."},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#once-our-product-found-we-download-the-data","position":12},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Once our product found, we download the data."},"content":"\n\nfrom IPython.display import JSON\n\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = requests.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\nprint(download_url)\nresponse.raise_for_status()\n\nIf the data is already available in the cache we can directly download it.\nIf the data is not available, we can see that our request is in queuedstatus.We will then poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 second\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = requests.get(url, headers=auth_headers, stream=True)\n    return response\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = call_api(url,auth_headers)\n\nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n     (print(response.text))        \nresponse.raise_for_status()        \n\nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#once-our-product-found-we-download-the-data","position":13},{"hierarchy":{"lvl1":"EarthKit"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#earthkit","position":14},{"hierarchy":{"lvl1":"EarthKit"},"content":"Lets plot the result file\n[EarthKit Documentation] \n\nhttps://​earthkit​-data​.readthedocs​.io​/en​/latest​/index​.html\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\nimport earthkit.regrid\n\ndata = earthkit.data.from_source(\"file\", filename)\ndata.ls\nearthkit.maps.quickplot(data,#style=style\n                       )","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#earthkit","position":15},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial"},"type":"lvl1","url":"/climatedt-parameterplotter","position":0},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial"},"content":"\n\n","type":"content","url":"/climatedt-parameterplotter","position":1},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial"},"type":"lvl1","url":"/climatedt-parameterplotter#dedl-hda-climate-dt-parameter-plotter-tutorial","position":2},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT \nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example to access and plot Climate DT parameter.\n\nDocumentation DestinE DataLake HDA\n\nDocumentation Digital Twin - Parameter Usage\n\n","type":"content","url":"/climatedt-parameterplotter#dedl-hda-climate-dt-parameter-plotter-tutorial","position":3},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Import the required packages"},"type":"lvl2","url":"/climatedt-parameterplotter#import-the-required-packages","position":4},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Import the required packages"},"content":"\n\nImport the Climate DT parameter & scenario dictionary\n\nfrom destinelab import climate_dt_dictionary\nimport ipywidgets as widgets\nimport json\nimport datetime\n\nimport importlib.metadata\n\n","type":"content","url":"/climatedt-parameterplotter#import-the-required-packages","position":5},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Climate DT parameter selection (we limit the plotting to one parameter)"},"type":"lvl2","url":"/climatedt-parameterplotter#climate-dt-parameter-selection-we-limit-the-plotting-to-one-parameter","position":6},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Climate DT parameter selection (we limit the plotting to one parameter)"},"content":"\n\n# Create search box\nsearch_box = widgets.Text(placeholder='Search by parameter name', description='Search:', disabled=False)\n\n# Create dropdown to select entry\nentry_dropdown = widgets.Dropdown(\n    options=[(entry['paramName'], i) for i, entry in enumerate(climate_dt_dictionary.climateDT_params)],\n    description='Select Entry:'\n)\n\ndef filter_entries(search_string):\n    return [(entry['paramName'], i) for i, entry in enumerate(climate_dt_dictionary.climateDT_params) if search_string.lower() in entry['paramName'].lower()]\n\ndef on_search_change(change):\n    search_string = change.new\n    if search_string:\n        filtered_options = filter_entries(search_string)\n        entry_dropdown.options = filtered_options\n    else:\n        entry_dropdown.options = [(entry['paramName'], i) for i, entry in enumerate(climate_dt_dictionary.climateDT_params)]\n\nsearch_box.observe(on_search_change, names='value')\n\n# Display widgets\ndisplay(search_box, entry_dropdown)\n\ndef get_selected_entry():\n    return entry_dropdown.value\n\n\n# Print the details of the parameter (Polytope convention):\nselected_index = get_selected_entry()\nselected_entry = climate_dt_dictionary.climateDT_params[selected_index]\nprint(json.dumps(selected_entry,indent=4))\n\n","type":"content","url":"/climatedt-parameterplotter#climate-dt-parameter-selection-we-limit-the-plotting-to-one-parameter","position":7},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Choose now the Scenario from which we want to obtain the Climate Parameter"},"type":"lvl2","url":"/climatedt-parameterplotter#choose-now-the-scenario-from-which-we-want-to-obtain-the-climate-parameter","position":8},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Choose now the Scenario from which we want to obtain the Climate Parameter"},"content":"\n\n# Create dropdown to select scenario\nscenario_dropdown = widgets.Dropdown(\n    options=[(f\"{entry['experiment']} - {entry['model']} - {resolution}\", (i, resolution)) for i, entry in enumerate(climate_dt_dictionary.climateDT_scenario) for resolution in entry['resolution']],\n    description='Scenario:'\n)\n\n# Function to generate hourly slots\ndef generate_hourly_slots():\n    hours = []\n    for hour in range(0, 24):\n        for minute in range(0, 60, 60):  # Step by 60 minutes (1 hour)\n            hours.append(datetime.time(hour, minute))\n    return hours\n\n# Create dropdown to select hour\nhourly_slots = generate_hourly_slots()\nhour_dropdown = widgets.Dropdown(options=[(str(slot), slot) for slot in hourly_slots], description='Select Hour:', disabled=False)\n\n# Create date picker widgets\nstart_date_picker = widgets.DatePicker(description='Start Date:', disabled=False)\n\ndef on_scenario_change(change):\n    selected_index, selected_resolution = change.new\n    selected_sc_entry = climate_dt_dictionary.climateDT_scenario[selected_index]\n    date_from = datetime.datetime.strptime(selected_sc_entry['dateFrom'], '%m/%d/%Y').date()\n    start_date_picker.max = None\n    start_date_picker.min = date_from\n    start_date_picker.max = datetime.datetime.strptime(selected_sc_entry['dateTo'], '%m/%d/%Y').date()\n    start_date_picker.value = date_from\n\nscenario_dropdown.observe(on_scenario_change, names='value')\n\n# Set initial values directly\nselected_sc_entry = climate_dt_dictionary.climateDT_scenario[0]\n# Convert dateFrom string to date object\ndate_from = datetime.datetime.strptime(selected_sc_entry['dateFrom'], '%m/%d/%Y').date()\n\n# Set initial values directly\nstart_date_picker.min = date_from\nstart_date_picker.max = datetime.datetime.strptime(selected_sc_entry['dateTo'], '%m/%d/%Y').date()\nstart_date_picker.value = date_from\n\n# Display widgets\nif selected_entry[\"time\"] == \"Hourly\":\n    display(scenario_dropdown, start_date_picker, hour_dropdown)\nelse:\n    display(scenario_dropdown, start_date_picker)\n\ndef get_selected_values():\n    selected_scenario_index, selected_resolution = scenario_dropdown.value\n    selected_scenario = climate_dt_dictionary.climateDT_scenario[selected_scenario_index]\n    selected_start_date = start_date_picker.value\n    selected_end_date = \"\" # end_date_picker.value\n    selected_hour = \"00:00:00\"\n    if selected_entry[\"time\"] == \"Hourly\":\n        selected_hour = hour_dropdown.value\n        \n    return selected_scenario_index, selected_scenario, selected_resolution, selected_start_date, selected_end_date, selected_hour\n\n# Example usage:\nselected_scenario_index, selected_scenario, selected_resolution, selected_start_date, selected_end_date, selected_hour = get_selected_values()\n\n\nHandle different Levels to be selected (if any)\n\n# Define a global variable\nglobal global_widget\nglobal_widget = None\n\nif selected_entry[\"levelist\"] != \"\":\n    # Convert levelist string to list of integers\n    levelist = list(map(int, selected_entry[\"levelist\"].split('/')))\n    if(selected_scenario['model']=='IFS-NEMO'):\n        levelist = levelist + [73,74,75]\n\n      \n    # Create a function to generate the widget based on the selection mode\n    def generate_widget(selection_mode):\n        global global_widget\n        if selection_mode == 'Single':\n            global_widget = widgets.Dropdown(options=levelist, description='Select level:')\n            return global_widget\n        elif selection_mode == 'Multiple':\n            global_widget = widgets.SelectMultiple(options=levelist, description='Select levels:')\n            return global_widget\n\n    # Create a dropdown widget to choose selection mode\n    selection_mode_dropdown = widgets.Dropdown(options=['Single', 'Multiple'], description='Selection Mode:')\n\n    # Create an output widget to display the selected option(s)\n    output = widgets.Output()\n\n    # Function to display the widget based on the selection mode\n    def display_widget(selection_mode):\n        output.clear_output()\n        with output:\n            display(generate_widget(selection_mode))\n\n    # Define a function to handle the change in selection mode\n    def on_dropdown_change(change):\n        display_widget(change.new)\n\n    # Register the function to handle dropdown changes\n    selection_mode_dropdown.observe(on_dropdown_change, names='value')\n\n    # Display the widgets\n    display(selection_mode_dropdown, output)\n\n    # Display the initial widget based on default selection mode\n    display_widget('Single')\n\n# Function to convert tuple or single integer to string separated by \"/\"\ndef convert_to_string(input):\n    if isinstance(input, tuple):\n        return '/'.join(map(str, input))\n    elif isinstance(input, int):\n        return str(input)\n    else:\n        return None  # Handle other types if needed\n\nlevlInput = \"\"\nif global_widget != None:\n    # Test cases\n    levlInput = convert_to_string(global_widget.value)\n\n\nhourchoice4 = '{shour}00'.format(shour = str(get_selected_values()[5]).split(\":\")[0])\n\nfilter_params = {\n  \"class\": \"d1\",             # fixed \n  \"dataset\": \"climate-dt\",   # fixed climate-dt access\n  \"activity\" : get_selected_values()[1][\"activity\"],\n  \"experiment\" : get_selected_values()[1][\"experiment\"].upper(),\n  \"model\": get_selected_values()[1][\"model\"],\n  \"generation\": \"1\",         # fixed Specifies the generation of the dataset, which can be incremented as required (latest is 1)\n  \"realization\": \"1\",        # fixed Specifies the climate realization. Default 1. Based on perturbations of initial conditions\n  \"resolution\": get_selected_values()[2],      # standard/ high \n  \"expver\": \"0001\",          # fixed experiment version \n  \"stream\": selected_entry[\"stream\"],\n  \"time\": hourchoice4,            # choose the hourly slot(s)\n  \"type\": \"fc\",              # fixed forecasted fields\n  \"levtype\": selected_entry[\"levtype\"],  \n  \"levelist\": str(levlInput),  \n  \"param\": str(selected_entry[\"param\"]),  \n}\n\n# Print the result in JSON format\ndatechoice = \"{fname}T{shour}Z\".format(fname = get_selected_values()[3], shour = get_selected_values()[5] )\nprint(\"datechoice = \", datechoice)\nprint(json.dumps(filter_params, indent=4))\n\n\n","type":"content","url":"/climatedt-parameterplotter#choose-now-the-scenario-from-which-we-want-to-obtain-the-climate-parameter","position":9},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Obtain Authentication Token"},"type":"lvl2","url":"/climatedt-parameterplotter#obtain-authentication-token","position":10},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Obtain Authentication Token"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nDESP_USERNAME = input(\"Please input your DESP username: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/climatedt-parameterplotter#obtain-authentication-token","position":11},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"type":"lvl3","url":"/climatedt-parameterplotter#check-if-dt-access-is-granted","position":12},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/climatedt-parameterplotter#check-if-dt-access-is-granted","position":13},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Query using the DEDL HDA API"},"type":"lvl2","url":"/climatedt-parameterplotter#query-using-the-dedl-hda-api","position":14},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/climatedt-parameterplotter#query-using-the-dedl-hda-api","position":15},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/climatedt-parameterplotter#filter","position":16},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"content":"We have to setup up a filter and define which data to obtain.\n\n# Check if levelist is empty and remove it\nif filter_params.get(\"levelist\") == \"\":\n    del filter_params[\"levelist\"]\n\nif selected_entry[\"time\"] == \"Daily\":\n    del filter_params[\"time\"]\n\n    \nhdaFilters = {\n    key: {\"eq\": value}\n    for key, value in filter_params.items()\n}\n\n#print(hdaFilters)\n\n","type":"content","url":"/climatedt-parameterplotter#filter","position":17},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/climatedt-parameterplotter#make-data-request","position":18},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"content":"\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\n\nresponse = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n \"collections\": [\"EO.ECMWF.DAT.DT_CLIMATE_ADAPTATION\"],\n    \"datetime\": datechoice,\n    \"query\": hdaFilters\n})\n\nif(response.status_code!= 200):\n    (print(response.text))\nresponse.raise_for_status()\n# Requests to EO.ECMWF.DAT.DT_CLIMATE always return a single item containing all the requested data\nproduct = response.json()[\"features\"][0]\nproduct[\"id\"]\n\n","type":"content","url":"/climatedt-parameterplotter#make-data-request","position":19},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/climatedt-parameterplotter#submission-worked-once-our-product-found-we-download-the-data","position":20},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"content":"\n\nfrom IPython.display import JSON\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = session.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\nprint(download_url)\nresponse.raise_for_status()\n    \n\n","type":"content","url":"/climatedt-parameterplotter#submission-worked-once-our-product-found-we-download-the-data","position":21},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Wait until data is there"},"type":"lvl2","url":"/climatedt-parameterplotter#wait-until-data-is-there","position":22},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Wait until data is there"},"content":"This data is not available at the moment. And we can see that our request is in queuedstatus.We will now poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install --user ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 seconds\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = requests.get(url, headers=auth_headers, stream=True)\n    return response\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = call_api(url,auth_headers)\n\nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n     (print(response.text))\n\n# Check if Content-Disposition header is present\nif \"Content-Disposition\" not in response.headers:\n    print(response)\n    print(response.text)\n    raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n        \nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/climatedt-parameterplotter#wait-until-data-is-there","position":23},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"EarthKit"},"type":"lvl2","url":"/climatedt-parameterplotter#earthkit","position":24},{"hierarchy":{"lvl1":"DEDL - HDA Climate DT Parameter Plotter - Tutorial","lvl2":"EarthKit"},"content":"Lets plot the result file\n[EarthKit Documentation] \n\nhttps://​earthkit​-data​.readthedocs​.io​/en​/latest​/index​.html\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\nimport earthkit.regrid\n\ndata = earthkit.data.from_source(\"file\", filename)\ndata.ls\nearthkit.maps.quickplot(data)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action = \"ignore\", category = RuntimeWarning)","type":"content","url":"/climatedt-parameterplotter#earthkit","position":25},{"hierarchy":{"lvl1":"Destination Earth - Climate Change Adaptation Digital Twin Series Plot- Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series","position":0},{"hierarchy":{"lvl1":"Destination Earth - Climate Change Adaptation Digital Twin Series Plot- Data Access using DEDL HDA"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT ","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series","position":1},{"hierarchy":{"lvl1":"Destination Earth - Climate Change Adaptation Digital Twin Series Plot- Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#destination-earth-climate-change-adaptation-digital-twin-series-plot-data-access-using-dedl-hda","position":2},{"hierarchy":{"lvl1":"Destination Earth - Climate Change Adaptation Digital Twin Series Plot- Data Access using DEDL HDA"},"content":"Documentation DestinE Data Lake HDA\n\nClimate DT data catalogue\n\nAuthor: EUMETSAT\n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example.\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#destination-earth-climate-change-adaptation-digital-twin-series-plot-data-access-using-dedl-hda","position":3},{"hierarchy":{"lvl1":"Obtain Authentication Token"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#obtain-authentication-token","position":4},{"hierarchy":{"lvl1":"Obtain Authentication Token"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nFirst, we get an access token for the API\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#obtain-authentication-token","position":5},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl3":"Check if DT access is granted"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#check-if-dt-access-is-granted","position":6},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl3":"Check if DT access is granted"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#check-if-dt-access-is-granted","position":7},{"hierarchy":{"lvl1":"Query using the DEDL HDA API"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#query-using-the-dedl-hda-api","position":8},{"hierarchy":{"lvl1":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#query-using-the-dedl-hda-api","position":9},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Filter"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#filter","position":10},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Filter"},"content":"We have to setup up a filter and define which data to obtain.\n\nChoose a valid combination of => Activity + Experiment + Model (based on the year of interest)\n\nFollowing activities/experiment/model & dates are possible:\n\nScenarioMIP/ssp3-7.0/ICON: Start date 20200101, 40 years\nScenarioMIP/ssp3-7.0/IFS-NEMO: Start date 20200101, 40 years\n\ndatechoice = \"2028-06-10T00:00:00Z\"\nfilters = {\n    key: {\"eq\": value}\n    for key, value in {\n        \"class\": \"d1\",             # fixed \n        \"dataset\": \"climate-dt\",   # fixed climate-dt access\n        \"activity\": \"ScenarioMIP\", # activity + experiment + model (go together)\n        \"experiment\": \"SSP3-7.0\",  # activity + experiment + model (go together)\n        \"model\": \"IFS-NEMO\",       # activity + experiment + model (go together)\n        \"generation\": \"1\",         # fixed Specifies the generation of the dataset, which can be incremented as required (latest is 1)\n        \"realization\": \"1\",        # fixed Specifies the climate realization. Default 1. Based on perturbations of initial conditions\n        \"resolution\": \"high\",      # standard/ high \n        \"expver\": \"0001\",          # fixed experiment version \n        \"stream\": \"clte\",          # fixed climate\n        \"time\": \"0000\",            # choose the hourly slot(s)\n        \"type\": \"fc\",              # fixed forecasted fields\n        \"levtype\": \"sfc\",          # Surface fields (levtype=sfc), Height level fields (levtype=hl), Pressure level fields (levtype=pl), Model Level (Levtype=ml)\n#        \"levelist\": \"1/2/3/...\",  # for ml/pl/sol type data\n        \"param\": \"167\"             # 2m Temperature parameter\n    }.items()\n}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#filter","position":11},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Make Data Request"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#make-data-request","position":12},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Make Data Request"},"content":"We request data, it is not available at the moment. We can see that our request is in queuedstatus.We will poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom datetime import datetime\nfrom IPython.display import JSON\nfrom ratelimit import limits, sleep_and_retry\n\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\n###\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\n\n\n# Set limit: max 4 calls per 1 seconds\n###\nCALLS = 4\nPERIOD = 1  # seconds\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = session.get(url, headers=auth_headers, stream=True)\n    return response\n\n# Define date choice and filters if needed\ndatechoice = \"2024-07-01\"\n\n# Initialize a list to store filenames\nfilenames = []\n\n# Define start and end years\nstart_year = 2024\nstart_month = 7\nend_year = 2028\n\n# Loop \nfor year in range(start_year, end_year + 1):\n    # Create a datetime object \n    obsdate = datetime(year, start_month, 1)\n    datechoice = obsdate.strftime(\"%Y-%m-%dT12:00:00Z\")\n    response = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n        \"collections\": [\"EO.ECMWF.DAT.DT_CLIMATE_ADAPTATION\"],\n        \"datetime\": datechoice,\n        \"query\": filters\n    })\n\n    # Requests to EO.ECMWF.DAT.DT_CLIMATE always return a single item containing all the requested data\n    # print(response.json())\n    product = response.json()[\"features\"][0]\n\n    # DownloadLink is an asset representing the whole product\n    download_url = product[\"assets\"][\"downloadLink\"][\"href\"]\n    print(download_url)\n    HTTP_SUCCESS_CODE = 200\n    HTTP_ACCEPTED_CODE = 202\n\n    direct_download_url = ''\n\n    response =session.get(download_url, headers=auth_headers)\n    response.raise_for_status()\n    if (response.status_code == HTTP_SUCCESS_CODE):\n        direct_download_url = product['assets']['downloadLink']['href']\n    elif (response.status_code != HTTP_ACCEPTED_CODE):\n        JSON(response.json(), expanded=True)\n\n    # we poll as long as the data is not ready\n    if direct_download_url=='':\n        while url := response.headers.get(\"Location\"):\n            print(f\"order status: {response.json()['status']}\")\n            response = call_api(url,auth_headers)  \n            response.raise_for_status()\n            \n            \n    # Check if Content-Disposition header is present\n    if \"Content-Disposition\" not in response.headers:\n        print(response)\n        print(response.json())\n        raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n    filename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    print(f\"downloading {filename}\")\n\n    with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n        with open(filename, 'wb') as f:\n            for data in response.iter_content(1024):\n                progress_bar.update(len(data))\n                f.write(data)\n    \n    # Add the filename to the list\n    filenames.append(filename)\n\n\n    \n    \n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#make-data-request","position":13},{"hierarchy":{"lvl1":"EarthKit"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#earthkit","position":14},{"hierarchy":{"lvl1":"EarthKit"},"content":"Lets plot the result file\n[EarthKit Documentation] \n\nhttps://​earthkit​-data​.readthedocs​.io​/en​/latest​/index​.html\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\nimport earthkit.regrid\n\n# Iterate over filenames\nfor filename in filenames:\n    print(filename)  # For example, print each filename\n    data = earthkit.data.from_source(\"file\", filename)\n    data.ls\n    earthkit.maps.quickplot(data,#style=style\n                       )\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#earthkit","position":15},{"hierarchy":{"lvl1":"Destination Earth - Climate Change Adaptation Digital Twin - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate","position":0},{"hierarchy":{"lvl1":"Destination Earth - Climate Change Adaptation Digital Twin - Data Access using DEDL HDA"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT ","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate","position":1},{"hierarchy":{"lvl1":"Destination Earth - Climate Change Adaptation Digital Twin - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#destination-earth-climate-change-adaptation-digital-twin-data-access-using-dedl-hda","position":2},{"hierarchy":{"lvl1":"Destination Earth - Climate Change Adaptation Digital Twin - Data Access using DEDL HDA"},"content":"Documentation DestinE Data Lake HDA\n\nDocumentation Digital Twin - Parameter Usage\n\nAuthor: EUMETSAT\n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example.\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#destination-earth-climate-change-adaptation-digital-twin-data-access-using-dedl-hda","position":3},{"hierarchy":{"lvl1":"Obtain Authentication Token"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#obtain-authentication-token","position":4},{"hierarchy":{"lvl1":"Obtain Authentication Token"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nimport importlib.metadata\n\nFirst, we get an access token for the API\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#obtain-authentication-token","position":5},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl3":"Check if DT access is granted"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#check-if-dt-access-is-granted","position":6},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl3":"Check if DT access is granted"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#check-if-dt-access-is-granted","position":7},{"hierarchy":{"lvl1":"Query using the DEDL HDA API"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#query-using-the-dedl-hda-api","position":8},{"hierarchy":{"lvl1":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#query-using-the-dedl-hda-api","position":9},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Filter"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#filter","position":10},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Filter"},"content":"We have to setup up a filter and define which data to obtain.\n\nChoose a valid combination of => Activity + Experiment + Model (based on the year of interest)\n\nFollowing activities/experiment/model & dates are possible:\n\nScenarioMIP/ssp3-7.0/ICON: Start date 20200101, 40 years\nScenarioMIP/ssp3-7.0/IFS-NEMO: Start date 20200101, 40 years\n\ndatechoice = \"2028-06-10T00:00:00Z\"\nfilters = {\n    key: {\"eq\": value}\n    for key, value in {\n        \"class\": \"d1\",             # fixed \n        \"dataset\": \"climate-dt\",   # fixed climate-dt access\n        \"activity\": \"ScenarioMIP\", # activity + experiment + model (go together)\n        \"experiment\": \"SSP3-7.0\",  # activity + experiment + model (go together)\n        \"model\": \"IFS-NEMO\",       # activity + experiment + model (go together)\n        \"generation\": \"1\",         # fixed Specifies the generation of the dataset, which can be incremented as required (latest is 1)\n        \"realization\": \"1\",        # fixed Specifies the climate realization. Default 1. Based on perturbations of initial conditions\n        \"resolution\": \"high\",      # standard/ high \n        \"expver\": \"0001\",          # fixed experiment version \n        \"stream\": \"clte\",          # fixed climate\n        \"time\": \"0000\",            # choose the hourly slot(s)\n        \"type\": \"fc\",              # fixed forecasted fields\n        \"levtype\": \"sfc\",          # Surface fields (levtype=sfc), Height level fields (levtype=hl), Pressure level fields (levtype=pl), Model Level (Levtype=ml)\n#        \"levelist\": \"1/2/3/...\",  # for ml/pl/sol type data\n        \"param\": \"134\"             # Surface Pressure parameter\n    }.items()\n}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#filter","position":11},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Make Data Request"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#make-data-request","position":12},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Make Data Request"},"content":"\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\n\nresponse = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n \"collections\": [\"EO.ECMWF.DAT.DT_CLIMATE_ADAPTATION\"],\n    \"datetime\": datechoice,\n    \"query\": filters\n})\n\nif(response.status_code!= 200):\n    (print(response.text))\nresponse.raise_for_status()\n\n# Requests to EO.ECMWF.DAT.DT_CLIMATE always return a single item containing all the requested data\nproduct = response.json()[\"features\"][0]\nproduct[\"id\"]\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#make-data-request","position":13},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Submission worked ? Once our product found, we download the data."},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#submission-worked-once-our-product-found-we-download-the-data","position":14},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Submission worked ? Once our product found, we download the data."},"content":"\n\nfrom IPython.display import JSON\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = session.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\nprint(download_url)\nresponse.raise_for_status()    \n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#submission-worked-once-our-product-found-we-download-the-data","position":15},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Wait until data is there"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#wait-until-data-is-there","position":16},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Wait until data is there"},"content":"This data is not available at the moment. And we can see that our request is in queuedstatus.We will now poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 seconds\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = session.get(url, headers=auth_headers, stream=True)\n    return response\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = call_api(url,auth_headers)  \n        \nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n     (print(response.text))\n\n# Check if Content-Disposition header is present\nif \"Content-Disposition\" not in response.headers:\n    print(response)\n    print(response.text)\n    raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n        \nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#wait-until-data-is-there","position":17},{"hierarchy":{"lvl1":"EarthKit"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#earthkit","position":18},{"hierarchy":{"lvl1":"EarthKit"},"content":"Lets plot the result file\n[EarthKit Documentation] \n\nhttps://​earthkit​-data​.readthedocs​.io​/en​/latest​/index​.html\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\nimport earthkit.regrid\n\ndata = earthkit.data.from_source(\"file\", filename)\ndata.ls\nearthkit.maps.quickplot(data,#style=style\n                       )","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#earthkit","position":19},{"hierarchy":{"lvl1":"Destination Earth - Weather-Induced Extremes Digital Twin - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series","position":0},{"hierarchy":{"lvl1":"Destination Earth - Weather-Induced Extremes Digital Twin - Data Access using DEDL HDA"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT ","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series","position":1},{"hierarchy":{"lvl1":"Destination Earth - Weather-Induced Extremes Digital Twin - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#destination-earth-weather-induced-extremes-digital-twin-data-access-using-dedl-hda","position":2},{"hierarchy":{"lvl1":"Destination Earth - Weather-Induced Extremes Digital Twin - Data Access using DEDL HDA"},"content":"Documentation DestinE Data Lake HDA\n\nDocumentation Digital Twin - Parameter Usage\n\n Author: EUMETSAT \n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example.\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#destination-earth-weather-induced-extremes-digital-twin-data-access-using-dedl-hda","position":3},{"hierarchy":{"lvl1":"Obtain Authentication Token"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#obtain-authentication-token","position":4},{"hierarchy":{"lvl1":"Obtain Authentication Token"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport ipywidgets as widgets\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\nfrom datetime import datetime, timedelta\n\nFirst, we get an access token for the API\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#obtain-authentication-token","position":5},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl3":"Check if DT access is granted"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#check-if-dt-access-is-granted","position":6},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl3":"Check if DT access is granted"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#check-if-dt-access-is-granted","position":7},{"hierarchy":{"lvl1":"Query using the DEDL HDA API"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#query-using-the-dedl-hda-api","position":8},{"hierarchy":{"lvl1":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#query-using-the-dedl-hda-api","position":9},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Filter"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#filter","position":10},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Filter"},"content":"We have to setup up a filter and define which data to obtain.\n\nExtreme DT data is available for specific time ranges (last 14 days) around the current date.\n\nIt is possible to use the ECMWF Aviso package to check data availability in the last 14 days (see \n\nhttps://github.com/destination-earth/DestinE-DataLake-Lab/blob/main/HDA/DestinE Digital Twins/ExtremeDT-dataAvailability.ipynb or \n\nextremes​-dt​/extremes​-dt​.py) and request data accordingly.\n\nfrom datetime import datetime, timedelta\n\n# Get the current date and time in UTC\ncurrent_date = datetime.utcnow()\n\n# Calculate the date 15 days before the current date\ndate_14_days_ago = current_date - timedelta(days=14)\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = date_14_days_ago.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_from = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = current_date.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_to = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\nstart_date_picker = widgets.DatePicker(description='Start Date:', disabled=False)\n\n# Set initial values directly\nstart_date_picker.min = date_from\nstart_date_picker.max = date_to\nstart_date_picker.value = date_from\n\ndef get_selected_values():\n    selected_start_date = start_date_picker.value\n    return selected_start_date\n\n# Display widgets\ndisplay(start_date_picker)\n\n\n# Example usage:\ndatechoice = get_selected_values().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n#print(datechoice)\nfilters = {\n    key: {\"eq\": value}\n    for key, value in {\n        \"class\": \"d1\",             # fixed (rd or d1)\n        \"dataset\": \"extremes-dt\",  # fixed extreme dt \n        \"expver\": \"0001\",          # fixed experiment version \n        \"stream\": \"oper\",          # fixed oper\n        \"step\": \"0/6/12/18/24\",    # Forcast step hourly (1..96)\n        \"type\": \"fc\",              # fixed forecasted fields\n        \"levtype\": \"sfc\",          # Surface fields (levtype=sfc), Height level fields (levtype=hl), Pressure level fields (levtype=pl), Model Level (Levtype=ml)\n        \"param\": \"151/228029\"      # 10m Wind gust & Mean Sea Level Pressure\n    }.items()\n}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#filter","position":11},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Make Data Request"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#make-data-request","position":12},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Make Data Request"},"content":"\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\n\nresponse = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n    \"collections\": [\"EO.ECMWF.DAT.DT_EXTREMES\"],\n    \"datetime\": datechoice,\n    \"query\": filters\n})\n\n# Requests to EO.ECMWF.DAT.DT_EXTREMES always return a single item containing all the requested data\nif(response.status_code!= 200):\n    (print(response.text))\nresponse.raise_for_status()\n\nproduct = response.json()[\"features\"][0]\nproduct[\"id\"]\n#product\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#make-data-request","position":13},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Submission worked ? Once our product found, we download the data."},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#submission-worked-once-our-product-found-we-download-the-data","position":14},{"hierarchy":{"lvl1":"Query using the DEDL HDA API","lvl2":"Submission worked ? Once our product found, we download the data."},"content":"\n\nfrom IPython.display import JSON\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\n\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = session.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\n\nresponse.raise_for_status()    \nprint(download_url)   \n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#submission-worked-once-our-product-found-we-download-the-data","position":15},{"hierarchy":{"lvl1":"Wait until data is there"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#wait-until-data-is-there","position":16},{"hierarchy":{"lvl1":"Wait until data is there"},"content":"This data is not available at the moment. And we can see that our request is in queuedstatus.We will now poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 seconds\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = session.get(url, headers=auth_headers, stream=True)\n    return response\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        print(f\"order status: {response.text}\")\n        response = call_api(url,auth_headers)  \n        \nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n    (print(response.text))\n\n        \n# Check if Content-Disposition header is present\nif \"Content-Disposition\" not in response.headers:\n    print(response)\n    print(response.text)\n    raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n        \nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#wait-until-data-is-there","position":17},{"hierarchy":{"lvl1":"Render the sea ice coverage on a map"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#render-the-sea-ice-coverage-on-a-map","position":18},{"hierarchy":{"lvl1":"Render the sea ice coverage on a map"},"content":"Lets plot the result file\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\n\ndata = earthkit.data.from_source(\"file\", filename)\ndata.ls()\n\nimport matplotlib.pyplot as plt\n\nchart = earthkit.maps.Chart(domain=[-5, 23, 40, 58], rows=3, columns=4)\n\ngust_style = earthkit.maps.Style(\n    colors=[\"#85AAEE\", \"#208EFC\", \"#6CA632\", \"#FFB000\", \"#FF0000\", \"#7A11B1\"],\n    levels=[12, 15, 20, 25, 30, 35, 50],\n    units=\"m s-1\",\n)\n\nchart.add_subplot(row=0, column=3)\nfor i in range(4):\n    chart.add_subplot(row=1+i//4, column=i%4)\n\nchart.plot(data.sel(shortName=\"i10fg\"), style=gust_style)\nchart.plot(data.sel(shortName=\"msl\"), units=\"hPa\")\n\nchart.land(color=\"lightgrey\")\nchart.coastlines()\n\nax = plt.axes((0.05, 0.8, 0.65, 0.025))\nchart.legend(ax=ax)\n\nchart.subplot_titles(\"{time:%Y-%m-%d %H} UTC (+{lead_time}h)\")\nchart.title(\n    \"ECMWF HRES Run: {base_time:%Y-%m-%d %H} UTC\\n{variable_name}\",\n    fontsize=15, horizontalalignment=\"left\", x=0, y=0.96,\n)\n\nchart.show()","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#render-the-sea-ice-coverage-on-a-map","position":19},{"hierarchy":{"lvl1":"Destination Earth - Weather-Induced Extremes Digital Twin - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes","position":0},{"hierarchy":{"lvl1":"Destination Earth - Weather-Induced Extremes Digital Twin - Data Access using DEDL HDA"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT ","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes","position":1},{"hierarchy":{"lvl1":"Destination Earth - Weather-Induced Extremes Digital Twin - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#destination-earth-weather-induced-extremes-digital-twin-data-access-using-dedl-hda","position":2},{"hierarchy":{"lvl1":"Destination Earth - Weather-Induced Extremes Digital Twin - Data Access using DEDL HDA"},"content":"Documentation DestinE Data Lake HDA\n\nDocumentation Digital Twin - Parameter Usage\n\nAuthor: EUMETSAT\n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example.\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#destination-earth-weather-induced-extremes-digital-twin-data-access-using-dedl-hda","position":3},{"hierarchy":{"lvl1":"Obtain Authentication Token"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#obtain-authentication-token","position":4},{"hierarchy":{"lvl1":"Obtain Authentication Token"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport ipywidgets as widgets\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#obtain-authentication-token","position":5},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl3":"Check if DT access is granted"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#check-if-dt-access-is-granted","position":6},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl3":"Check if DT access is granted"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#check-if-dt-access-is-granted","position":7},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl2":"Query using the DEDL HDA API"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#query-using-the-dedl-hda-api","position":8},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl2":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#query-using-the-dedl-hda-api","position":9},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl2":"Filter"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#filter","position":10},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl2":"Filter"},"content":"We have to setup up a filter and define which data to obtain.\n\nExtreme DT data is available for specific time ranges (last 14 days) around the current date.\n\nIt is possible to use the ECMWF Aviso package to check data availability in the last 14 days (see \n\nhttps://github.com/destination-earth/DestinE-DataLake-Lab/blob/main/HDA/DestinE Digital Twins/ExtremeDT-dataAvailability.ipynb or \n\nextremes​-dt​/extremes​-dt​.py) and request data accordingly.\n\nfrom datetime import datetime, timedelta\n\n# Get the current date and time in UTC\ncurrent_date = datetime.utcnow()\n\n# Calculate the date 14 days before the current date\ndate_14_days_ago = current_date - timedelta(days=14)\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = date_14_days_ago.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_from = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = current_date.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_to = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\n\n\nstart_date_picker = widgets.DatePicker(description='Start Date:', disabled=False)\n\n# Set initial values directly\nstart_date_picker.min = date_from\nstart_date_picker.max = date_to\nstart_date_picker.value = date_from\n\ndef get_selected_values():\n    selected_start_date = start_date_picker.value\n    return selected_start_date\n\n# Display widgets\ndisplay(start_date_picker)\n\ndatechoice =  get_selected_values().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\nfilters = {\n    key: {\"eq\": value}\n    for key, value in {\n        \"class\": \"d1\",             # fixed (rd or d1)\n        \"dataset\": \"extremes-dt\",  # fixed extreme dt \n        \"expver\": \"0001\",          # fixed experiment version \n        \"stream\": \"oper\",          # fixed oper\n        \"step\": \"0\",               # Forcast step hourly (1..96)\n        \"type\": \"fc\",              # fixed forecasted fields\n        \"levtype\": \"sfc\",          # Surface fields (levtype=sfc), Height level fields (levtype=hl), Pressure level fields (levtype=pl), Model Level (Levtype=ml)\n        \"param\": \"31\"             # Sea ice area fraction\n    }.items()\n}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#filter","position":11},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl2":"Make Data Request"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#make-data-request","position":12},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl2":"Make Data Request"},"content":"\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\nresponse = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n    \"collections\": [\"EO.ECMWF.DAT.DT_EXTREMES\"],\n    \"datetime\": datechoice,\n    \"query\": filters\n})\n\n# Requests to EO.ECMWF.DAT.DT_EXTREMES always return a single item containing all the requested data\nif(response.status_code!= 200):\n    (print(response.text))\nresponse.raise_for_status()\n\nproduct = response.json()[\"features\"][0]\nproduct[\"id\"]\n#product\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#make-data-request","position":13},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl2":"Submission worked ? Once our product found, we download the data."},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#submission-worked-once-our-product-found-we-download-the-data","position":14},{"hierarchy":{"lvl1":"Obtain Authentication Token","lvl2":"Submission worked ? Once our product found, we download the data."},"content":"\n\nfrom IPython.display import JSON\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\n\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = session.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\n\nresponse.raise_for_status()    \nprint(download_url)  \n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#submission-worked-once-our-product-found-we-download-the-data","position":15},{"hierarchy":{"lvl1":"Wait until data is there"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#wait-until-data-is-there","position":16},{"hierarchy":{"lvl1":"Wait until data is there"},"content":"This data is not available at the moment. And we can see that our request is in queuedstatus.We will now poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 seconds\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = session.get(url, headers=auth_headers, stream=True)\n    return response\n\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = call_api(url,auth_headers)\n\nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n     (print(response.text))\n        \n# Check if Content-Disposition header is present\nif \"Content-Disposition\" not in response.headers:\n    print(response)\n    print(response.text)\n    raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n        \nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#wait-until-data-is-there","position":17},{"hierarchy":{"lvl1":"Render the sea ice coverage on a map"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#render-the-sea-ice-coverage-on-a-map","position":18},{"hierarchy":{"lvl1":"Render the sea ice coverage on a map"},"content":"Lets plot the result file\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport xarray as xr\nimport cfgrib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nds = xr.load_dataset(filename, engine=\"cfgrib\")\n\nds\n\nimport cartopy.crs as crs\nimport cartopy.feature as cfeature\n\nfig = plt.figure(figsize=[10, 10])\n\n#ax = fig.add_subplot(1,1,1, projection=crs.Robinson())\ncrs_epsg=crs.NorthPolarStereo(central_longitude=0)\nax = fig.add_subplot(1,1,1, projection=crs_epsg)\n\nax.set_extent([-3850000.0, 3750000.0, -5350000, 5850000.0],crs_epsg)\n\nax.add_feature(cfeature.COASTLINE)\nax.gridlines()\n\ncs = plt.scatter(x=ds.longitude[::10], y=ds.latitude.data[::10], c=ds.siconc[::10], cmap=\"Blues\",\n            s=1,\n            transform=crs.PlateCarree())\n\nfig.colorbar(cs, ax=ax, location='right', shrink =0.8)\nplt.show()","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#render-the-sea-ice-coverage-on-a-map","position":19},{"hierarchy":{"lvl1":"DEDL - HDA Extreme DT Parameter Plotter - Tutorial"},"type":"lvl1","url":"/extremedt-parameterplotter","position":0},{"hierarchy":{"lvl1":"DEDL - HDA Extreme DT Parameter Plotter - Tutorial"},"content":"\n\n","type":"content","url":"/extremedt-parameterplotter","position":1},{"hierarchy":{"lvl1":"DEDL - HDA Extreme DT Parameter Plotter - Tutorial"},"type":"lvl1","url":"/extremedt-parameterplotter#dedl-hda-extreme-dt-parameter-plotter-tutorial","position":2},{"hierarchy":{"lvl1":"DEDL - HDA Extreme DT Parameter Plotter - Tutorial"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT \n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example to access and plot Extreme DT parameter.\n\nDocumentation DestinE DataLake HDA\n\nDocumentation Digital Twin - Parameter Usage\n\n","type":"content","url":"/extremedt-parameterplotter#dedl-hda-extreme-dt-parameter-plotter-tutorial","position":3},{"hierarchy":{"lvl1":"DEDL - HDA Extreme DT Parameter Plotter - Tutorial","lvl2":"Import the required packages"},"type":"lvl2","url":"/extremedt-parameterplotter#import-the-required-packages","position":4},{"hierarchy":{"lvl1":"DEDL - HDA Extreme DT Parameter Plotter - Tutorial","lvl2":"Import the required packages"},"content":"\n\nImport the Climate DT parameter & scenario dictionary\n\nfrom destinelab import extreme_dt_dictionary\nimport ipywidgets as widgets\nimport json\nfrom datetime import datetime, timedelta\n\n","type":"content","url":"/extremedt-parameterplotter#import-the-required-packages","position":5},{"hierarchy":{"lvl1":"DEDL - HDA Extreme DT Parameter Plotter - Tutorial","lvl2":"Extreme DT parameter selection (we limit the plotting to one parameter)"},"type":"lvl2","url":"/extremedt-parameterplotter#extreme-dt-parameter-selection-we-limit-the-plotting-to-one-parameter","position":6},{"hierarchy":{"lvl1":"DEDL - HDA Extreme DT Parameter Plotter - Tutorial","lvl2":"Extreme DT parameter selection (we limit the plotting to one parameter)"},"content":"\n\n# Create search box\nsearch_box = widgets.Text(placeholder='Search by parameter name', description='Search:', disabled=False)\n\n# Create dropdown to select entry\nentry_dropdown = widgets.Dropdown(\n    options=[(entry['paramName'], i) for i, entry in enumerate(extreme_dt_dictionary.extremeDT_params)],\n    description='Select Entry:'\n)\n\ndef filter_entries(search_string):\n    return [(entry['paramName'], i) for i, entry in enumerate(extreme_dt_dictionary.extremeDT_params) if search_string.lower() in entry['paramName'].lower()]\n\ndef on_search_change(change):\n    search_string = change.new\n    if search_string:\n        filtered_options = filter_entries(search_string)\n        entry_dropdown.options = filtered_options\n    else:\n        entry_dropdown.options = [(entry['paramName'], i) for i, entry in enumerate(extreme_dt_dictionary.extremeDT_params)]\n\nsearch_box.observe(on_search_change, names='value')\n\n# Display widgets\ndisplay(search_box, entry_dropdown)\n\ndef get_selected_entry():\n    return entry_dropdown.value\n\n\n","type":"content","url":"/extremedt-parameterplotter#extreme-dt-parameter-selection-we-limit-the-plotting-to-one-parameter","position":7},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):"},"type":"lvl1","url":"/extremedt-parameterplotter#print-the-details-of-the-parameter-polytope-convention","position":8},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):"},"content":"\n\n\nselected_index = get_selected_entry()\nselected_entry = extreme_dt_dictionary.extremeDT_params[selected_index]\nprint(json.dumps(selected_entry,indent=4))\n\n","type":"content","url":"/extremedt-parameterplotter#print-the-details-of-the-parameter-polytope-convention","position":9},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"Select the Date of Observation to be selected"},"type":"lvl2","url":"/extremedt-parameterplotter#select-the-date-of-observation-to-be-selected","position":10},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"Select the Date of Observation to be selected"},"content":"\n\nExtremes DT data that is older than 15 days is archived, below it is then possible to choose only dates in the last 15 days.\n\nTo choose your date, please consider also the current Extremes DT data availability. Extremes DT data availability can be find out using the ExtremeDT-dataAvailability.ipynb in this folder.\n\nfrom datetime import datetime, timedelta\n\n# Get the current date and time in UTC\ncurrent_date = datetime.utcnow()\n\n# Calculate the date 15 days before the current date\ndate_15_days_ago = current_date - timedelta(days=15)\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = date_15_days_ago.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_from = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = current_date.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_to = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\nfrom ipywidgets import Label\n# Create dropdown to select scenario\nscenario_dropdown = widgets.Dropdown(\n    options=[(f\"{entry['model']}\", (i)) for i, entry in enumerate(extreme_dt_dictionary.extremeDT_scenario)],\n    description='Scenario:'\n)\n\n# Create date picker widgets\nstart_date_picker = widgets.DatePicker(description='Start Date:', disabled=False)\n\ndef on_scenario_change(change):\n    print(\"scenario_change\")\n    selected_index = change.new\n    selected_entry = extreme_dt_dictionary.extremeDT_scenario[selected_index]\n    start_date_picker.min = date_from\n    start_date_picker.max = date_to\n    # Set the initial date of the start_date_picker to the scenario's start date\n    start_date_picker.value = date_to\n    selected_start_date = start_date_picker.value\n    \nscenario_dropdown.observe(on_scenario_change, names='value')\n\n# Set initial values directly\nselected_entry = extreme_dt_dictionary.extremeDT_scenario[0]\n\n# Set initial values directly\nstart_date_picker.min = date_from\nstart_date_picker.max = date_to\nstart_date_picker.value = date_to\n\n# Display widgets\n\ndef get_selected_values():\n    selected_scenario_index = scenario_dropdown.value\n    selected_scenario = extreme_dt_dictionary.extremeDT_scenario[selected_scenario_index]\n    selected_start_date = start_date_picker.value\n    return selected_scenario_index, selected_scenario, selected_start_date\n\n# Display widgets\ndisplay(Label(\"To choose your date, please consider the current Extremes DT data availability (ExtremeDT-dataAvailability.ipynb in this folder).\"),scenario_dropdown,  start_date_picker)\n\n# Example usage:\nselected_scenario_index, selected_scenario, selected_start_date = get_selected_values()\n\n\n","type":"content","url":"/extremedt-parameterplotter#select-the-date-of-observation-to-be-selected","position":11},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"Choose now the Steps within the observation to be retrieved (one step usually one hour)"},"type":"lvl2","url":"/extremedt-parameterplotter#choose-now-the-steps-within-the-observation-to-be-retrieved-one-step-usually-one-hour","position":12},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"Choose now the Steps within the observation to be retrieved (one step usually one hour)"},"content":"\n\nselected_entry = extreme_dt_dictionary.extremeDT_params[selected_index]\ninput_string = selected_entry[\"step\"]\n\ndef parse_input_string(input_string):\n    ranges = input_string.split('/')\n    step_start = \"\"\n    step_end = \"\"\n    step_width = 1\n    hypen = \"\"\n    options = []\n    for rng in ranges:\n        if rng:\n            if '-' in rng:\n                hypen = \"-\"\n                start, end = rng.split('-')\n                step_width = int(end) - int(start)\n                if step_start == \"\":\n                    step_start = int(start.strip())\n                step_end = int(end.strip())\n            elif 'to' not in rng:\n                if step_start == \"\":\n                    step_start = int(rng.strip())\n                step_end = int(rng.strip())\n                #options.append(option)\n    #print(str(step_start) + \":\" + str(step_end))\n    if hypen != \"\":\n        options.extend([f\"{i}-{i+step_width}\" for i in range(step_start, step_end, step_width)])\n    else:\n        options.extend([f\"{i}\" for i in range(step_start, step_end+1, step_width)])\n    return options\n\ndef get_selected_step_values():\n    selected_values = multi_select.value\n    selected_values_string = \"/\".join(selected_values)\n    return selected_values_string\n\noptions = parse_input_string(input_string)\n\nmulti_select = widgets.SelectMultiple(\n    options=options,\n    description='Select (Steps):',\n    disabled=False\n)\n\ndisplay(multi_select)\n\n\n","type":"content","url":"/extremedt-parameterplotter#choose-now-the-steps-within-the-observation-to-be-retrieved-one-step-usually-one-hour","position":13},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"Handle different Levels to be selected (if any)"},"type":"lvl2","url":"/extremedt-parameterplotter#handle-different-levels-to-be-selected-if-any","position":14},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"Handle different Levels to be selected (if any)"},"content":"\n\n# Define a global variable\nglobal global_widget\nglobal_widget = None\n\nif selected_entry[\"levelist\"] != \"\":\n    # Convert levelist string to list of integers\n    levelist = list(map(int, selected_entry[\"levelist\"].split('/')))\n\n      \n    # Create a function to generate the widget based on the selection mode\n    def generate_widget(selection_mode):\n        global global_widget\n        if selection_mode == 'Single':\n            global_widget = widgets.Dropdown(options=levelist, description='Select level:')\n            return global_widget\n        elif selection_mode == 'Multiple':\n            global_widget = widgets.SelectMultiple(options=levelist, description='Select levels:')\n            return global_widget\n\n    # Create a dropdown widget to choose selection mode\n    selection_mode_dropdown = widgets.Dropdown(options=['Single', 'Multiple'], description='Selection Mode:')\n\n    # Create an output widget to display the selected option(s)\n    output = widgets.Output()\n\n    # Function to display the widget based on the selection mode\n    def display_widget(selection_mode):\n        output.clear_output()\n        with output:\n            display(generate_widget(selection_mode))\n\n    # Define a function to handle the change in selection mode\n    def on_dropdown_change(change):\n        display_widget(change.new)\n\n    # Register the function to handle dropdown changes\n    selection_mode_dropdown.observe(on_dropdown_change, names='value')\n\n    # Display the widgets\n    display(selection_mode_dropdown, output)\n\n    # Display the initial widget based on default selection mode\n    display_widget('Single')\n\n# Function to convert tuple or single integer to string separated by \"/\"\ndef convert_to_string(input):\n    if isinstance(input, tuple):\n        return '/'.join(map(str, input))\n    elif isinstance(input, int):\n        return str(input)\n    else:\n        return None  # Handle other types if needed\n\nlevlInput = \"\"\nif global_widget != None:\n    # Test cases\n    levlInput = convert_to_string(global_widget.value)\n\n\n# Call get_selected_values after the display is finished\nselected_step_values = get_selected_step_values()\n\n# Print the result in JSON format\n#datechoice = get_selected_values()[2].strftime('%Y%m%d')\ndatechoice = \"{fname}T00:00:00Z\".format(fname = get_selected_values()[2])\n\nfilter_params = {\n  \"class\": \"d1\",                       # fixed \n  \"dataset\": \"extremes-dt\",             # fixed extreme-dt access\n  \"expver\": \"0001\",                    # fixed experiment version \n  \"stream\": selected_entry[\"stream\"],\n  \"type\": \"fc\",                        # fixed forecasted fields\n#  \"date\": datechoice,                  # choose the date\n  \"time\": \"0000\",                      # fixed \n  \"step\": selected_step_values,        # step choice \n  \"levtype\": selected_entry[\"levtype\"],  \n  \"levelist\": str(levlInput),  \n  \"param\": str(selected_entry[\"param\"]),  \n}\n\n# Print the result in JSON format\nprint(datechoice)\nprint(json.dumps(filter_params, indent=4))\n\n","type":"content","url":"/extremedt-parameterplotter#handle-different-levels-to-be-selected-if-any","position":15},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"Obtain Authentication Token"},"type":"lvl2","url":"/extremedt-parameterplotter#obtain-authentication-token","position":16},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"Obtain Authentication Token"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nDESP_USERNAME = input(\"Please input your DESP username: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/extremedt-parameterplotter#obtain-authentication-token","position":17},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl4":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"type":"lvl4","url":"/extremedt-parameterplotter#check-if-dt-access-is-granted","position":18},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl4":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/extremedt-parameterplotter#check-if-dt-access-is-granted","position":19},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"Query using the DEDL HDA API"},"type":"lvl2","url":"/extremedt-parameterplotter#query-using-the-dedl-hda-api","position":20},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/extremedt-parameterplotter#query-using-the-dedl-hda-api","position":21},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/extremedt-parameterplotter#filter","position":22},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"content":"We have to setup up a filter and define which data to obtain.\n\n# Check if levelist is empty and remove it\nif filter_params.get(\"levelist\") == \"\":\n    del filter_params[\"levelist\"]\n   \nhdaFilters = {\n    key: {\"eq\": value}\n    for key, value in filter_params.items()\n}\n\n#print(hdaFilters)\n\n\n\n","type":"content","url":"/extremedt-parameterplotter#filter","position":23},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/extremedt-parameterplotter#make-data-request","position":24},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"content":"Please repeat this call is you have a timeout failure (under investion in the moment)\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\n\nresponse = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n \"collections\": [\"EO.ECMWF.DAT.DT_EXTREMES\"],\n    \"datetime\": datechoice,\n    \"query\": hdaFilters\n})\n\n# Requests to EO.ECMWF.DAT.DT_EXTREMES always return a single item containing all the requested data\n#print(response.json())\nproduct = response.json()[\"features\"][0]\nproduct[\"id\"]\n\n","type":"content","url":"/extremedt-parameterplotter#make-data-request","position":25},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/extremedt-parameterplotter#submission-worked-once-our-product-found-we-download-the-data","position":26},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"content":"\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = session.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\nprint(download_url)\nresponse.raise_for_status()\n    \n\n","type":"content","url":"/extremedt-parameterplotter#submission-worked-once-our-product-found-we-download-the-data","position":27},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl3":"Wait until data is there","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/extremedt-parameterplotter#wait-until-data-is-there","position":28},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl3":"Wait until data is there","lvl2":"Query using the DEDL HDA API"},"content":"This data is not available at the moment. And we can see that our request is in queuedstatus.We will now poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 seconds\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = session.get(url, headers=auth_headers, stream=True)\n    return response\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = call_api(url,auth_headers)\n\nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n     (print(response.text))\n\n# Check if Content-Disposition header is present\nif \"Content-Disposition\" not in response.headers:\n    print(response)\n    print(response.text)\n    raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n        \nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/extremedt-parameterplotter#wait-until-data-is-there","position":29},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"EarthKit"},"type":"lvl2","url":"/extremedt-parameterplotter#earthkit","position":30},{"hierarchy":{"lvl1":"Print the details of the parameter (Polytope convention):","lvl2":"EarthKit"},"content":"Lets plot the result file\n[EarthKit Documentation] \n\nhttps://​earthkit​-data​.readthedocs​.io​/en​/latest​/index​.html\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\nimport earthkit.regrid\n\ndata = earthkit.data.from_source(\"file\", filename)\ndata.ls\n\nearthkit.maps.quickplot(data, #style=style\n                       )","type":"content","url":"/extremedt-parameterplotter#earthkit","position":31},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/extremedt-dataavailability","position":0},{"hierarchy":{"lvl1":""},"content":"\n\n","type":"content","url":"/extremedt-dataavailability","position":1},{"hierarchy":{"lvl1":"","lvl2":"Aviso notification for DT data availability"},"type":"lvl2","url":"/extremedt-dataavailability#aviso-notification-for-dt-data-availability","position":2},{"hierarchy":{"lvl1":"","lvl2":"Aviso notification for DT data availability"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT \n\nCredit: The pyaviso package is provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nThis notebook shows how to check the data availablility for the Weather-Induced Extremes Digital Twin (Extremes DT) using the ECMWF Aviso package.\n\n","type":"content","url":"/extremedt-dataavailability#aviso-notification-for-dt-data-availability","position":3},{"hierarchy":{"lvl1":"","lvl3":"Install the pyaviso package","lvl2":"Aviso notification for DT data availability"},"type":"lvl3","url":"/extremedt-dataavailability#install-the-pyaviso-package","position":4},{"hierarchy":{"lvl1":"","lvl3":"Install the pyaviso package","lvl2":"Aviso notification for DT data availability"},"content":"\n\n!pip  install  pyaviso --quiet\n\n","type":"content","url":"/extremedt-dataavailability#install-the-pyaviso-package","position":5},{"hierarchy":{"lvl1":"","lvl3":"Import pyaviso","lvl2":"Aviso notification for DT data availability"},"type":"lvl3","url":"/extremedt-dataavailability#import-pyaviso","position":6},{"hierarchy":{"lvl1":"","lvl3":"Import pyaviso","lvl2":"Aviso notification for DT data availability"},"content":"Import pyaviso and other useful libraries. Defining constants and functions.\n\nfrom datetime import datetime\nfrom pprint import pprint as pp\n\nfrom pyaviso import NotificationManager, user_config\n\nLISTENER_EVENT = \"data\"  # Event for the listener, options are mars and dissemination\nTRIGGER_TYPE = \"function\"  # Type of trigger for the listener\n\n","type":"content","url":"/extremedt-dataavailability#import-pyaviso","position":7},{"hierarchy":{"lvl1":"","lvl3":"Defining the data to be notified","lvl2":"Aviso notification for DT data availability"},"type":"lvl3","url":"/extremedt-dataavailability#defining-the-data-to-be-notified","position":8},{"hierarchy":{"lvl1":"","lvl3":"Defining the data to be notified","lvl2":"Aviso notification for DT data availability"},"content":"The following request describes the data whose availability we want to be notified\n\nREQUEST = {\n    \"class\": \"d1\",\n#    \"dataset\": \"extremes-dt\",\n    \"expver\": \"0001\",\n    \"stream\": \"wave\",\n    \"type\": \"fc\",\n    \"time\": \"00\",\n    \"step\": \"0\",\n    \"levtype\": \"sfc\",\n#    \"levelist\": \"\",\n#    \"param\": \"168\"   \n}  # Request configuration for the listener\n\n\n","type":"content","url":"/extremedt-dataavailability#defining-the-data-to-be-notified","position":9},{"hierarchy":{"lvl1":"","lvl3":"Aviso configuration","lvl2":"Aviso notification for DT data availability"},"type":"lvl3","url":"/extremedt-dataavailability#aviso-configuration","position":10},{"hierarchy":{"lvl1":"","lvl3":"Aviso configuration","lvl2":"Aviso notification for DT data availability"},"content":"\n\nCONFIG = {\n    \"notification_engine\": {\n        \"host\": \"aviso.lumi.apps.dte.destination-earth.eu\",\n        \"port\": 443,\n        \"https\": True,\n    },\n    \"configuration_engine\": {\n        \"host\": \"aviso.lumi.apps.dte.destination-earth.eu\",\n        \"port\": 443,\n        \"https\": True,\n    },\n    \"schema_parser\": \"generic\",\n    \"remote_schema\": True,\n    \"auth_type\": \"none\",\n    \"quiet\" : True\n}  # manually defined configuration\n\n\n","type":"content","url":"/extremedt-dataavailability#aviso-configuration","position":11},{"hierarchy":{"lvl1":"","lvl3":"Searching for old notifications","lvl2":"Aviso notification for DT data availability"},"type":"lvl3","url":"/extremedt-dataavailability#searching-for-old-notifications","position":12},{"hierarchy":{"lvl1":"","lvl3":"Searching for old notifications","lvl2":"Aviso notification for DT data availability"},"content":"Ssearching for old notifications where available. This way users can explicitly replay past notifications and executes triggers.\n\nimport sys\nSTART_DATE = datetime(2020, 12, 12)  # Start date for the notification listener\n\ndef triggered_function(notification):\n    \"\"\"\n    Function for the listener to trigger.\n    \"\"\"\n    \n    #pp(notification)\n    # Access the date field\n    date_str = notification['request']['date']    \n\n    # Convert the date string to a datetime object\n    date_obj = datetime.strptime(date_str, '%Y%m%d')\n    formatted_date = date_obj.strftime('%Y-%m-%d')\n    pp(\"ExtremeDT data available=>\" + formatted_date)\n    \n    \n\ndef create_hist_listener():\n    \"\"\"\n    Creates and returns a listener configuration.\n    \"\"\"\n\n    trigger = {\n        \"type\": TRIGGER_TYPE,\n        \"function\": triggered_function,\n    }  # Define the trigger for the listener\n    \n    # Return the complete listener configuration\n    return {\"event\": LISTENER_EVENT, \"request\": REQUEST, \"triggers\": [trigger]}\n\n\ntry:\n    listener = create_hist_listener()  # Create listener configuration\n    listeners_config = {\"listeners\": [listener]}  # Define listeners configuration\n    config = user_config.UserConfig(**CONFIG)\n    print(\"loaded config:\")\n    pp(CONFIG)\n    nmh = NotificationManager()  # Initialize the NotificationManager\n\n    nmh.listen(\n        listeners=listeners_config, from_date=START_DATE, config=config\n    )  # Start listening\nexcept Exception as e:\n    print(f\"Failed to initialize the Notification Manager: {e}\")","type":"content","url":"/extremedt-dataavailability#searching-for-old-notifications","position":13},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)"},"type":"lvl1","url":"/hda-eodag-full-version","position":0},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)"},"content":"\n\n","type":"content","url":"/hda-eodag-full-version","position":1},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)"},"type":"lvl1","url":"/hda-eodag-full-version#dedl-eodag-destine-data-lake-provider-dedl","position":2},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT \n\n","type":"content","url":"/hda-eodag-full-version#dedl-eodag-destine-data-lake-provider-dedl","position":3},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl3":"How to use EODAG to search and access DEDL data"},"type":"lvl3","url":"/hda-eodag-full-version#how-to-use-eodag-to-search-and-access-dedl-data","position":4},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl3":"How to use EODAG to search and access DEDL data"},"content":"EODAG is a command line tool and a Python package for searching and downloading earth observation data via a unified API regardless of the data provider. Detailed information about the usage of EODAG can be found on the \n\nproject documentation page.\n\nThis notebook demonstrates how to use the DEDL provider in EODAG, using Python code.\n\nSetup: EODAG configuration to use the provider DEDL .\n\nSearch: search DEDL data, we search for Sentinel-3 data.\n\nFilter: filter DEDL data.\n\nDownload: download DEDL data.\n\nThe complete guide on how to use EODAG Python API is available via \n\nhttps://​eodag​.readthedocs​.io​/en​/stable​/api​_user​_guide​.html.\n\nPrequisites: For search and download dedl products : \n\nDestinE user account\n\nNote:\n\nPlease note that the two factor authentication (2FA) is still not implemented in EODAG. The users who have enabled 2FA on DESP will not be able to run this notebook.\n\n","type":"content","url":"/hda-eodag-full-version#how-to-use-eodag-to-search-and-access-dedl-data","position":5},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl2":"Setup"},"type":"lvl2","url":"/hda-eodag-full-version#setup","position":6},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl2":"Setup"},"content":"In this section, we set:\n\nThe output_dir, the directory where to store downloaded products.\n\nThe DEDL credentials, you’ll be asked to enter your DEDL credentials.\n\nThe search timeout, it is of 60 seconds to avoid any unexpected errors because of long running search queries.\n\nimport os\nfrom getpass import getpass\n\nworkspace = 'eodag_workspace'\nif not os.path.isdir(workspace):\n    os.mkdir(workspace)\n    \nos.environ[\"EODAG__DEDL__DOWNLOAD__OUTPUT_DIR\"] = os.path.abspath(workspace)\n#os.environ[\"EODAG__DEDL__DOWNLOAD__OUTPUTS_PREFIX\"] = os.path.abspath(workspace)\n\nos.environ[\"EODAG__DEDL__PRIORITY\"]=\"10\"\nos.environ[\"EODAG__DEDL__SEARCH__TIMEOUT\"]=\"60\"\n\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__USERNAME\"]=DESP_USERNAME\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__PASSWORD\"]=DESP_PASSWORD\n\n\n","type":"content","url":"/hda-eodag-full-version#setup","position":7},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl3":"Import EODAG and list available products on DEDL","lvl2":"Setup"},"type":"lvl3","url":"/hda-eodag-full-version#import-eodag-and-list-available-products-on-dedl","position":8},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl3":"Import EODAG and list available products on DEDL","lvl2":"Setup"},"content":"We now need to import the EODataAccessGateway class. The class is going to take care of  all the following operations.\n\nWe can start listing the products available using dedl as provider.\n\n\nfrom eodag import EODataAccessGateway\ndag = EODataAccessGateway()\n\n[product_type[\"ID\"] + \", \" + product_type[\"title\"] for product_type in dag.list_product_types(\"dedl\")]\n\n","type":"content","url":"/hda-eodag-full-version#import-eodag-and-list-available-products-on-dedl","position":9},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl2":"Search"},"type":"lvl2","url":"/hda-eodag-full-version#search","position":10},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl2":"Search"},"content":"To search we use the search method passing the ID of our dataset of interest and a geo-time filter.\n\nThe search method returns a SearchResult object that stores the products obtained from a given page (default: page=1) and a given maximum number of items per page (default: items_per_page=20). The search_all() method can be used instead.\n\nIn the following cell, we change the default value of items_per_page and define the search criteria to retrieve Sentinel-2 MSI Level-2 images over Sicily, first days of July 2024. Our goal is to check whether any effects of Mount Etna’s eruptions during that period are visible in the Sentinel-2 imagery.\n\nsearch_criteria = {\n    \"provider\":\"dedl\",\n    \"productType\": \"EO.ESA.DAT.SENTINEL-2.MSI.L2A\",\n    \"start\": \"2024-07-04T07:00:00.00Z\",\n    \"end\": \"2024-07-08T07:00:00.00Z\",\n    \"geom\": {\"lonmin\": 12, \"latmin\": 37, \"lonmax\": 16, \"latmax\": 39},\n    \"count\": True,\n    \"items_per_page\": 50\n}\n\nproducts_first_page = dag.search(**search_criteria)\n\nResults are stored in a ‘SearchResult’ object that contains the details on the single search result.\n\nproducts_first_page\n\nIt is possible to list the metadata associated with a certain product, we choose the first one returned [0], and look into it.\n\none_product = products_first_page[0]\none_product.properties.keys()\n\none_product.properties['cloudCover']\n\n","type":"content","url":"/hda-eodag-full-version#search","position":11},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl2":"Filter"},"type":"lvl2","url":"/hda-eodag-full-version#filter","position":12},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl2":"Filter"},"content":"EODAG can filter the search result. We can then refine our initial search without asking the provider again.\nProducts can be filtered according to their properties or also with finer geometry filters.\n\nThe following example shows how to filter products to keep only those whose cloud coverage is less than 20%. And then restrict the results to products containing a smaller area over the mount Etna.\n\nLet’s define now a smaller area around the mount Etna and a function to see the area on a map together with the results\n\nfrom eodag.crunch import FilterProperty\nfrom eodag.crunch import FilterOverlap\nimport shapely\nimport folium\nfrom shapely.geometry import Polygon\n\nsmall_geom = Polygon([[15.1, 37.7], [15.5, 37.7], [15.1, 37.75], [15.1, 37.75], [15.1, 37.7]])\n\nsmaller_area = {\"lonmin\": 15.1, \"latmin\": 37.7, \"lonmax\": 15.5, \"latmax\": 37.75}\n\nsearch_geometry = shapely.geometry.box(\n    smaller_area[\"lonmin\"],\n    smaller_area[\"latmin\"],\n    smaller_area[\"lonmax\"],\n    smaller_area[\"latmax\"],\n)\n\n\ndef create_search_result_map(search_results, extent):\n    \"\"\"Small utility to create an interactive map with folium\n    that displays an extent in red and EO Producs in blue\"\"\"\n    fmap = folium.Map([38, 14], zoom_start=7)\n    folium.GeoJson(\n        extent,\n        style_function=lambda x: dict(color=\"red\")\n    ).add_to(fmap)\n    folium.GeoJson(\n        search_results\n    ).add_to(fmap)\n    return fmap\n\n# Crunch the results\nfiltered_results = products_first_page.crunch(FilterProperty({\"cloudCover\": 20, \"operator\" : \"lt\"}))\n\nprint(f\"Got now {len(filtered_results)} products after filtering by cloudCover.\")\n\nfiltered_products = filtered_results.crunch(\n    FilterOverlap(dict(contains=True)),\n    geometry=small_geom\n)\nprint(f\"Got now {len(filtered_products)} products after filtering by geometry.\")\n\nLet’s use the function defined to see the area defined on a map (red) together with the initial results (blue) filtered by cloud coverage and geometry (green).\n\nfmap = create_search_result_map(products_first_page, search_geometry)\n# Create a layer that represents the filtered products in green\nfolium.GeoJson(\n    filtered_products,\n    style_function=lambda x: dict(color=\"green\")\n).add_to(fmap)\nfmap\n\n\n","type":"content","url":"/hda-eodag-full-version#filter","position":13},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl2":"Download"},"type":"lvl2","url":"/hda-eodag-full-version#download","position":14},{"hierarchy":{"lvl1":"DEDL - EODAG - DestinE Data Lake Provider (DEDL)","lvl2":"Download"},"content":"Before downloading any product, it can be useful to have a quick look at them.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nquicklooks_dir = os.path.join(workspace, \"quicklooks\")\nif not os.path.isdir(quicklooks_dir):\n    os.mkdir(quicklooks_dir)\n\n\n\nfig = plt.figure(figsize=(20, 40))\nfor i, product in enumerate(filtered_products, start=1):\n\n    # This line takes care of downloading the quicklook\n    quicklook_path = product.get_quicklook()\n    \n    img = mpimg.imread(quicklook_path)\n    ax = fig.add_subplot(8, 2, i)\n    ax.set_title(product.properties['dedl:beginningDateTime'] + \"TILE: \" +product.properties['dedl:tileIdentifier'])\n    plt.imshow(img)\nplt.tight_layout()\n\nThe quicklook shows effectively the ash plume caused by the eruptions.\n\nEOProducts can be downloaded individually. The last image is going to be downloaded.\n\nproduct_to_download = filtered_products[-1]\nproduct_path = dag.download(product_to_download)\nproduct_path\n\nThe location property of this product now points to a local path.\n\nproduct_to_download.location","type":"content","url":"/hda-eodag-full-version#download","position":15},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL"},"type":"lvl1","url":"/hda-eodag-quick-start","position":0},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL"},"content":"\n\n","type":"content","url":"/hda-eodag-quick-start","position":1},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL"},"type":"lvl1","url":"/hda-eodag-quick-start#dedl-eodag-a-quick-start-with-dedl","position":2},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT \n\n","type":"content","url":"/hda-eodag-quick-start#dedl-eodag-a-quick-start-with-dedl","position":3},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL","lvl3":"How to use EODAG to search and access DEDL data - quick start"},"type":"lvl3","url":"/hda-eodag-quick-start#how-to-use-eodag-to-search-and-access-dedl-data-quick-start","position":4},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL","lvl3":"How to use EODAG to search and access DEDL data - quick start"},"content":"EODAG is a command line tool and a Python package for searching and downloading earth observation data via a unified API.\n\nThis quickstart is to help get DEDL data using EODAG. Detailed information about the usage of EODAG can be found on the \n\nproject documentation page.\n\nThroughout this quickstart notebook, you will learn:\n\nSetup: How to configure EODAG to use the provider DEDL.\n\nDiscover: How to discover DEDL datasets through EODAG.\n\nSearch products:  How to search DEDL data through EODAG.\n\nDownload products: How to download DEDL data through EODAG.\n\nThe complete guide on how to use EODAG Python API is available via \n\nhttps://​eodag​.readthedocs​.io​/en​/stable​/api​_user​_guide​.html.\n\nPrequisites: Search and download dedl products : \n\nDestinE user account\n\nNote:\n\nPlease note that the two factor authentication (2FA) is still not implemented in EODAG. The users who have enabled 2FA on DESP will not be able to run this notebook.\n\n","type":"content","url":"/hda-eodag-quick-start#how-to-use-eodag-to-search-and-access-dedl-data-quick-start","position":5},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL","lvl2":"Setup"},"type":"lvl2","url":"/hda-eodag-quick-start#setup","position":6},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL","lvl2":"Setup"},"content":"In this section, we set:\n\nThe output_dir, the directory where to store downloaded products.\n\nThe DEDL credentials, you’ll be asked to enter your DEDL credentials.\n\nThe search timeout, it is of 60 seconds to avoid any unexpected errors because of long running search queries.\n\nimport os\nfrom getpass import getpass\n\nworkspace = 'eodag_workspace'\nif not os.path.isdir(workspace):\n    os.mkdir(workspace)\n    \nos.environ[\"EODAG__DEDL__DOWNLOAD__OUTPUTS_PREFIX\"] = os.path.abspath(workspace)\n\nos.environ[\"EODAG__DEDL__SEARCH__TIMEOUT\"]=\"60\"\nos.environ[\"DEFAULT_STREAM_REQUESTS_TIMEOUT\"] = \"15\"\nos.environ[\"EODAG__DEDL__PRIORITY\"]=\"10\"\n\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__USERNAME\"]=DESP_USERNAME\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__PASSWORD\"]=DESP_PASSWORD\n\n\n","type":"content","url":"/hda-eodag-quick-start#setup","position":7},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL","lvl2":"EODiscover"},"type":"lvl2","url":"/hda-eodag-quick-start#eodiscover","position":8},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL","lvl2":"EODiscover"},"content":"We now need to import the EODataAccessGateway class in order to discover the available DEDL collections.\n\nCollections are presented in a dropdown menu, selecting a collection its description will be prompted.\n\n\nfrom eodag import EODataAccessGateway, setup_logging\ndag = EODataAccessGateway()\n\nsetup_logging(1)\n\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output, HTML\nfrom ipywidgets import Layout, Box\nimport json\n\n#default values\n\nDATASET_ID = 'EO.EUM.DAT.SENTINEL-3.OL_2_WFR___'\n\n\n# Event listeners\ndef on_change(change):\n    with output_area:\n        clear_output()\n        print(f'Selected: {change[\"new\"]}')\n        print('---------------------------------------------')\n        delimiter=''\n        global DATASET_ID\n        DATASET_ID = delimiter.join(change[\"new\"])\n        product_types=dag.list_product_types(\"dedl\")\n        index = next((i for i, d in enumerate(product_types) if d.get('ID') == DATASET_ID), None)\n        \n        print(\"TITLE: \"+product_types[index]['title'])\n        print(\"ABSTRACT: \"+product_types[index]['abstract'])\n        print(\"KEYWORDS: \"+product_types[index]['abstract'])\n\noptions=[product_type[\"ID\"] for product_type in dag.list_product_types(\"dedl\")]\n\n# Widgets\noutput_area = widgets.Output()\n\ndropdown = widgets.Dropdown(\n    options=options,\n    value=options[0],\n    description=\"Datasets:\",\n    disabled=False,\n) \n\ndropdown.observe(on_change, names='value')\n\n\n# Layout\n# Define the layout for the dropdown\ndropdown_layout = Layout(display='space-between', justify_content='center', width='80%')\n# Create a box to hold the dropdown with the specified layout\nbox = Box([dropdown, output_area], layout=dropdown_layout)\ndisplay( box)  \n\n\n","type":"content","url":"/hda-eodag-quick-start#eodiscover","position":9},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL","lvl2":"EOSearch"},"type":"lvl2","url":"/hda-eodag-quick-start#eosearch","position":10},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL","lvl2":"EOSearch"},"content":"Once we selected our dataset of interest, we can define the search criteria to find data inside the chosen dataset.\n\nSelect bbox, start and end date to use in the search:\n\n# Import necessary libraries\nimport folium, datetime\nfrom ipywidgets import interactive, HBox, VBox, FloatText, Button, Label\n\n#default values\n\nSTART_DATE = '2024-08-11'\nEND_DATE = '2024-08-13'\n\nsw_lat = 37.0\nsw_lng = 14.0\nne_lat = 38.0\nne_lng = 16.0\n\n#functions to handle the chosen values\ndef on_date_change(change):\n    with output_area:\n        clear_output()\n        global START_DATE\n        global END_DATE\n        t1 = start_date.value\n        t2 = end_date.value\n        START_DATE = t1.strftime('%Y-%m-%d')\n        END_DATE = t2.strftime('%Y-%m-%d')\n\n        \ndef update_map(sw_lat, sw_lng, ne_lat, ne_lng):\n    # Clear the map\n    m = folium.Map(location=[(sw_lat + ne_lat) / 2, (sw_lng + ne_lng) / 2], zoom_start=4, tiles=None)\n\n    nasa_wms = folium.WmsTileLayer(\n        url='https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi',\n        name='NASA Blue Marble',\n        layers='BlueMarble_ShadedRelief',\n        format='image/png',\n        transparent=True,\n        attr='NASA'\n    )\n    nasa_wms.add_to(m)\n    \n    # Add the bounding box rectangle\n    folium.Rectangle(\n        bounds=[[sw_lat, sw_lng], [ne_lat, ne_lng]],\n        color=\"#ff7800\",\n        fill=True,\n        fill_opacity=0.3\n    ).add_to(m)\n    \n    # Display the map\n    display(m)\n    return (sw_lat, sw_lng, ne_lat, ne_lng)\n\ndef save_bbox(button):\n    global sw_lat \n    global sw_lng  \n    global ne_lat \n    global ne_lng \n\n    sw_lat = sw_lat_input.value\n    sw_lng = sw_lng_input.value\n    ne_lat = ne_lat_input.value\n    ne_lng = ne_lng_input.value\n    \n    with output:\n        clear_output()\n        print(f\"BBox: sw_lat {sw_lat} sw_lng {sw_lng} ne_lat {ne_lat} ne_lng {ne_lng}\")\n\n\n# Create a base map\nm = folium.Map(location=[40, 20], zoom_start=4)\n        \n        \n# Widgets\noutput = widgets.Output()\n\n# Widgets for bbox coordinates input\nsw_lat_input = FloatText(value=sw_lat, description='SW Latitude:')\nsw_lng_input = FloatText(value=sw_lng, description='SW Longitude:')\nne_lat_input = FloatText(value=ne_lat, description='NE Latitude:')\nne_lng_input = FloatText(value=ne_lng, description='NE Longitude:')\n\nsave_button = Button(description=\"Save BBox\")\n\n\n# Create DatePicker widgets\nstart_date = widgets.DatePicker(\n    description='Start Date',value = datetime.date(2024,8,11),\n    disabled=False\n)\n\nend_date = widgets.DatePicker(\n    description='End Date',value = datetime.date(2024,8,13),\n    disabled=False\n)\n\n\nui = VBox([\n   # HBox([sw_lat_input, sw_lng_input]),\n  #  HBox([ne_lat_input, ne_lng_input]),\n    save_button,\n    output,\n    start_date, \n    end_date\n])\n\n# Display the interactive map and UI\ninteractive_map = interactive(update_map, sw_lat=sw_lat_input, sw_lng=sw_lng_input,\n                              ne_lat=ne_lat_input, ne_lng=ne_lng_input)\n\n\ndisplay(interactive_map, ui)\n\n     \n#Events\n\nstart_date.observe(on_date_change, names='value')\nend_date.observe(on_date_change, names='value')\nsave_button.on_click(save_bbox)\n\nSelected criteria:\n\nsearch_criteria = {\n    \"productType\": DATASET_ID,\n    \"start\": START_DATE,\n    \"end\": END_DATE,\n    \"geom\": {\"lonmin\": sw_lng, \"latmin\": sw_lat, \"lonmax\": ne_lng, \"latmax\": ne_lat},\n    \"count\": True\n}\n\nprint(json.dumps(search_criteria, indent=2))\n\nUse selected criteria to search data:\n\nproducts_first_page = dag.search(**search_criteria)\nprint(f\"Got {len(products_first_page)} products and an estimated total number of {products_first_page.number_matched} products.\")\nproducts_first_page\n\nSee the available metadata:\n\n\nif(len(products_first_page)>0):\n    one_product = products_first_page[0]\n    print(one_product.properties.keys())\n\n","type":"content","url":"/hda-eodag-quick-start#eosearch","position":11},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL","lvl2":"EODownload"},"type":"lvl2","url":"/hda-eodag-quick-start#eodownload","position":12},{"hierarchy":{"lvl1":"DEDL - EODAG - A quick start with DEDL","lvl2":"EODownload"},"content":"EOProducts can be downloaded individually. The last image is going to be downloaded.\n\nif(len(products_first_page)>0):\n    product_to_download = one_product\n    product_path = dag.download(product_to_download)\n    print(product_path)","type":"content","url":"/hda-eodag-quick-start#eodownload","position":13},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1","position":0},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1","position":1},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#destination-earth-avhrr-level-1b-metop-global-data-access-using-dedl-hda","position":2},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA"},"content":"Author: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT \n\nThe Advanced Very High Resolution Radiometer (AVHRR) operates at 5 different channels simultaneously in the visible and infrared bands. Channel 3 switches between 3a and 3b for daytime and nighttime. As a high-resolution imager (about 1.1 km near nadir) its main purpose is to provide cloud and surface information such as cloud coverage, cloud top temperature, surface temperature over land and sea, and vegetation or snow/ice.\n\nDestinE Data Lake HDA\n\nAVHRR Level 1B - Metop - Global\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#destination-earth-avhrr-level-1b-metop-global-data-access-using-dedl-hda","position":3},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl3":"How to access and visualize AVHRR Level 1B Metop Global"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#how-to-access-and-visualize-avhrr-level-1b-metop-global","position":4},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl3":"How to access and visualize AVHRR Level 1B Metop Global"},"content":"This notebook demonstrates how to search and access Metop data using HDA and how to read, process and visualize it using satpy\n\nThroughout this notebook, you will learn:\n\nAuthenticate: How to authenticate for searching and access DEDL collections.\n\nSearch Metop AVHRR data:  How to search DEDL data using datetime and bbox filters.\n\nDownload Metop AVHRR data: How to download DEDL data through HDA.\n\nRead and visualize Metop AVHRR data: How to load process and visualize Metop AVHRR data using Satpy.\n\nPrerequisites:\n\nFor filtering data inside collections : \n\nDestinE user account\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#how-to-access-and-visualize-avhrr-level-1b-metop-global","position":5},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl2":"Authenticate"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#authenticate","position":6},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl2":"Authenticate"},"content":"\n\nimport destinelab as deauth\n\nimport requests\nimport json\nimport os\nfrom getpass import getpass\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#authenticate","position":7},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl2":"Search Metop AVHRR data"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#search-metop-avhrr-data","position":8},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl2":"Search Metop AVHRR data"},"content":"\n\nsearch_response = requests.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n        \"BBox\":  [-5 ,31,20,51],\n    \"collections\": [\"EO.EUM.DAT.METOP.AVHRRL1\"],\n    \"datetime\": \"2024-07-04T11:00:00Z/2024-07-04T13:00:00Z\"\n})\n\n\nThe first item in the search results\n\nfrom IPython.display import JSON\n\nJSON(search_response.json()[\"features\"][0])\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#search-metop-avhrr-data","position":9},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl2":"Download Metop AVHRR data"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#download-metop-avhrr-data","position":10},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl2":"Download Metop AVHRR data"},"content":"We can download now the returned data.\n\nfrom tqdm import tqdm\nimport time\nimport zipfile\n\n#number of products to download:\nnptd=1\n\n# Define a list of assets to download\nfor i in range(0,nptd,1):\n    product=search_response.json()[\"features\"][i]\n    download_url = product[\"assets\"][\"downloadLink\"][\"href\"]\n    print(download_url)\n    filename = \"downloadLink\"\n    response = requests.get(download_url, headers=auth_headers)\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    print(f\"downloading {filename}\")\n\n    with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n        with open(filename, 'wb') as f:\n            for data in response.iter_content(1024):\n                progress_bar.update(len(data))\n                f.write(data)\n        \n    zf=zipfile.ZipFile(filename)\n    with zipfile.ZipFile(filename, 'r') as zip_ref:\n        zip_ref.extractall('.')\n\ndel response\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#download-metop-avhrr-data","position":11},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#read-and-visualize-metop-avhrr-data-using-satpy","position":12},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"content":"\n\nThe Python package satpy supports reading and loading data from many input files.\nFor Metop data in the native format, we can use the satpy reader ‘avhrr_l1b_eps’.\n\npip install --quiet satpy pyspectral\n\nimport os\nfrom glob import glob\n\nimport xarray as xr\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nfrom matplotlib.axes import Axes\n\nimport satpy\nfrom satpy.scene import Scene\nfrom satpy.composites import GenericCompositor\nfrom satpy.writers import to_image\nfrom satpy.resample import get_area_def\nfrom satpy import available_readers\nfrom satpy import MultiScene\n\nimport pyresample\nimport pyspectral\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action = \"ignore\", category = RuntimeWarning)\n\nsatpy_installation_path=satpy.__path__\ndelimiter = \"\" \nsatpy_installation_path = delimiter.join(satpy_installation_path)\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#read-and-visualize-metop-avhrr-data-using-satpy","position":13},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#read-and-load-data","position":14},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#read-and-load-data","position":15},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl4":"Single scene","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"type":"lvl4","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#single-scene","position":16},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl4":"Single scene","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"content":"We can use the Scene constructor from the satpy library, a Scene object represents a single geographic region of data.\nOnce loaded we can list all the available bands (spectral channel) for that scene.\n\nfilenames = glob('./AVHR_xxx_1B_M0*.nat')\n#len(filenames)\n\n# read the last file in filenames\nscn = Scene(reader='avhrr_l1b_eps', filenames=[filenames[-1]])\n# print available datasets\nscn.available_dataset_names()\n\nWe can then load the first and the second spectral channels and have a look to some info\n\n# load  \nscn.load(['1','2'])\nscn['1']\n\nscn['1'].attrs['wavelength']\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#single-scene","position":17},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl4":"Do some calculation","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"type":"lvl4","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#do-some-calculation","position":18},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl4":"Do some calculation","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"content":"Calculations based on loaded datasets/channels can easily be assigned to a new dataset.\n\nWe resample the scene in a smaller area over the Spain and use the 2 loaded datasets to calculate a new dataset.\n\nnewscn = scn.resample('spain')\n\nnewscn[\"ndvi\"] = (newscn['2'] - newscn['1']) / (newscn['2'] + newscn['1'])\n#scn.show(\"ndvi\")\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#do-some-calculation","position":19},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl3":"Visualize datasets","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#visualize-datasets","position":20},{"hierarchy":{"lvl1":"Destination Earth - AVHRR Level 1B Metop Global - Data Access using DEDL HDA","lvl3":"Visualize datasets","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"content":"\n\nimport matplotlib.pyplot as plt\nfrom pyresample.kd_tree import resample_nearest\nfrom pyresample.geometry import AreaDefinition\nfrom pyresample import load_area\n\n\n\n\narea_def = load_area(satpy_installation_path+'/etc/areas.yaml', 'spain') \n#scene \nlons, lats = newscn[\"1\"].area.get_lonlats()\nswath_def = pyresample.geometry.SwathDefinition(lons, lats)\nndvi = newscn[\"ndvi\"].data.compute()\nresult = resample_nearest(swath_def, ndvi, area_def, radius_of_influence=20000, fill_value=None)\n\n#cartopy\ncrs = area_def.to_cartopy_crs()\nfig, ax = plt.subplots(subplot_kw=dict(projection=crs))\ncoastlines = ax.coastlines()  \nax.set_global()\n\n#plot\nimg = plt.imshow(result, transform=crs, extent=crs.bounds, origin='upper')\ncbar = plt.colorbar()","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#visualize-datasets","position":21},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err","position":0},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT ","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err","position":1},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#destination-earth-olci-level-1b-reduced-resolution-sentinel-3-data-access-using-dedl-hda","position":2},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA"},"content":"Documentation DestinE Data Lake HDA\n\nOLCI Level 1B Reduced Resolution - Sentinel-3\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#destination-earth-olci-level-1b-reduced-resolution-sentinel-3-data-access-using-dedl-hda","position":3},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl3":"How to access and visualize OLCI Level 1B Reduced Resolution - Sentinel-3"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#how-to-access-and-visualize-olci-level-1b-reduced-resolution-sentinel-3","position":4},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl3":"How to access and visualize OLCI Level 1B Reduced Resolution - Sentinel-3"},"content":"This notebook demonstrates how to search and access Sentinel-3 data using HDA and how to read and visualize it using satpy\n\nThroughout this notebook, you will learn:\n\nAuthenticate: How to authenticate for searching and access DEDL collections.\n\nSearch OLCI data:  How to search DEDL data using datetime and bbox filters.\n\nDownload OLCI data: How to download DEDL data through HDA.\n\nRead and visualize OLCI data: How to load process and visualize OlCI data using Satpy.\n\nPrerequisites:\n\nFor filtering data inside collections : \n\nDestinE user account\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#how-to-access-and-visualize-olci-level-1b-reduced-resolution-sentinel-3","position":5},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl2":"Authenticate"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#authenticate","position":6},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl2":"Authenticate"},"content":"We start off by importing the relevant modules for DestnE authentication, HTTP requests, json handling.\nThen we authenticate in DestinE.\n\nimport destinelab as deauth\n\nimport requests\nimport json\nimport os\nfrom getpass import getpass\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#authenticate","position":7},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl2":"Search"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#search","position":8},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl2":"Search"},"content":"\n\nOnce authenticated, we search a product matching our filters.\n\nFor this example, we search data for the \n\nOLCI Level 1B Reduced Resolution - Sentinel-3 dataset.\n\nThe corresponding collection ID in HDA for this dataset is: EO.EUM.DAT.SENTINEL-3.OL_1_ERR___.\n\nresponse = requests.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n    \"collections\": [\"EO.EUM.DAT.SENTINEL-3.OL_1_ERR___\"],\n    \"datetime\": \"2024-06-25T00:00:00Z/2024-06-30T00:00:00Z\",\n    \"bbox\":  [10,53,30,66]\n})\nif(response.status_code!= 200):\n    (print(response.text))\nresponse.raise_for_status()\n\nWe can have a look at the metadata of the first products returned by the search.\n\nfrom IPython.display import JSON\n\nproduct = response.json()[\"features\"][0]\nJSON(product)\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#search","position":9},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl2":"Download"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#download","position":10},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl2":"Download"},"content":"The product metadata contains the link to download it. We can use that link to download the selected product.\nIn this case we download the first product returned by our search.\n\nfrom tqdm import tqdm\nimport time\n\nassets = [\"downloadLink\"]\n\nfor asset in assets:\n    download_url = product[\"assets\"][asset][\"href\"]\n    print(download_url)\n    filename = product[\"id\"]\n    print(filename)\n    response = requests.get(download_url, headers=auth_headers)\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    print(f\"downloading {filename}\")\n\n    with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n        with open(filename, 'wb') as f:\n            for data in response.iter_content(1024):\n                progress_bar.update(len(data))\n                f.write(data)\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#download","position":11},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl3":"Unfold the product","lvl2":"Download"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#unfold-the-product","position":12},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl3":"Unfold the product","lvl2":"Download"},"content":"\n\ndel response\nimport os\nimport zipfile\n\nzf=zipfile.ZipFile(filename)\nwith zipfile.ZipFile(filename, 'r') as zip_ref:\n    zip_ref.extractall('.')\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#unfold-the-product","position":13},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl2":"Read and visualize OLCI data using Satpy"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#read-and-visualize-olci-data-using-satpy","position":14},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl2":"Read and visualize OLCI data using Satpy"},"content":"\n\nThe Python package satpy supports reading and loading data from many input files.\n\nBelow the installation and import of useful modules and packages.\n\npip install --quiet satpy pyspectral\n\nfrom datetime import datetime\nfrom satpy import find_files_and_readers\nfrom satpy.scene import Scene\nfrom satpy.composites import GenericCompositor\nfrom satpy.writers import to_image\nfrom satpy.resample import get_area_def\nfrom satpy import available_readers\n\nimport pyresample\nimport pyspectral\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action = \"ignore\", category = RuntimeWarning)\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#read-and-visualize-olci-data-using-satpy","position":15},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl3":"Read data","lvl2":"Read and visualize OLCI data using Satpy"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#read-data","position":16},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl3":"Read data","lvl2":"Read and visualize OLCI data using Satpy"},"content":"We can read the downloaded data using the “olci_l1b” satpy reader\n\n\n\nfiles = find_files_and_readers(sensor=\"olci\",\n                               start_time=datetime(2024, 6, 25, 0, 0),\n                               end_time=datetime(2024, 6, 30, 0, 0),\n                               base_dir=\".\",\n                               reader=\"olci_l1b\")\n\nscn = Scene(filenames=files)\n# print available datasets\nscn.available_dataset_names()\n\nWe can print the available datasets for the loaded scene.\n\nWith the function load(), you can specify an individual band by name. If you then select the loaded band, you see the xarray.DataArray band object\n\n# load bands \nscn.load(['humidity','total_ozone'])\nscn['humidity']\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#read-data","position":17},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl3":"Visualize data","lvl2":"Read and visualize OLCI data using Satpy"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#visualize-data","position":18},{"hierarchy":{"lvl1":"Destination Earth - OLCI Level 1B Reduced Resolution - Sentinel-3 - Data Access using DEDL HDA","lvl3":"Visualize data","lvl2":"Read and visualize OLCI data using Satpy"},"content":"We can visualize the available datasets on a map.\n\nimport matplotlib.pyplot as plt\nfrom pyresample.kd_tree import resample_nearest\nfrom pyresample.geometry import AreaDefinition\n\n\n#area definition\narea_id = 'worldeqc30km'\ndescription = 'World in 3km, platecarree'\nproj_id = 'eqc'\nprojection = {'proj': 'eqc', 'ellps': 'WGS84'}\nwidth = 820\nheight = 410\narea_extent = (-20037508.3428, -10018754.1714, 20037508.3428, 10018754.1714)\narea_def = AreaDefinition(area_id, description, proj_id, projection,\n                          width, height, area_extent)\n\n#scene \nlons, lats = scn[\"total_ozone\"].area.get_lonlats()\nswath_def = pyresample.geometry.SwathDefinition(lons, lats)\ntotal_ozone = scn[\"total_ozone\"].data.compute()\nresult = resample_nearest(swath_def, total_ozone, area_def, radius_of_influence=20000, fill_value=None)\n\n#cartopy\nplt.rcParams['figure.figsize'] = [15, 15]\ncrs = area_def.to_cartopy_crs()\nfig, ax = plt.subplots(subplot_kw=dict(projection=crs))\ncoastlines = ax.coastlines()  \nax.set_global()\n\n#plot\nimg = plt.imshow(result, transform=crs, extent=crs.bounds, origin='upper')\n# Calculate (height_of_image / width_of_image)\nim_ratio = result.shape[0]/result.shape[1]\n \n# Plot vertical colorbar\nplt.colorbar(fraction=0.047*im_ratio)\nplt.show()","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#visualize-data","position":19},{"hierarchy":{"lvl1":"Example of Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd","position":0},{"hierarchy":{"lvl1":"Example of Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data"},"content":"\n\nAuthor: EUMETSATCopyright: 2024 EUMETSATLicence: MIT\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd","position":1},{"hierarchy":{"lvl1":"Example of Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#example-of-using-hda-to-find-and-download-data-for-urban-area-monitoring-using-sentinel-1-data","position":2},{"hierarchy":{"lvl1":"Example of Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data"},"content":"In this notebook, we will present a simple example of how you can access data from DEDL using HDA and what you can do with it. We will demonstrate how to utilize thresholding techniques and compare the values of VV and VH polarizations to analyze urban areas. As an illustration, we will attempt to download Sentinel-1 images containing data of the urban area of Warsaw (Poland).\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#example-of-using-hda-to-find-and-download-data-for-urban-area-monitoring-using-sentinel-1-data","position":3},{"hierarchy":{"lvl1":"1. Prerequisites"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-prerequisites","position":4},{"hierarchy":{"lvl1":"1. Prerequisites"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-prerequisites","position":5},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.1 DestinE account"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-1-destine-account","position":6},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.1 DestinE account"},"content":"Firstly, to work with HDA we will need account on DestinE Core Service Platfrom website. You can register under this url: \n\nhttps://​platform​.destine​.eu/\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-1-destine-account","position":7},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.2 Libraries"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-2-libraries","position":8},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.2 Libraries"},"content":"\n\nimport datetime\nimport requests\nimport numpy as np\nimport rasterio\nimport matplotlib.pyplot as plt\nimport requests\nimport zipfile\nfrom getpass import getpass\nimport io\nfrom rasterio.mask import mask\nimport os\nimport destinelab as deauth\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-2-libraries","position":9},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.3 Prerequisites data"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-3-prerequisites-data","position":10},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.3 Prerequisites data"},"content":"Before reuqesting some data from DEDL, let’s specify what data we want to obtain. We will define 3 variables:\n\nStart date and end date,\n\nOutput directory,\n\nGeometry of interesting us area\n\nStart date and end date will define our timerange in reuqest. HDA will search only for products that were obtained between those two dates.\n\nOutput directory will define directory for downloaded products.\n\nGeometry wll define our area of interest. It will be passed as BBOX (Bounding Box), as a list of coordinates - Xmin, Ymin, Xmax, Ymax. All coordinates will be defined in EPSG:4326.\n\n# Timerange of data that we want to recieve\nstart_date = '2023-07-01'\nend_date = '2023-07-07'\n# Output directory of our desired data\noutput_dir = 'sen_1/'\n# Geometry in form of a BBOX\nbbox = [20.8510, 52.0976, 21.2712, 52.3345,]\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-3-prerequisites-data","position":11},{"hierarchy":{"lvl1":"2. Work with HDA"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-work-with-hda","position":12},{"hierarchy":{"lvl1":"2. Work with HDA"},"content":"HDA (Harmonized Data Access) uses STAC protocol, that allows its user access the Earth Observation data, stored in various provides. Thanks to that, HDA serves as an one stream of data, allowing for comfortable work with sattelite imagery.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-work-with-hda","position":13},{"hierarchy":{"lvl1":"2.1 API URLs"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-1-api-urls","position":14},{"hierarchy":{"lvl1":"2.1 API URLs"},"content":"HDA, as all API, is build upon many endpoints. In this notebook we will use only one for collections and searching. Below there are definitions of those endpoints. We will be using the common one site, but you can change it to central, lumi or leonardo if you want.\n\nCOLLECTIONS_URL = 'https://hda.data.destination-earth.eu/stac/collections'\nSEARCH_URL = 'https://hda.data.destination-earth.eu/stac/search'\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-1-api-urls","position":15},{"hierarchy":{"lvl1":"2.2 Listing available collections"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-2-listing-available-collections","position":16},{"hierarchy":{"lvl1":"2.2 Listing available collections"},"content":"Firstly lets see to what collections we can get access, while using HDA.\n\ndef get_stac_collections(api_url):\n    response = requests.get(api_url)\n    if response.status_code == 200:\n        stac_data = response.json()['collections']\n        collections = [x['id'] for x in stac_data]\n        return collections\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n    \nget_stac_collections(COLLECTIONS_URL)\n\nAs you can see, there are many dataset, that can be access using just one single tool - HDA. In this notebook we will use only Sentinel-1 (GRDH) images, so our collection will be EO.ESA.DAT.SENTINEL-1.L1_GRD.\n\ncollections = ['EO.ESA.DAT.SENTINEL-1.L1_GRD']\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-2-listing-available-collections","position":17},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.3 Authorization"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-3-authorization","position":18},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.3 Authorization"},"content":"As stated before, to use HDA you will need an account on DestinE. Using your credentials, you will be able to generate access token, that will be needed in upcoming requests. In listing collections’ cell you didn’t have to create token, because only more advanced requests (like listing, searching and downloading items) need it.\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-3-authorization","position":19},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.4 Find newest product"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-4-find-newest-product","position":20},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.4 Find newest product"},"content":"After defining all prerequired data and obtaining access token, we can start searching for interesting us products. To do that, we will firstly create body of a POST request with ours parameters. Then, we will send it to HDA and, if request is successful, we will read from response download link.\n\ndef search_items(access_token: str, search_url: str, collection: str, \n                 bbox: list[float | int], start_date: str, end_date: str):\n    body = {\n        'datetime': f'{start_date}T00:00:00Z/{end_date}T23:59:59Z',\n        'collections': [collection],\n        'bbox': bbox\n    }\n    response = requests.post(search_url, json=body, headers={'Authorization': 'Bearer {}'.format(access_token)})\n    if response.status_code != 200:\n        print(f'Error in search request: {response.status_code} - {response.text}')\n        return None\n    else:\n        print(\"Request successful! Reading data...\")\n        products_list = [(feature.get('assets').get('downloadLink').get('href'), feature.get('links')[0].get('title')) for feature in response.json().get('features', [])]\n        return products_list\n\ncollections_items = []\nfor c in collections:\n    collections_items.append(search_items(access_token, SEARCH_URL, c, bbox, start_date, end_date))\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-4-find-newest-product","position":21},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.5 Download founded images"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-5-download-founded-images","position":22},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.5 Download founded images"},"content":"After obtaining URLs for each interesting us product, we can download it with one request. Product will be downloaded compressed in zip format.\n\ndef hda_download(access_token: str, url: str, output: str):\n    response = requests.get(url,stream=True,headers={'Authorization': 'Bearer {}'.format(access_token), 'Accept-Encoding': None})\n    if response.status_code == 200:\n        print('Downloading dataset...')\n        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n            z.extractall(output)\n        print('The dataset has been downloaded to: {}'.format(output))\n    else:\n        print('Request Unsuccessful! Error-Code: {}'.format(response.status_code))\n\nfor collection in collections_items:\n    for item in collection:\n        url = item[0]\n        product_id = item[1]\n        download_path = output_dir + product_id\n        hda_download(access_token, url, download_path)\n        break\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-5-download-founded-images","position":23},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-urban-areas-extraction-by-tresholding","position":24},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding"},"content":"The analysis of Synthetic Aperture Radar (SAR) images enables the detection of variations in radar wave reflections, which is crucial for identifying urbanized areas. SAR images exhibit distinct reflection patterns that allow for the delineation of urban areas, including agglomerations and smaller urban settlements. Urbanized areas stand out in these images primarily due to the intense reflection of radar waves by man-made structures, roads, and other artificial elements of infrastructure, facilitating their clear identification contrasted with natural terrain. Such analysis of areas like the Warsaw agglomeration can serve for monitoring city growth and development, urban planning, assessment of natural and anthropogenic threats, and environmental management. The image clearly depicts the Warsaw agglomeration area, adjacent to the region between the Warsaw and Łódź agglomerations. This space is currently planned for development to connect both agglomerations. In the central part of the frame, there are plans for establishing a large airport complex.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-urban-areas-extraction-by-tresholding","position":25},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.1 Reading and preprocessing data"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-1-reading-and-preprocessing-data","position":26},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.1 Reading and preprocessing data"},"content":"The first step in our analysis will be data reading. The data will be read using the rasterio library from the output folder path provided in the previous section.\n\n# Function to find files containing specific substrings (\"vv\" and \"vh\") in their names\ndef find_files_with_names(folder_path, *names):\n    vv_path = None  \n    vh_path = None  \n    \n    # Traverse the directory tree starting at folder_path\n    for root, dirs, files in os.walk(folder_path):\n        if dirs:  # Check if there are subdirectories\n            first_subfolder = dirs[0]\n            measurement_folder = os.path.join(root, first_subfolder, \"measurement\")\n            \n            if os.path.isdir(measurement_folder):\n                # Look through the files in the \"measurement\" folder\n                for file_name in os.listdir(measurement_folder):\n                    if \"vv\" in file_name:\n                        vv_path = os.path.join(measurement_folder, file_name)\n                    elif \"vh\" in file_name:\n                        vh_path = os.path.join(measurement_folder, file_name)\n                break  # Exit after finding the files\n    return vv_path, vh_path\n\n# Find the \"vv\" and \"vh\" files in the specified directory\ninput_file_vv, input_file_vh = find_files_with_names(output_dir, \"vv\", \"vh\")\n\n# Open the \"vv\" file and read its data and profile\nwith rasterio.open(input_file_vv) as src_vv:\n    vv_band = src_vv.read(1)\n    profile_vv = src_vv.profile\n\n# Open the \"vh\" file and read its data and profile\nwith rasterio.open(input_file_vh) as src_vh:\n    vh_band = src_vh.read(1)\n    profile_vh = src_vh.profile\n\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-1-reading-and-preprocessing-data","position":27},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.2 Pixel value covertion to decibels"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-2-pixel-value-covertion-to-decibels","position":28},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.2 Pixel value covertion to decibels"},"content":"After reading the data, we can convert the pixel values to decibels. Data from Sentinel-1 is converted to the decibel scale (dB) to better visualize differences in the intensity of radar wave reflection, which aids in the analysis of urban areas due to their specific reflection characteristics.\n\ndef convert_to_db(image):\n    # Replace zeros with a small positive value to avoid taking log of zero\n    image[image == 0] = np.finfo(float).eps  \n\n    # Calculate dB values\n    image_db = 10 * np.log10(image)\n\n    # Replace infinite values with the maximum finite value\n    max_value = np.nanmax(image_db[np.isfinite(image_db)])  \n    image_db[np.isinf(image_db)] = max_value  \n\n    return image_db\n\nvv_band_db = convert_to_db(vv_band)\nvh_band_db = convert_to_db(vh_band)\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-2-pixel-value-covertion-to-decibels","position":29},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.3 Data analysis and tresholding"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-3-data-analysis-and-tresholding","position":30},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.3 Data analysis and tresholding"},"content":"The next step involves plotting histograms of the decibel pixel values for both the VV and VH bands. These histograms provide insights into the distribution of radar reflection intensities, aiding in understanding the characteristics of the observed area. The chosen thresholds, 23.5 dB for VV and 21.5 dB for VH, are selected based on prior knowledge or analysis requirements specific to urban areas to segment regions of interest in the images. The thresholding method applied here converts pixel values above the threshold to 1 and those below to 0, aiding in the delineation of features or areas with specific radar reflection characteristics typical of urban environments.\n\nfig, axs = plt.subplots(1, 2, figsize=(18, 5))\n\n# Histogram of VV Image in dB\naxs[0].hist(vv_band_db.flatten(), bins=50, color='blue', alpha=0.7, label='VV dB')\naxs[0].set_title('Histogram of VV Image in dB')\naxs[0].set_xlabel('Radar Reflection Value (dB)')\naxs[0].set_ylabel('Number of Pixels')\naxs[0].legend()\naxs[0].grid(True, linestyle='--', alpha=0.5)\n\n# Histogram of VH Image in dB\naxs[1].hist(vh_band_db.flatten(), bins=50, color='red', alpha=0.7, label='VH dB')\naxs[1].set_title('Histogram of VH Image in dB')\naxs[1].set_xlabel('Radar Reflection Value (dB)')\naxs[1].set_ylabel('Number of Pixels')\naxs[1].legend()\naxs[1].grid(True, linestyle='--', alpha=0.5)\n\nplt.show()\n\n# Function to apply a threshold to an image\n# Pixels greater than the threshold are set to 1, others to 0\ndef thresholding(image, threshold):\n    return (image > threshold).astype(np.uint8)\n\n# Define threshold values for VV and VH bands\nthreshold_value_vv = 23.5\nthreshold_value_vh = 21.5\n\n# Apply thresholding to the VV and VH band\nvv_band_threshold = thresholding(vv_band_db, threshold_value_vv)\nvh_band_threshold = thresholding(vh_band_db, threshold_value_vh)\n\n# Combine the thresholded images using a logical AND operation\n# The result is 1 where both VV and VH are above their respective thresholds\ncombined_threshold = np.logical_and(vv_band_threshold, vh_band_threshold).astype(np.uint8)\n\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-3-data-analysis-and-tresholding","position":31},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.4 Images ploting"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-4-images-ploting","position":32},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.4 Images ploting"},"content":"Next on the subplots we can visualize the differences in radio signal reflection for various polarization types. By displaying the original and decibel-transformed images for both VV and VH polarizations, we can observe distinct features and intensities in urban areas. Furthermore, by combining information from both polarizations through thresholding, we enhance our ability to mask urbanized regions, providing a more comprehensive understanding of the observed area’s urban characteristics.\n\n# Create a figure with a 3x2 grid of subplots, setting the overall figure size\nfig, ax = plt.subplots(3, 2, figsize=(18, 18)) \n\n# Display the original VV image\nax[0, 1].imshow(vv_band, cmap='viridis')\nax[0, 1].set_title('Original VV Image')\n\n# Display the VV image in decibels\nax[1, 1].imshow(vv_band_db, cmap='viridis')\nax[1, 1].set_title('VV Image in dB')\n\n# Display the VV image after thresholding\nax[2, 1].imshow(vv_band_threshold, cmap='inferno')\nax[2, 1].set_title('VV Image after Thresholding')\n\n# Display the original VH image\nax[0, 0].imshow(vh_band, cmap='viridis')\nax[0, 0].set_title('Original VH Image')\n\n# Display the VH image in decibels\nax[1, 0].imshow(vh_band_db, cmap='viridis')\nax[1, 0].set_title('VH Image in dB')\n\n# Display the VH image after thresholding\nax[2, 0].imshow(vh_band_threshold, cmap='inferno')\nax[2, 0].set_title('VH Image after Thresholding')\n\n# Turn off the axes for all subplots to improve the visual presentation\nfor i in range(2):\n    for j in range(3):\n        ax[j, i].axis('off')\n\n# Show the complete figure\nplt.show()\n\n# Display the combined threshold image using a specific colormap\nplt.figure(figsize=(18, 9))  \nplt.imshow(combined_threshold, cmap='twilight_shifted')\nplt.title('Combined Threshold Image')  \nplt.axis('off')  \nplt.show()\n\nThis method of data analysis facilitates straightforward masking of urbanized areas. In the processed image, the Warsaw agglomeration is clearly visible on the right side. A star-shaped accumulation of population is apparent in Warsaw along major transportation corridors such as railways and expressways. Smaller urban centers are visible in the image. Furthermore, it can be observed that the number of masked pixels accumulates as we approach the city center, indicating denser urban development in the downtown area compared to the suburbs.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-4-images-ploting","position":33},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.5 Cuantitive analysis"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-5-cuantitive-analysis","position":34},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.5 Cuantitive analysis"},"content":"After processing our satellite images, we can conduct quantitative analysis. The characteristics of the satellite data enable us to analyze individual pixel values, which is crucial for detailed and statistical studies. In our case, having the data thresholded and the size of each pixel being 10x10 meters, we can calculate the percentage and the adequate size of the urbanized area.\n\n# Calculate the number of urban pixels by summing up the binary values in the combined threshold image\nurban_pixels = np.sum(combined_threshold)\n\n# Define the area covered by a single pixel (in square meters)\npixel_area = 10 * 10  # Assuming each pixel represents a 10m x 10m area\n\n# Calculate the total urban area in square meters\nurban_area = urban_pixels * pixel_area\n\n# Calculate the total number of pixels in the combined threshold image\ntotal_pixels = combined_threshold.size\n\n# Calculate the percentage of urbanized area in the image\nurban_percentage = (urban_pixels / total_pixels) * 100\n\n# Convert urban area from square meters to hectares\nurban_area_hectares = urban_area / 10_000  \n\n# Convert total area from square meters to hectares\ntotal_area_hectares = (total_pixels * pixel_area) / 10_000\n\nprint(f\"Percentage of Urbanized Area: {urban_percentage:.2f}%\")\nprint(f\"Urbanized Area: {urban_area_hectares:.2f} ha\")\nprint(f\"Total Image Area: {total_area_hectares:.2f} ha\")\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-5-cuantitive-analysis","position":35},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.6 Results download"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-6-results-download","position":36},{"hierarchy":{"lvl1":"4. Urban areas extraction by tresholding","lvl2":"4.6 Results download"},"content":"\n\n# Define output file names\noutput_file_db_vv = 'vv_db_urban_warsaw.tif'\noutput_file_db_vh = 'vh_db_urban_warsaw.tif'\noutput_file_threshold_combined = 'warsaw_urban_threshold_combined.tif'\n\n# Update the data type in the profile to float32 for dB conversion\nprofile_vv.update(dtype=rasterio.float32)\nprofile_vh.update(dtype=rasterio.float32)\n\n# Write the result data in dB to the output file\nwith rasterio.open(output_file_db_vv, 'w', **profile_vv) as dst:\n    dst.write(vv_band_db.astype(np.float32), 1)\n    \nwith rasterio.open(output_file_db_vh, 'w', **profile_vh) as dst:\n    dst.write(vh_band_db.astype(np.float32), 1)\n\n# Update the data type in the VV profile to uint8 for the combined thresholded image\nprofile_vv.update(dtype=rasterio.uint8)\n\n# Write the combined thresholded image data to the output file\nwith rasterio.open(output_file_threshold_combined, 'w', **profile_vv) as dst:\n    dst.write(combined_threshold, 1)","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-6-results-download","position":37},{"hierarchy":{"lvl1":"Example of how to use HDA to find and download data for conducting monitoring of Śniadrwy lake"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a","position":0},{"hierarchy":{"lvl1":"Example of how to use HDA to find and download data for conducting monitoring of Śniadrwy lake"},"content":"\n\nAuthor: EUMETSATCopyright: 2024 EUMETSATLicence: MIT\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a","position":1},{"hierarchy":{"lvl1":"Example of how to use HDA to find and download data for conducting monitoring of Śniadrwy lake"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#example-of-how-to-use-hda-to-find-and-download-data-for-conducting-monitoring-of-niadrwy-lake","position":2},{"hierarchy":{"lvl1":"Example of how to use HDA to find and download data for conducting monitoring of Śniadrwy lake"},"content":"In this notebook, we will present a simple example on how you can access data from DEDL using HDA and what you can do with it. As an example, we will try to download Sentinel-2 images containining data of Śniadrwy lake from first week of July 2023. With usage of HDA and few Python packages, you will be able to obtain rasters with NDWI index.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#example-of-how-to-use-hda-to-find-and-download-data-for-conducting-monitoring-of-niadrwy-lake","position":3},{"hierarchy":{"lvl1":"1. Prerequisites"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-prerequisites","position":4},{"hierarchy":{"lvl1":"1. Prerequisites"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-prerequisites","position":5},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.1 DestinE account"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-1-destine-account","position":6},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.1 DestinE account"},"content":"Firstly, to work with HDA we will need account on DestinE Core Service Platfrom website. You can register under this url: \n\nhttps://​platform​.destine​.eu/\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-1-destine-account","position":7},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.2 Python’s packages"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-2-pythons-packages","position":8},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.2 Python’s packages"},"content":"\n\nimport requests\nimport zipfile\nimport io\nimport destinelab as deauth\nfrom getpass import getpass\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-2-pythons-packages","position":9},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.3 Prerequiared data"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-3-prerequiared-data","position":10},{"hierarchy":{"lvl1":"1. Prerequisites","lvl2":"1.3 Prerequiared data"},"content":"Before reuqesting some data from DEDL HDA, let’s specify what data we want to obtain. We will define 3 variables:\n\nStart date and end date,\n\nOutput directory,\n\nGeometry of interesting us area\n\nStart date and end date will define our timerange in reuqest. HDA will search only for products that were obtained between those two dates.\n\nOutput directory will define directory for downloaded products.\n\nGeometry wll define our area of interest. It will be passed as BBOX (Bounding Box), as a list of coordinates - Xmin, Ymin, Xmax, Ymax. All coordinates will be defined in EPSG:4326.\n\n# Timerange of data that we want to recieve\nstart_date = '2023-07-01'\nend_date = '2023-07-07'\n# Output directory of our desired data\noutput_dir = 'output/'\n# Geometry in form of a BBOX\nbbox = [21.61868,53.66627,21.90926,53.82351]\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-3-prerequiared-data","position":11},{"hierarchy":{"lvl1":"2. Work with HDA"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-work-with-hda","position":12},{"hierarchy":{"lvl1":"2. Work with HDA"},"content":"HDA (Harmonized Data Access) uses STAC protocol, that allows its user access the Earth Observation data, stored in various provides. Thanks to that, HDA serves as an one stream of data, allowing for comfortable work with sattelite imagery.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-work-with-hda","position":13},{"hierarchy":{"lvl1":"2.1 API URLs"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-1-api-urls","position":14},{"hierarchy":{"lvl1":"2.1 API URLs"},"content":"HDA, as all API, is build upon many endpoints. In this notebook we will use only one for collections and searching. Below there are definitions of those endpoints. We will be using the common one site, but you can change it to central, lumi or leonardo if you want.\n\nCOLLECTIONS_URL = 'https://hda.data.destination-earth.eu/stac/collections'\nSEARCH_URL = 'https://hda.data.destination-earth.eu/stac/search'\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-1-api-urls","position":15},{"hierarchy":{"lvl1":"2.2 Listing available collections"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-2-listing-available-collections","position":16},{"hierarchy":{"lvl1":"2.2 Listing available collections"},"content":"Firstly lets see to which collections we can get access, while using HDA.\n\ndef get_stac_collections(api_url):\n    response = requests.get(api_url)\n    if response.status_code == 200:\n        stac_data = response.json()['collections']\n        collections = [x['id'] for x in stac_data]\n        return collections\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n    \nget_stac_collections(COLLECTIONS_URL)\n\nAs you can see, there are many dataset, that can be access using just one single tool - HDA. In this notebook we will use only Sentinel-2 images, so our collections will be EO.ESA.DAT.SENTINEL-2.MSI.L1C and EO.ESA.DAT.SENTINEL-2.MSI.L2A.\n\ncollections = ['EO.ESA.DAT.SENTINEL-2.MSI.L1C', 'EO.ESA.DAT.SENTINEL-2.MSI.L2A']\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-2-listing-available-collections","position":17},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.3 Authorization"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-3-authorization","position":18},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.3 Authorization"},"content":"As stated before, to use HDA you will need an account on DestinE. Using your credentials, you will be able to generate access token, that will be needed in upcoming requests. In listing collections’ cell you didn’t have to create token, because only more advanced requests (like listing, searching and downloading items) need it.\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-3-authorization","position":19},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.4 Find newest product"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-4-find-newest-product","position":20},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.4 Find newest product"},"content":"After defining all prerequired data and obtaining access token, we can start searching for interesting us products. To do that, we will firstly create body of a POST request with ours parameters. Then, we will send it to HDA and, if request is successful, we will read from response download link.\n\ndef search_items(access_token: str, search_url: str, collection: str, \n                 bbox: list[float | int], start_date: str, end_date: str):\n    body = {\n        'datetime': f'{start_date}T00:00:00Z/{end_date}T23:59:59Z',\n        'collections': [collection],\n        'bbox': bbox\n    }\n    response = requests.post(search_url, json=body, headers={'Authorization': 'Bearer {}'.format(access_token)})\n    if response.status_code != 200:\n        print(f'Error in search request: {response.status_code} - {response.text}')\n        return None\n    else:\n        print(\"Request successful! Reading data...\")\n        products_list = [(feature.get('assets').get('downloadLink').get('href'), feature.get('links')[0].get('title')) for feature in response.json().get('features', [])]\n        return products_list\n\nTo obtain products from two levels of Sentinel-2 - L2A and L1C, we will use loop, iterating over every single collection.\n\ncollections_items = []\nfor c in collections:\n    collections_items.append(search_items(access_token, SEARCH_URL, c, bbox, start_date, end_date))\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-4-find-newest-product","position":21},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.5 Download founded images"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-5-download-founded-images","position":22},{"hierarchy":{"lvl1":"2.2 Listing available collections","lvl2":"2.5 Download founded images"},"content":"After obtaining download links for each of interesting us product, we can finally download it. With single request, we will download compressed product in zip format to provided directory. Function will set filename as product’s id. Mind that Sentinel-2 products might be over 1 GB, so it may take a few minutes to download them, based on your internet connection.\n\ndef hda_download(access_token: str, url: str, output: str):\n    response = requests.get(url,stream=True,headers={'Authorization': 'Bearer {}'.format(access_token), 'Accept-Encoding': None})\n    if response.status_code == 200:\n        print('Downloading dataset...')\n        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n            z.extractall(output)\n        print('The dataset has been downloaded to: {}'.format(output))\n    else:\n        print('Request Unsuccessful! Error-Code: {}'.format(response.status_code))\n\nFrom previous section, we obtained 2D list - with one dimension being collection and second being one item (single product). Becouse of that, we will use two loops to iterate over single products.\n\nfor collection in collections_items:\n    for item in collection:\n        url = item[0]\n        product_id = item[1]\n        download_path = output_dir + product_id\n        hda_download(access_token, url, download_path)\n        break\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-5-download-founded-images","position":23},{"hierarchy":{"lvl1":"4. Simple data computing - obtaining NDWI"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-simple-data-computing-obtaining-ndwi","position":24},{"hierarchy":{"lvl1":"4. Simple data computing - obtaining NDWI"},"content":"In this chapter we will conduct simple data computing. As stated before, this notebook concentrate on monitoring of Śniadrwy lake, so we will try to calculate NDWI index for each pixel and create raster from it. Using all downloaded items, we will be able to monitor lake status from entire month.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-simple-data-computing-obtaining-ndwi","position":25},{"hierarchy":{"lvl1":"4. Simple data computing - obtaining NDWI","lvl2":"4.1 Libraries"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-1-libraries","position":26},{"hierarchy":{"lvl1":"4. Simple data computing - obtaining NDWI","lvl2":"4.1 Libraries"},"content":"In this chapter we will try to compute obtained by us imagery data, with usage of Python and its spatial-oriented packages.\n\nimport rasterio\nfrom osgeo import gdal, gdal_array, osr\nimport numpy as np\nimport os\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-1-libraries","position":27},{"hierarchy":{"lvl1":"4. Simple data computing - obtaining NDWI","lvl2":"4.2 Functions for reading, calculating and saving raster data"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-2-functions-for-reading-calculating-and-saving-raster-data","position":28},{"hierarchy":{"lvl1":"4. Simple data computing - obtaining NDWI","lvl2":"4.2 Functions for reading, calculating and saving raster data"},"content":"Here we present you some functions for reading raster data into Numpy matrix, calculating NDWI with NIR and GREEN matrixes and saving result as a new raster. We will conduct such calculation for each downloaded item. In the end, we will obtain NDWI data on Śniadrwy Lake from whole week.\n\ndef getFullPath(dir: str, resolution: int, band: str):\n    if not os.path.isdir(dir):\n        raise ValueError(f\"Provided path does not exist: {dir}\")\n    elif resolution not in [10,20,60]:\n        raise ValueError(f\"Provided resolution does not exist: R{resolution}m\")\n    else:\n        full_path = dir\n        while True:\n            content = os.listdir(full_path)\n            if len(content) == 0:\n                raise ValueError(f\"Directory empty: {full_path}\")\n            elif len(content) == 1:\n                if full_path[-1] != '/':\n                    full_path = full_path + '/' + content[0]\n                else:\n                    full_path = full_path + content[0]\n            else:\n                if 'GRANULE' in content:\n                    full_path = full_path + '/' + 'GRANULE'\n                    break\n                else:\n                    raise ValueError(f\"Unsupported dir architecture: {full_path}\")\n        full_path = full_path + '/' + os.listdir(full_path)[0]\n        full_path = full_path + '/' + \"IMG_DATA\"\n        if len(os.listdir(full_path)) == 3:\n            full_path = full_path + '/' + f'R{resolution}m'\n            images = os.listdir(full_path)\n            for img in images:\n                if band in img:\n                    return full_path + '/' + img\n            raise ValueError(f'No such band {band} in directory: {full_path}')\n        else:\n            images = os.listdir(full_path)\n            for img in images:\n                if band in img:\n                    return full_path + '/' + img\n            raise ValueError(f'No such band {band} in directory: {full_path}')\n\n# Get transformation matrix from raster\ndef getTransform(pathToRaster):\n    dataset = gdal.Open(pathToRaster)\n    transformation = dataset.GetGeoTransform()\n    return transformation\n\n# Read raster and return pixels' values matrix as int16, new transformation matrix, crs\ndef readRaster(path, resolution, band):\n    path = getFullPath(path, resolution, band)\n    trans = getTransform(path) # trzeba zdefiniować który kanał\n    raster, crs = rasterToMatrix(path)\n    return raster.astype(np.int16), crs, trans\n\ndef rasterToMatrix(pathToRaster):\n    with rasterio.open(pathToRaster) as src:\n        matrix = src.read(1)\n    return matrix, src.crs.to_epsg()\n\n# Transform numpy's matrix to geotiff; pass new raster's filepath, matrix with pixels' values, gdal file type, transformation matrix, projection, nodata value\ndef npMatrixToGeotiff(filepath, matrix, gdalType, projection, transformMatrix, nodata = None):\n    driver = gdal.GetDriverByName('Gtiff')\n    if len(matrix.shape) > 2:\n        (bandNr, yRes, xRes) = matrix.shape\n        image = driver.Create(filepath, xRes, yRes, bandNr, gdalType)\n        for b in range(bandNr):\n            b = b + 1\n            band = image.GetRasterBand(b)\n            if nodata is not None:\n                band.SetNoDataValue(nodata)\n            band.WriteArray(matrix[b-1,:,:])\n            band.FlushCache\n    else:\n        bandNr = 1\n        (yRes, xRes) = matrix.shape\n        image = driver.Create(filepath, xRes, yRes, bandNr, gdalType)\n        print(type(image))\n        band = image.GetRasterBand(bandNr)\n        if nodata is not None:\n            band.SetNoDataValue(nodata)\n        band.WriteArray(matrix)\n        band.FlushCache\n    image.SetGeoTransform(transformMatrix)\n    image.SetProjection(projection)\n    del driver, image, band\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-2-functions-for-reading-calculating-and-saving-raster-data","position":29},{"hierarchy":{"lvl1":"4. Simple data computing - obtaining NDWI","lvl2":"4.3 Computing"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-3-computing","position":30},{"hierarchy":{"lvl1":"4. Simple data computing - obtaining NDWI","lvl2":"4.3 Computing"},"content":"With usage of defined functions, we will now generate NWDI rasters. Only data that will be needed in this step is a list with paths to our products (extracted from zip archive). Function readRaster will choose specified band from specified path.\n\n# List of products' paths. If your output directory contains more than just downloaded products, please provide them in a list, just like in the commented lines below\n#dataset = [output_dir+x for x in os.listdir(output_dir)]\ndataset = [\n     'output/S2B_MSIL1C_20230701T094549_N0509_R079_T34UEE_20230701T104205'\n ]\n# Output directiry for new images\ncompution_output = 'output/ndwi_rasters'\n\n# Iterating over single product\nfor item in dataset:\n    # Reading name from path\n    name = item.split('/')[-1]\n    # Reading green band into matrix\n    green = readRaster(item, 10, 'B03')\n    # Reading NIR band into matrix\n    nir = readRaster(item, 10, 'B08')\n    # Calculating NDWI matrix\n    ndwi = (green[0]-nir[0]) / (green[0]+nir[0])\n    # Setting treshhold for water-containing pixels\n    ndwi[ndwi >= 0] = 1\n    ndwi[ndwi < 0] = 0\n    # Creating SpatialReference object and setting it to match original's raster CRS\n    projection = osr.SpatialReference()\n    projection.ImportFromEPSG(green[1])\n    # Creating raster from matrix in GeoTiff format\n    npMatrixToGeotiff(f'{compution_output}/{name}.tif', ndwi, gdal_array.NumericTypeCodeToGDALTypeCode(np.float32), projection.ExportToWkt(), green[2])\n\nAfter successfuly creating and saving new images, we can now visualize them in Python using raterio package.\n\nimg = rasterio.open('output/ndwi_rasters/S2B_MSIL1C_20230701T094549_N0509_R079_T34UEE_20230701T104205.tif')\nfrom rasterio.plot import show\nshow(img)","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-3-computing","position":31},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction"},"type":"lvl1","url":"/hda-pystac-client","position":0},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT ","type":"content","url":"/hda-pystac-client","position":1},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction"},"type":"lvl1","url":"/hda-pystac-client#hda-pystac-client-introduction","position":2},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction"},"content":"This notebook shows the basic use of DestinE Data Lake Harmonised Data Access using pystac-client.\nIt will include iterating through Collections and Items, and perform simple spatio-temporal searches.\n\n","type":"content","url":"/hda-pystac-client#hda-pystac-client-introduction","position":3},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl2":"Obtain DEDL Access Token to use the HDA service"},"type":"lvl2","url":"/hda-pystac-client#obtain-dedl-access-token-to-use-the-hda-service","position":4},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl2":"Obtain DEDL Access Token to use the HDA service"},"content":"\n\npip install --quiet --upgrade destinelab\n\nimport requests\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/hda-pystac-client#obtain-dedl-access-token-to-use-the-hda-service","position":5},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl2":"Set username and password as environment variables to be used for DEDL data access"},"type":"lvl2","url":"/hda-pystac-client#set-username-and-password-as-environment-variables-to-be-used-for-dedl-data-access","position":6},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl2":"Set username and password as environment variables to be used for DEDL data access"},"content":"\n\nimport os\n\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__USERNAME\"] = DESP_USERNAME\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__PASSWORD\"] = DESP_PASSWORD\n\n","type":"content","url":"/hda-pystac-client#set-username-and-password-as-environment-variables-to-be-used-for-dedl-data-access","position":7},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API"},"type":"lvl1","url":"/hda-pystac-client#create-pystac-client-object-for-hda-stac-api","position":8},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API"},"content":"We first connect to an API by retrieving the root catalog, or landing page, of the API with the \n\nClient.open function.\n\nfrom pystac_client import Client\n\nHDA_API_URL = \"https://hda.data.destination-earth.eu/stac\"\ncat = Client.open(HDA_API_URL, headers=auth_headers)\n\n","type":"content","url":"/hda-pystac-client#create-pystac-client-object-for-hda-stac-api","position":9},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API","lvl2":"Query all available collections"},"type":"lvl2","url":"/hda-pystac-client#query-all-available-collections","position":10},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API","lvl2":"Query all available collections"},"content":"As with a static catalog the get_collections function will iterate through the Collections in the Catalog.\nNotice that because this is an API it can get all the Collections through a single call, rather than having to fetch each one individually.\n\nfrom rich.console import Console\nimport rich.table\n\nconsole = Console()\n\nhda_collections = cat.get_collections()\n\ntable = rich.table.Table(title=\"HDA collections\", expand=True)\ntable.add_column(\"ID\", style=\"cyan\", justify=\"right\",no_wrap=True)\ntable.add_column(\"Title\", style=\"violet\", no_wrap=True)\nfor collection in hda_collections:\n    table.add_row(collection.id, collection.title)\nconsole.print(table)\n\n","type":"content","url":"/hda-pystac-client#query-all-available-collections","position":11},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API","lvl2":"Obtain provider information for each individual collection"},"type":"lvl2","url":"/hda-pystac-client#obtain-provider-information-for-each-individual-collection","position":12},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API","lvl2":"Obtain provider information for each individual collection"},"content":"\n\ntable = rich.table.Table(title=\"HDA collections | Providers\", expand=True)\ntable.add_column(\"Title\", style=\"cyan\", justify=\"right\", no_wrap=True)\ntable.add_column(\"Provider\", style=\"violet\", no_wrap=True)\n\nhda_collections = cat.get_collections()\n\nfor collection in hda_collections:\n    collection_details = cat.get_collection(collection.id)\n    provider = ','.join(str(x.name) for x in collection_details.providers)\n    table.add_row(collection_details.title, provider)\nconsole.print(table)\n\n","type":"content","url":"/hda-pystac-client#obtain-provider-information-for-each-individual-collection","position":13},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API","lvl2":"Inspect Items of a Collection"},"type":"lvl2","url":"/hda-pystac-client#inspect-items-of-a-collection","position":14},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API","lvl2":"Inspect Items of a Collection"},"content":"The main functions for getting items return iterators, where pystac-client will handle retrieval of additional pages when needed. Note that one request is made for the first ten items, then a second request for the next ten.\n\ncoll_name = 'EO.ESA.DAT.SENTINEL-1.L1_GRD'\nsearch = cat.search(\n    max_items=10,\n    collections=[coll_name],\n    bbox=[-72.5,40.5,-72,41],\n    datetime=\"2023-09-09T00:00:00Z/2023-09-20T23:59:59Z\"\n)\n\ncoll_items = search.item_collection()\nconsole.print(f\"For collection {coll_name} we found {len(coll_items)} items\")\n\nimport geopandas\n\ndf = geopandas.GeoDataFrame.from_features(coll_items.to_dict(), crs=\"epsg:4326\")\ndf.head()\n\n\n\n","type":"content","url":"/hda-pystac-client#inspect-items-of-a-collection","position":15},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API","lvl2":"Inspect STAC assets of an item"},"type":"lvl2","url":"/hda-pystac-client#inspect-stac-assets-of-an-item","position":16},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API","lvl2":"Inspect STAC assets of an item"},"content":"\n\nimport rich.table\n\nselected_item = coll_items[3]\n\ntable = rich.table.Table(title=\"Assets in STAC Item\")\ntable.add_column(\"Asset Key\", style=\"cyan\", no_wrap=True)\ntable.add_column(\"Description\")\nfor asset_key, asset in selected_item.assets.items():\n    table.add_row(asset_key, asset.title)\n\nconsole.print(table)\n\nfrom IPython.display import Image\n\nImage(url=selected_item.assets[\"thumbnail\"].href, width=500)\n\ndown_uri = selected_item.assets[\"downloadLink\"].href\nconsole.print(f\"Download link of asset is {down_uri}\")\n\n","type":"content","url":"/hda-pystac-client#inspect-stac-assets-of-an-item","position":17},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API","lvl3":"Download asset to JupyterLab","lvl2":"Inspect STAC assets of an item"},"type":"lvl3","url":"/hda-pystac-client#download-asset-to-jupyterlab","position":18},{"hierarchy":{"lvl1":"Create pystac client object for HDA STAC API","lvl3":"Download asset to JupyterLab","lvl2":"Inspect STAC assets of an item"},"content":"\n\nselected_item.id\n\nselected_item.assets[\"downloadLink\"]\n\n# Make http request for remote file data\ndata = requests.get(selected_item.assets[\"downloadLink\"].href,\n                   headers=auth_headers)\nmtype = selected_item.assets[\"downloadLink\"].media_type.split(\"/\")[1]\n# Save file data to local copy\nwith open(f\"{selected_item.id}.{mtype}\", 'wb')as file:\n    file.write(data.content)","type":"content","url":"/hda-pystac-client#download-asset-to-jupyterlab","position":19},{"hierarchy":{"lvl1":"DestinE Harmonised Data Access (HDA)"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"DestinE Harmonised Data Access (HDA)"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"DestinE Harmonised Data Access (HDA)"},"type":"lvl1","url":"/#destine-harmonised-data-access-hda","position":2},{"hierarchy":{"lvl1":"DestinE Harmonised Data Access (HDA)"},"content":" Author: EUMETSAT\n\n\nMaterials to learn how to use Harmonised Data Access API and examples\n\nNotebooks\n\nHow to use the HDA API sending a few HTTP requests, quick-start\n\nHow to use the HDA API sending a few HTTP requests, full-version\n\nHow to use the queryables HDA API to search and access C3S and Digital Twins data\n\nDiscover, access and visualise Digital twins data with HDA\n\nPlot any Climate DT parameter\n\nPlot any Extreme DT parameter\n\nClimate Adaptation Digital Twin data\n\nClimate Adaptation Digital Twin data - timeseries\n\nExtremes event  Digital Twin data - timeseries\n\nExtremes event  Digital Twin data - xarray\n\nExtremes event data availability - Aviso\n\nDiscover, access and visualise federated data with HDA\n\nDiscover Sentinel-3 OLCI products example\n\nDiscover AVHRR Metop products example\n\nDiscover and visualise ERA5 products example\n\nDiscover and access DEDL data using EODAG client\n\nHow to use the EODAG client with DEDL data, quick-start\n\nHow to use the EODAG client with DEDL data, full version\n\nAccess DEDL data using PySTAC client\n\nDiscover and access DEDL data using PySTAC client\n\nFurther information available in DestinE Data Lake documentation: \n\nhttps://​destine​-data​-lake​-docs​.data​.destination​-earth​.eu​/en​/latest​/index​.html\n\nAdditional ressources:\n\nDestinE Data Portfolio: \n\nhttps://​hda​.data​.destination​-earth​.eu​/ui​/catalog\n\nDataLake Priority services: \n\nhttps://​hda​.data​.destination​-earth​.eu​/ui​/services\n\nHDA SWAGGER UI: \n\nhttps://​hda​.data​.destination​-earth​.eu​/docs/\n\nDestinE Core Platform Insula Users\n\nPlease perform the following and select my_env kernel when running the provided Notebooks\n\nOpen a terminal window (File, New, Terminal) and run the following commands in sequence:\n\nCreate a virtual environment: python -m venv /home/jovyan/my_env\n\nActivate it: source /home/jovyan/my_env/bin/activate\n\nInstall required dependencies for this example Notebooks: pip install -r /home/jovyan/datalake-lab/requirements.txt\n\nVerify the installation: pip list | grep destinelab\n\nThis should give:\n\ndestinelab         0.11\n\nInstall kernel my_env. Run the command: python -m ipykernel install --name my_env --user\n\nSelect the kernel my_env from the top-right menu of these notebooks.\n\nUsers who already have a previous version of the ‘my_env’ environment installed, should delete the kernel before running the steps above. To delete the my_env kernel please run the following command: ‘jupyter kernelspec uninstall my_env’ from a terminal window.","type":"content","url":"/#destine-harmonised-data-access-hda","position":3},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - Queryables"},"type":"lvl1","url":"/hda-rest-queryables","position":0},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - Queryables"},"content":"\n\n","type":"content","url":"/hda-rest-queryables","position":1},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - Queryables"},"type":"lvl1","url":"/hda-rest-queryables#dedl-hda-tutorial-queryables","position":2},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - Queryables"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT \n\n","type":"content","url":"/hda-rest-queryables#dedl-hda-tutorial-queryables","position":3},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - Queryables","lvl3":"How to use the queryables API"},"type":"lvl3","url":"/hda-rest-queryables#how-to-use-the-queryables-api","position":4},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - Queryables","lvl3":"How to use the queryables API"},"content":"The queryables API returns a list of variable terms that can be used for filtering the specified collection.\n\nThe queryables API presents the appropriate filters for the selected dataset, determined by the chosen values. If the user selects a certain variable, the choice is narrowed down for other variables. This notebook demonstrates how to filter data in a specific collection using the list of variable terms returned by the queryables API.\n\nThe notebook is dedicated to collections from C3S and DestinE digital twins. C3S and DestinE digital twins datasets offer a wide range of possibilities in terms of information and constraints and the queryables API is a valuable tool for exploring these datasets.\n\nThroughout this notebook, you will learn:\n\nAuthenticate: How to authenticate for searching and access DEDL collections.\n\nQueryables: How to exploit the STAC API filter extension features. The “queryables” API helps users to determine the property names and types available for filtering data.\n\nSearch data:  How to search DEDL data using filters obtained by the “queryables” API.\n\nDownload data: How to download DEDL data through HDA.\n\nThe detailed HDA API and definition of each endpoint and parameters is available in the HDA Swagger UI at:\n\n\n STAC API - Filter Extension \n\nPrequisites:\n\nFor queryables API: none\n\nFor filtering data inside collections : \n\nDestinE user account\n\n","type":"content","url":"/hda-rest-queryables#how-to-use-the-queryables-api","position":5},{"hierarchy":{"lvl1":"Authenticate"},"type":"lvl1","url":"/hda-rest-queryables#authenticate","position":6},{"hierarchy":{"lvl1":"Authenticate"},"content":"","type":"content","url":"/hda-rest-queryables#authenticate","position":7},{"hierarchy":{"lvl1":"Authenticate","lvl2":"Define some constants for the API URLs"},"type":"lvl2","url":"/hda-rest-queryables#define-some-constants-for-the-api-urls","position":8},{"hierarchy":{"lvl1":"Authenticate","lvl2":"Define some constants for the API URLs"},"content":"In this section, we define the relevant constants, holding the URL strings for the different endpoints.\n\n# Collection https://hda.data.destination-earth.eu/ui/dataset/EO.ECMWF.DAT.CAMS_EUROPE_AIR_QUALITY_FORECASTS as default\nCOLLECTION_ID = \"EO.ECMWF.DAT.CAMS_EUROPE_AIR_QUALITY_FORECASTS\"\n\n# Core API\nHDA_API_URL = \"https://hda.data.destination-earth.eu\"\n\n# STAC API\n## Core\nSTAC_API_URL = f\"{HDA_API_URL}/stac\"\n\n## Item Search\nSEARCH_URL = f\"{STAC_API_URL}/search\"\n\n## Collections\nCOLLECTIONS_URL = f\"{STAC_API_URL}/collections\"\n\n## Queryables\nQUERYABLES_URL = f\"{STAC_API_URL}/queryables\"\nQUERYABLES_BY_COLLECTION_ID = f\"{COLLECTIONS_URL}/{COLLECTION_ID}/queryables\"\nHDA_FILTERS =''\n\n## HTTP Success\nHTTP_SUCCESS_CODE = 200\n\n","type":"content","url":"/hda-rest-queryables#define-some-constants-for-the-api-urls","position":9},{"hierarchy":{"lvl1":"Authenticate","lvl2":"Import the relevant modules and define some functions"},"type":"lvl2","url":"/hda-rest-queryables#import-the-relevant-modules-and-define-some-functions","position":10},{"hierarchy":{"lvl1":"Authenticate","lvl2":"Import the relevant modules and define some functions"},"content":"We start off by importing the relevant modules for DestnE authentication, HTTP requests, json handling, widgets and some utility.\n\nimport destinelab as deauth\n\npip install --quiet --upgrade DEDLUtils\n\nfrom DEDLUtils import dedl_utilities\n#import dedl_utilities\n\nimport requests\nimport json\nfrom getpass import getpass\n\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output, HTML\nfrom ipywidgets import Layout, Box\nimport datetime\n\nfrom rich.console import Console\nimport rich.table\n\nfrom IPython.display import JSON\n\n\n","type":"content","url":"/hda-rest-queryables#import-the-relevant-modules-and-define-some-functions","position":11},{"hierarchy":{"lvl1":"Authenticate","lvl3":"Obtain Authentication Token","lvl2":"Import the relevant modules and define some functions"},"type":"lvl3","url":"/hda-rest-queryables#obtain-authentication-token","position":12},{"hierarchy":{"lvl1":"Authenticate","lvl3":"Obtain Authentication Token","lvl2":"Import the relevant modules and define some functions"},"content":"To perform a query on HDA we need to be authenticated.\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/hda-rest-queryables#obtain-authentication-token","position":13},{"hierarchy":{"lvl1":"Queryables"},"type":"lvl1","url":"/hda-rest-queryables#queryables","position":14},{"hierarchy":{"lvl1":"Queryables"},"content":"The “queryables” API helps users to determine the property names and types available for filtering data inside a specific collection.\n\nBelow a dropdown menu to choose the collection. We can choose the collection of which we want to inspect the filters.\n\nprefix = \"EO.ECMWF\"\n\ndedlUtils=dedl_utilities.DEDLUtilities(COLLECTION_ID )\ndedlUtils.create_collections_dropdown(prefix)\n\n# Layout\n# Define the layout for the dropdown\ndropdown_layout = Layout(display='space-between', justify_content='center', width='90%')\n# Create a box to hold the dropdown with the specified layout\nbox = Box([dedlUtils.dropdown, dedlUtils.outputArea], layout=dropdown_layout)\ndisplay( box)  \n\n\n\n\nThe QUERYABLES ENDPOINT (above) returns the applicable filters under the section named ‘properties’.\n\nThe ‘properties’ section contains\n\nthe name of the filter, title,\n\nthe filter type,\n\nthe possible filter values, enum (conditioned by the values selected for the other filters)\n\nand the the default (or chosen) default applied\n\nWe can print the’properties’ section for the selected collection in the table below.\nThe table shows the filters and the values applied by default when we perform a search for the chosen dataset without specifying any filter.\n\nfilters_resp=requests.get(dedlUtils.queryablesByCollectionId)\nfilters_resp.raise_for_status()\n\nfilters = filters_resp.json()[\"properties\"]\ndedlQueryablesUtils=dedl_utilities.DEDLQueryablesUtilities(dedlUtils.collectionId)\nconsole = Console()\nconsole.print(dedlQueryablesUtils.create_queryables_table(filters))\n\nCalling the queryables API specifying filters, that means using as parameters the values chosen for filtering the selected dataset, the API replies with the applicable filters, conditioned by the chosen values.\nThen if the user selects a certain value for a parameter then the choice is narrowed down for other variables.\n\nThe queryables API, in this way, helps user to build a correct search request for the given dataset.\n\nBelow an interactive example, to see that once you select a value for a property the choice is narrowed down for other variables.\n\n# Event listeners\ndedlQueryablesUtils=dedl_utilities.DEDLQueryablesUtilities(dedlUtils.collectionId)\ndedlQueryablesUtils.update_queryables_dropdowns()\ndisplay(dedlQueryablesUtils.dropdownContainer, dedlQueryablesUtils.outputArea)\n\n","type":"content","url":"/hda-rest-queryables#queryables","position":15},{"hierarchy":{"lvl1":"Queryables","lvl2":"Filtering a collection with the list returned by the queryable API"},"type":"lvl2","url":"/hda-rest-queryables#filtering-a-collection-with-the-list-returned-by-the-queryable-api","position":16},{"hierarchy":{"lvl1":"Queryables","lvl2":"Filtering a collection with the list returned by the queryable API"},"content":"This section wil show how to use the list of variable terms returned by the queryables API for filtering a specific dataset.\n\n","type":"content","url":"/hda-rest-queryables#filtering-a-collection-with-the-list-returned-by-the-queryable-api","position":17},{"hierarchy":{"lvl1":"Queryables","lvl4":"If you choose a digital twins collection, check if the access is granted","lvl2":"Filtering a collection with the list returned by the queryable API"},"type":"lvl4","url":"/hda-rest-queryables#if-you-choose-a-digital-twins-collection-check-if-the-access-is-granted","position":18},{"hierarchy":{"lvl1":"Queryables","lvl4":"If you choose a digital twins collection, check if the access is granted","lvl2":"Filtering a collection with the list returned by the queryable API"},"content":"If DT access is not granted, you will not be able to search and access DT data.\n\nauth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/hda-rest-queryables#if-you-choose-a-digital-twins-collection-check-if-the-access-is-granted","position":19},{"hierarchy":{"lvl1":"Queryables","lvl3":"Build the query from the selected values","lvl2":"Filtering a collection with the list returned by the queryable API"},"type":"lvl3","url":"/hda-rest-queryables#build-the-query-from-the-selected-values","position":20},{"hierarchy":{"lvl1":"Queryables","lvl3":"Build the query from the selected values","lvl2":"Filtering a collection with the list returned by the queryable API"},"content":"The parameters chosen in the previous steps can be used to build the corresponding HDA queries.\n\n# The JSON objects containing the generic query parameters:\njsonGenericQuery = '{\"collections\": [\"'+dedlUtils.collectionId+'\"], \"datetime\": \"2024-04-01T00:00:00Z/2024-04-19T00:00:00Z\"}'\n# Convert JSON strings to Python dictionaries\ndictQuery = json.loads(jsonGenericQuery)\n\n# Include the filters selected in the previous steps inside the JSON containing the generic query parameters:\ndictQuery['query'] = dedlQueryablesUtils.hdaFilters\n# Convert the merged dictionary back to a JSON string\nqueryJson = json.dumps(dictQuery, indent=4)\n\nprint(queryJson)\n\n","type":"content","url":"/hda-rest-queryables#build-the-query-from-the-selected-values","position":21},{"hierarchy":{"lvl1":"Queryables","lvl2":"Search"},"type":"lvl2","url":"/hda-rest-queryables#search","position":22},{"hierarchy":{"lvl1":"Queryables","lvl2":"Search"},"content":"\n\nresponse = requests.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json= json.loads(queryJson) )\n#print(response)\n# Requests to ADS data always return a single item containing all the requested data\nproduct = response.json()[\"features\"][0]\nJSON(product, expanded= False)\n\n","type":"content","url":"/hda-rest-queryables#search","position":23},{"hierarchy":{"lvl1":"Download"},"type":"lvl1","url":"/hda-rest-queryables#download","position":24},{"hierarchy":{"lvl1":"Download"},"content":"Once we have found the product we can download it:\n\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nprint(download_url )\n\nfrom tqdm import tqdm\nimport time\nimport re\ndirect_download_url=''\n\nresponse = requests.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelse:\n    print(response.text)\n\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = requests.get(url, headers=auth_headers, stream=True)\n        response.raise_for_status()\n\nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)","type":"content","url":"/hda-rest-queryables#download","position":25},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial"},"type":"lvl1","url":"/hda-rest-full-version","position":0},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial"},"content":"\n\n","type":"content","url":"/hda-rest-full-version","position":1},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial"},"type":"lvl1","url":"/hda-rest-full-version#dedl-hda-tutorial","position":2},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT \n\n","type":"content","url":"/hda-rest-full-version#dedl-hda-tutorial","position":3},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"First steps using Harmonised Data access API"},"type":"lvl3","url":"/hda-rest-full-version#first-steps-using-harmonised-data-access-api","position":4},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"First steps using Harmonised Data access API"},"content":"Discover data of DestinE Data Portfolio\n\nSearch data of DestinE Data Portfolio and visualize the results\n\nAccess Data of DestinE Data Portfolio and visualize the thumbnails\n\nThis notebook demonstrates how to use the HDA (Harmonized Data Access) API by sending a few HTTP requests to the API, using Python code.\n\nThroughout this quickstart notebook, you will learn:\n\nDiscover: How to discover DEDL services and data collections through HDA.\n\nAuthenticate: How to authenticate to search and access DEDL collections.\n\nSearch data:  How to search DEDL data through HDA.\n\nViisualize search results: How to see the results.\n\nDownload data: How to download DEDL data through HDA.\n\nThe detailed API and definition of each endpoint and parameters is available in the HDA Swagger UI at:\n\nhttps://​hda​.data​.destination​-earth​.eu​/docs/\n\nPrerequisites:\n\nFor Data discovery: none\n\nFor Data access : \n\nDestinE user account\n\n","type":"content","url":"/hda-rest-full-version#first-steps-using-harmonised-data-access-api","position":5},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl2":"Discover"},"type":"lvl2","url":"/hda-rest-full-version#discover","position":6},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl2":"Discover"},"content":"\n\n","type":"content","url":"/hda-rest-full-version#discover","position":7},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Settings","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-full-version#settings","position":8},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Settings","lvl2":"Discover"},"content":"","type":"content","url":"/hda-rest-full-version#settings","position":9},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl4":"Import the relevant modules","lvl3":"Settings","lvl2":"Discover"},"type":"lvl4","url":"/hda-rest-full-version#import-the-relevant-modules","position":10},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl4":"Import the relevant modules","lvl3":"Settings","lvl2":"Discover"},"content":"We start off by importing the relevant modules for HTTP requests and json handling.\n\nfrom typing import Union\nimport requests\nimport json\nimport urllib.parse\nfrom IPython.display import JSON\nfrom IPython.display import Image\n\nimport geopandas\nimport folium\nimport folium.plugins\nfrom branca.element import Figure\nimport shapely.geometry\n\n","type":"content","url":"/hda-rest-full-version#import-the-relevant-modules","position":11},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl4":"Define some constants for the API URLs","lvl3":"Settings","lvl2":"Discover"},"type":"lvl4","url":"/hda-rest-full-version#define-some-constants-for-the-api-urls","position":12},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl4":"Define some constants for the API URLs","lvl3":"Settings","lvl2":"Discover"},"content":"In this section, we define the relevant constants, holding the URL strings for the different endpoints.\n\n# IDS\nSERVICE_ID = \"dedl-hook\"\nCOLLECTION_ID = \"EO.EUM.DAT.SENTINEL-3.SL_1_RBT___\"\nITEM_ID = \"S3B_SL_1_RBT____20240918T102643_20240918T102943_20240919T103839_0179_097_336_2160_PS2_O_NT_004\"\n\n# Core API\nHDA_API_URL = \"https://hda.data.destination-earth.eu\"\nSERVICES_URL = f\"{HDA_API_URL}/services\"\nSERVICE_BY_ID_URL = f\"{SERVICES_URL}/{SERVICE_ID}\"\n\n# STAC API\n## Core\nSTAC_API_URL = f\"{HDA_API_URL}/stac\"\nCONFORMANCE_URL = f\"{STAC_API_URL}/conformance\"\n\n## Item Search\nSEARCH_URL = f\"{STAC_API_URL}/search\"\nDOWNLOAD_URL = f\"{STAC_API_URL}/download\"\n\n## Collections\nCOLLECTIONS_URL = f\"{STAC_API_URL}/collections\"\nCOLLECTION_BY_ID_URL = f\"{COLLECTIONS_URL}/{COLLECTION_ID}\"\n\n## Items\nCOLLECTION_ITEMS_URL = f\"{COLLECTIONS_URL}/{COLLECTION_ID}/items\"\nCOLLECTION_ITEM_BY_ID_URL = f\"{COLLECTIONS_URL}/{COLLECTION_ID}/items/{ITEM_ID}\"\n\n## HTTP Success\nHTTP_SUCCESS_CODE = 200\n\n","type":"content","url":"/hda-rest-full-version#define-some-constants-for-the-api-urls","position":13},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Core API","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-full-version#core-api","position":14},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Core API","lvl2":"Discover"},"content":"We can start off by requesting the HDA landing page, which provides links to the API definition, the available services  (links services and service-doc) as well as the STAC API index.\n\nresponse=requests.get(HDA_API_URL)\nJSON(response.json())\n\n","type":"content","url":"/hda-rest-full-version#core-api","position":15},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"STAC API","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-full-version#stac-api","position":16},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"STAC API","lvl2":"Discover"},"content":"The HDA is plugged to a STAC API.\nThe STAC API entry point is set to the /stac endpoint and provides the search capabilities provided by the DEDL STAC interface.\n\nprint(STAC_API_URL)\nJSON(requests.get(STAC_API_URL).json())\n\n","type":"content","url":"/hda-rest-full-version#stac-api","position":17},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Discover DEDL Services","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-full-version#discover-dedl-services","position":18},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Discover DEDL Services","lvl2":"Discover"},"content":"\n\nThe /services endpoint will return the list of the DEDL services available for users of the platform.\n\nprint(SERVICES_URL)\nJSON(requests.get(SERVICES_URL).json())\n\nThrough the /services endpoint is also possible discover services related to a certain topic:\n\nJSON(requests.get(SERVICES_URL,params = {\"q\": \"dask\"}).json())\n\nThe API can also describe a specific service, identified by its serviceID (e.g. dedl-hook).\n\nThe links describes and described by contains the reference documentation.\n\nprint(SERVICE_BY_ID_URL)\nJSON(requests.get(SERVICE_BY_ID_URL).json())\n\n","type":"content","url":"/hda-rest-full-version#discover-dedl-services","position":19},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Discover DEDL data collections","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-full-version#discover-dedl-data-collections","position":20},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Discover DEDL data collections","lvl2":"Discover"},"content":"It is also possible discover data collections related to a certain topic and provided by a certain provider in a specic time interval.\nWe specify an open time interval in order to have collections with data starting from a certain datetime.\n\nresponse = requests.get(COLLECTIONS_URL,params = {\"q\": \"ozone,methane,fire\",\"provider\":\"eumetsat\",\"datetime\":'2024-01-01T00:00:00Z/..'})\n\nJSON(response.json(), expanded=False)\n\n","type":"content","url":"/hda-rest-full-version#discover-dedl-data-collections","position":21},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl2":"Authenticate"},"type":"lvl2","url":"/hda-rest-full-version#authenticate","position":22},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl2":"Authenticate"},"content":"","type":"content","url":"/hda-rest-full-version#authenticate","position":23},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Obtain Authentication Token","lvl2":"Authenticate"},"type":"lvl3","url":"/hda-rest-full-version#obtain-authentication-token","position":24},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Obtain Authentication Token","lvl2":"Authenticate"},"content":"\n\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/hda-rest-full-version#obtain-authentication-token","position":25},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl2":"Search"},"type":"lvl2","url":"/hda-rest-full-version#search","position":26},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl2":"Search"},"content":"\n\n","type":"content","url":"/hda-rest-full-version#search","position":27},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"List Available Collections","lvl2":"Search"},"type":"lvl3","url":"/hda-rest-full-version#list-available-collections","position":28},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"List Available Collections","lvl2":"Search"},"content":"The /stac/collections endpoint returns a FeatureCollection object, listing all STAC collections available to the user.\n\nprint(COLLECTIONS_URL)\nJSON(requests.get(COLLECTIONS_URL).json())\n\nBy providing a specific collectionID (e.g. EO.EUM.DAT.SENTINEL-3.SL_1_RBT___), the user can get the metadata for a specific Collection.\nThe collection used for this tutorial is \n\nSLSTR Level 1B Radiances and Brightness Temperatures - Sentinel-3\n\nprint(COLLECTION_BY_ID_URL)\nJSON(requests.get(COLLECTION_BY_ID_URL).json())\n\n","type":"content","url":"/hda-rest-full-version#list-available-collections","position":29},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Search for Items in a specific collection","lvl2":"Search"},"type":"lvl3","url":"/hda-rest-full-version#search-for-items-in-a-specific-collection","position":30},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Search for Items in a specific collection","lvl2":"Search"},"content":"It is also possible to get the list of items available in a given Collection using a simple search and sorting the results.\n\nFILTER = \"?datetime=2024-09-18T00:00:00Z/2024-09-20T23:59:59Z&bbox=-10,34,-5,42.5&sortby=datetime&limit=5\"\n\nprint(COLLECTION_ITEMS_URL+FILTER)\nresponse=requests.get(COLLECTION_ITEMS_URL+FILTER, headers=auth_headers)  \n\nJSON(response.json())            \n\n","type":"content","url":"/hda-rest-full-version#search-for-items-in-a-specific-collection","position":31},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"The search endpoint","lvl2":"Search"},"type":"lvl3","url":"/hda-rest-full-version#the-search-endpoint","position":32},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"The search endpoint","lvl2":"Search"},"content":"The STAC API also provides an item endpoint (/stac/search).\nThis endpoint allows users to efficiently search for items that match the specified input filters.\n\nBy default, the /stac/search endpoint will return the first 20 items found in all the collections available at the /stac/collections endpoint.\nFilters can be added either via query parameters in a GET request or added to the JSON body of a POST request.\n\nThe full detail for each available filter is available in the \n\nAPI documentation.\n\nThe query parameters are added at the end of the URL as a query string: ?param1=val1&param2=val2&param3=val3\n\nFILTER = \"&datetime=2024-09-18T00:00:00Z/2024-09-20T23:59:59Z&bbox=-10,34,-5,42.5&sortby=datetime&limit=10\"\nSEARCH_QUERY_STRING = \"?collections=\"+COLLECTION_ID+FILTER\nresponse=requests.get(SEARCH_URL + SEARCH_QUERY_STRING, headers=auth_headers)\n\nJSON(response.json())    \n\nThe same filters can be added as the JSON body of a POST request.\n\nBODY = {\n    \"collections\": [\n        COLLECTION_ID,\n    ],\n    \"datetime\" : \"2024-09-18T00:00:00Z/2024-09-20T23:59:59Z\",\n    \"bbox\": [-10,34,\n              -5,42.5 ],\n    \"sortby\": [{\"field\": \"datetime\",\"direction\": \"desc\"}\n              ],\n    \"limit\": 10,\n}\n\nresponse=requests.post(SEARCH_URL, json=BODY, headers=auth_headers)\n\nJSON(response.json())    \n\n","type":"content","url":"/hda-rest-full-version#the-search-endpoint","position":33},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl2":"Visualize"},"type":"lvl2","url":"/hda-rest-full-version#visualize","position":34},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl2":"Visualize"},"content":"\n\n","type":"content","url":"/hda-rest-full-version#visualize","position":35},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Visualize search results in a table","lvl2":"Visualize"},"type":"lvl3","url":"/hda-rest-full-version#visualize-search-results-in-a-table","position":36},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Visualize search results in a table","lvl2":"Visualize"},"content":"Search results can be visualized on a map.\n\ndf = geopandas.GeoDataFrame.from_features(response.json()['features'], crs=\"epsg:4326\")\ndf.head()\n\n","type":"content","url":"/hda-rest-full-version#visualize-search-results-in-a-table","position":37},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Visualize search results in a map","lvl2":"Visualize"},"type":"lvl3","url":"/hda-rest-full-version#visualize-search-results-in-a-map","position":38},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Visualize search results in a map","lvl2":"Visualize"},"content":"\n\n#map1 = folium.Map([38, 0],\n#                  zoom_start=4, tiles='Esri Ocean Basemap', attr='Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ')\n\n#map1 = folium.Map([38, 0],zoom_start=4)\n\nmap1 = folium.Map([38, 0],zoom_start=4, tiles=None)\n\nnasa_wms = folium.WmsTileLayer(\n    url='https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi',\n    name='NASA Blue Marble',\n    layers='BlueMarble_ShadedRelief',\n    format='image/png',\n    transparent=True,\n    attr='NASA'\n)\nnasa_wms.add_to(map1)\n\nresults=folium.GeoJson( response.json(),name='Search results',style_function=lambda feature: {\n        \"fillColor\": \"#005577\",\n        \"color\": \"black\",\n        \"weight\": 1\n    })\n\nresults.add_to(map1)\n\n\nbbox=[-10,34,-5,42.5]\nbb=folium.GeoJson(\n    shapely.geometry.box(*bbox),name='Search bounding box',style_function=lambda feature: {\n        \"fillColor\": \"#ff0000\",\n        \"color\": \"black\",\n        \"weight\": 2,\n        \"dashArray\": \"5, 5\",\n    }\n)\nbb.add_to(map1)\n\n# Add layer control to toggle visibility\nfolium.LayerControl().add_to(map1)\n\n\n#display(fig)\nmap1\n\n\n","type":"content","url":"/hda-rest-full-version#visualize-search-results-in-a-map","position":39},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl2":"Download"},"type":"lvl2","url":"/hda-rest-full-version#download","position":40},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl2":"Download"},"content":"The items belonging to a specific collection can be downloaded entirely, or it is possible to download a single asset of a chosen item.\n\n","type":"content","url":"/hda-rest-full-version#download","position":41},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Download a specific item","lvl2":"Download"},"type":"lvl3","url":"/hda-rest-full-version#download-a-specific-item","position":42},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Download a specific item","lvl2":"Download"},"content":"To get the metadata specific to a given item (identified by its itemID in a collection, the user can request the /stac/collections/{collectionID}/items/{itemID}endpoint.\n\nprint(COLLECTION_ITEM_BY_ID_URL)\nresponse=requests.get(COLLECTION_ITEM_BY_ID_URL, headers=auth_headers) \nJSON(response.json())             \n\nThe metadata of a given item contains also the download link that the user can use to download a specific item.\n\nresult = json.loads(response.text)\ndownloadUrl = result['assets']['downloadLink']['href']\nprint(downloadUrl)\n\nresp_dl = requests.get(downloadUrl,stream=True,headers=auth_headers)\n\n# If the request was successful, download the file\nif (resp_dl.status_code == HTTP_SUCCESS_CODE):\n        print(\"Downloading \"+ ITEM_ID + \"...\")\n        filename = ITEM_ID + \".zip\"\n        with open(filename, 'wb') as f:\n            for chunk in resp_dl.iter_content(chunk_size=1024): \n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n        print(\"The dataset has been downloaded to: {}\".format(filename))\nelse: print(\"Request Unsuccessful! Error-Code: {}\".format(response.status_code))\n\n","type":"content","url":"/hda-rest-full-version#download-a-specific-item","position":43},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Download a specific asset of an item","lvl2":"Download"},"type":"lvl3","url":"/hda-rest-full-version#download-a-specific-asset-of-an-item","position":44},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Download a specific asset of an item","lvl2":"Download"},"content":"The metadata of a given item contains also the single assets download link, that the user can use to download a specific asset of the chosen item.\nIn the example below we download the asset: “xfdumanifest.xml”\n\ndownloadUrl = result['assets']['xfdumanifest.xml']['href']\nprint(downloadUrl)\n\nresp_dl = requests.get(downloadUrl,stream=True,headers=auth_headers)\n\n# If the request was successful, download the file\nif (resp_dl.status_code == HTTP_SUCCESS_CODE):\n        print(\"Downloading \"+ result['assets']['xfdumanifest.xml']['title'] + \"...\")\n        filename = result['assets']['xfdumanifest.xml']['title']\n        with open(filename, 'wb') as f:\n            for chunk in resp_dl.iter_content(chunk_size=1024): \n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n        print(\"The dataset has been downloaded to: {}\".format(filename))\nelse: print(\"Request Unsuccessful! Error-Code: {}\".format(response.status_code))\n\n","type":"content","url":"/hda-rest-full-version#download-a-specific-asset-of-an-item","position":45},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Visualize the quicklook asset","lvl2":"Download"},"type":"lvl3","url":"/hda-rest-full-version#visualize-the-quicklook-asset","position":46},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial","lvl3":"Visualize the quicklook asset","lvl2":"Download"},"content":"\n\nImage(url=result['assets']['thumbnail']['href'], width=500)","type":"content","url":"/hda-rest-full-version#visualize-the-quicklook-asset","position":47},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start"},"type":"lvl1","url":"/hda-rest-quick-start","position":0},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start"},"content":"\n\n","type":"content","url":"/hda-rest-quick-start","position":1},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start"},"type":"lvl1","url":"/hda-rest-quick-start#dedl-hda-tutorial-quick-start","position":2},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT \n\n","type":"content","url":"/hda-rest-quick-start#dedl-hda-tutorial-quick-start","position":3},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl3":"First steps using Harmonised Data access API"},"type":"lvl3","url":"/hda-rest-quick-start#first-steps-using-harmonised-data-access-api","position":4},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl3":"First steps using Harmonised Data access API"},"content":"Discover Data of DestinE Data Portfolio\n\nAccess Data of DestinE Data Portfolio\n\nThis notebook demonstrates how to use the HDA (Harmonized Data Access) API by sending a few HTTP requests to the API, using Python code.\n\nThroughout this quickstart notebook, you will learn:\n\nDiscover: How to discover DEDL collections and services through HDA.\n\nAuthenticate: How to authenticate fro searching and access DEDL collections.\n\nSearch data:  How to search DEDL data through HDA.\n\nDownload data: How to download DEDL data through HDA.\n\nThe detailed API and definition of each endpoint and parameters is available in the HDA Swagger UI at:\n\nhttps://​hda​.data​.destination​-earth​.eu​/docs/\n\nPrerequisites:\n\nFor Data discovery: none\n\nFor Data access : \n\nDestinE user account\n\n","type":"content","url":"/hda-rest-quick-start#first-steps-using-harmonised-data-access-api","position":5},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl2":"Discover"},"type":"lvl2","url":"/hda-rest-quick-start#discover","position":6},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl2":"Discover"},"content":"\n\n","type":"content","url":"/hda-rest-quick-start#discover","position":7},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl3":"Import the relevant modules","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-quick-start#import-the-relevant-modules","position":8},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl3":"Import the relevant modules","lvl2":"Discover"},"content":"\n\nimport requests\nimport json\nfrom getpass import getpass\nfrom tqdm import tqdm\nimport time\nimport re\n\nimport destinelab as deauth\n\nfrom IPython.display import JSON\n\n","type":"content","url":"/hda-rest-quick-start#import-the-relevant-modules","position":9},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl3":"Define some constants for the API URL","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-quick-start#define-some-constants-for-the-api-url","position":10},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl3":"Define some constants for the API URL","lvl2":"Discover"},"content":"\n\n# Define the collection to be used\nCOLLECTION_ID = \"EO.EUM.DAT.SENTINEL-3.OL_2_WRR___\"\n\n# Core API\nHDA_API_URL = \"https://hda.data.destination-earth.eu\"\n\n# STAC API\n## Core\nSTAC_API_URL = f\"{HDA_API_URL}/stac\"\n\n## Collections\nCOLLECTIONS_URL = f\"{STAC_API_URL}/collections\"\nCOLLECTION_BY_ID_URL = f\"{COLLECTIONS_URL}/{COLLECTION_ID}\"\n\n## Items\nCOLLECTION_ITEMS_URL = f\"{COLLECTIONS_URL}/{COLLECTION_ID}/items\"\n\n## Item Search\nSEARCH_URL = f\"{STAC_API_URL}/search\"\n\n## HTTP Success\nHTTP_SUCCESS_CODE = 200\n\n","type":"content","url":"/hda-rest-quick-start#define-some-constants-for-the-api-url","position":11},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl3":"Discover data","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-quick-start#discover-data","position":12},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl3":"Discover data","lvl2":"Discover"},"content":"Below an example for discovering collections concerning Chlorophyll-a Concentration and algal pigment.\n\nresponse = requests.get(COLLECTIONS_URL,params = {\"q\": \"Chlorophyll-a Concentration,algal pigment\"})\n\nJSON(response.json(), expanded=False)\n\n","type":"content","url":"/hda-rest-quick-start#discover-data","position":13},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl2":"Authenticate"},"type":"lvl2","url":"/hda-rest-quick-start#authenticate","position":14},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl2":"Authenticate"},"content":"\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/hda-rest-quick-start#authenticate","position":15},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl2":"Search"},"type":"lvl2","url":"/hda-rest-quick-start#search","position":16},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl2":"Search"},"content":"\n\nOnce selected a collection it is possible to search for items that match the specified input filters and order the results.\n\nThe collection used for this tutorial is \n\nOLCI Level 2 Ocean Colour Reduced Resolution - Sentinel-3\n\nBODY = {\n    \"collections\": [\n        COLLECTION_ID,\n    ],\n    \"datetime\" : \"2024-09-08T00:00:00Z/2024-09-09T23:59:59Z\",\n    \"bbox\": [-11,35,\n              50,72 ],\n    \"sortby\": [{\"field\": \"datetime\",\"direction\": \"desc\"}\n              ],\n    \"limit\": 3,\n}\n\nr=requests.post(SEARCH_URL, json=BODY, headers=auth_headers)\nif(r.status_code!= 200):\n    (print(r.text))\nr.raise_for_status()\nJSON(r.json(), expanded=False)   \n\n","type":"content","url":"/hda-rest-quick-start#search","position":17},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl2":"Download"},"type":"lvl2","url":"/hda-rest-quick-start#download","position":18},{"hierarchy":{"lvl1":"DEDL - HDA Tutorial - quick start","lvl2":"Download"},"content":"Once obtained the search results we can download the returned data.\n\n#select the first item in the result to download\nproduct = r.json()[\"features\"][0]\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nITEM_ID = product[\"id\"]\n\nresponse = requests.get(download_url,stream=True,headers=auth_headers)\n\n# If the request was successful, download the file\nif (response.status_code == HTTP_SUCCESS_CODE):\n        print(\"Downloading ...\")\n        filename = ITEM_ID + \".zip\"\n        with open(filename, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=1024): \n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n        print(\"The dataset has been downloaded to: {}\".format(filename))\nelse: print(\"Request Unsuccessful! Error-Code: {}\".format(response.status_code))","type":"content","url":"/hda-rest-quick-start#download","position":19},{"hierarchy":{"lvl1":"HDA Gallery"},"type":"lvl1","url":"/gallery","position":0},{"hierarchy":{"lvl1":"HDA Gallery"},"content":"","type":"content","url":"/gallery","position":1},{"hierarchy":{"lvl1":"Access to Hook services "},"type":"lvl1","url":"/dedl-hook-access","position":0},{"hierarchy":{"lvl1":"Access to Hook services "},"content":"\n# Access to Hook services \n\n``` **Licence**: MIT <br>\n\n\nThis Notebook demonstrates:\n\nRetrieval of a token appropriate for interaction with the OnDemand Processing API (Hook API)\n\nListing of available Hooks (Processors) using the retrieved token\n\n%pip install pycurl\n\nimport json\nfrom io import BytesIO\nfrom urllib.parse import urlencode\nimport getpass\nimport pycurl\nimport requests\nfrom getpass import getpass\nfrom IPython.display import JSON\n\n","type":"content","url":"/dedl-hook-access","position":1},{"hierarchy":{"lvl1":"Access to Hook services ","lvl2":"Autehentication - function"},"type":"lvl2","url":"/dedl-hook-access#autehentication-function","position":2},{"hierarchy":{"lvl1":"Access to Hook services ","lvl2":"Autehentication - function"},"content":"\n\nimport requests\nfrom lxml import html\nfrom urllib.parse import parse_qs, urlparse\n\nIAM_URL = \"https://auth.destine.eu/\"\nCLIENT_ID = \"dedl-hook\"\nREALM = \"desp\"\nSERVICE_URL = \"https://odp.data.destination-earth.eu/odata/v1/\"\n\n\nclass DESPAuth:\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n    def get_token(self):\n        with requests.Session() as s:\n\n            # Get the auth url\n            auth_url = (\n                html.fromstring(\n                    s.get(\n                        url=IAM_URL\n                        + \"/realms/\"\n                        + REALM\n                        + \"/protocol/openid-connect/auth\",\n                        params={\n                            \"client_id\": CLIENT_ID,\n                            \"redirect_uri\": SERVICE_URL,\n                            \"scope\": \"openid\",\n                            \"response_type\": \"code\",\n                        },\n                    ).content.decode()\n                )\n                .forms[0]\n                .action\n            )\n\n            # Login and get auth code\n            login = s.post(\n                auth_url,\n                data={\n                    \"username\": self.username,\n                    \"password\": self.password,\n                },\n                allow_redirects=False,\n            )\n\n            # We expect a 302, a 200 means we got sent back to the login page and there's probably an error message\n            if login.status_code == 200:\n                tree = html.fromstring(login.content)\n                error_message_element = tree.xpath('//span[@id=\"input-error\"]/text()')\n                error_message = (\n                    error_message_element[0].strip()\n                    if error_message_element\n                    else \"Error message not found\"\n                )\n                raise Exception(error_message)\n\n            if login.status_code != 302:\n                raise Exception(\"Login failed\")\n\n            auth_code = parse_qs(urlparse(login.headers[\"Location\"]).query)[\"code\"][0]\n\n            # Use the auth code to get the token\n            response = requests.post(\n                IAM_URL + \"/realms/\" + REALM + \"/protocol/openid-connect/token\",\n                data={\n                    \"client_id\": CLIENT_ID,\n                    \"redirect_uri\": SERVICE_URL,\n                    \"code\": auth_code,\n                    \"grant_type\": \"authorization_code\",\n                    \"scope\": \"\",\n                },\n            )\n\n            if response.status_code != 200:\n                raise Exception(\"Failed to get token\")\n\n            token = response.json()[\"access_token\"]\n\n            return token\n\n\nclass DEDLAuth:\n    def __init__(self, desp_access_token):\n        self.desp_access_token = desp_access_token\n\n    def get_token(self):\n        DEDL_TOKEN_URL = \"https://identity.data.destination-earth.eu/auth/realms/dedl/protocol/openid-connect/token\"\n        DEDL_CLIENT_ID = \"hda-public\"\n        AUDIENCE = \"hda-public\"\n\n        data = {\n            \"grant_type\": \"urn:ietf:params:oauth:grant-type:token-exchange\",\n            \"subject_token\": self.desp_access_token,\n            \"subject_issuer\": \"desp-oidc\",\n            \"subject_token_type\": \"urn:ietf:params:oauth:token-type:access_token\",\n            \"client_id\": DEDL_CLIENT_ID,\n            \"audience\": AUDIENCE,\n        }\n\n        response = requests.post(DEDL_TOKEN_URL, data=data)\n\n        print(\"Response code:\", response.status_code)\n\n        if response.status_code == 200:\n            dedl_token = response.json()[\"access_token\"]\n            return dedl_token\n        else:\n            print(response.json())\n            print(\"Error obtaining DEDL access token\")\n\n\nclass AuthHandler:\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n        self.desp_access_token = None\n        self.dedl_access_token = None\n\n    def get_token(self):\n        # Get DESP auth token\n        desp_auth = DESPAuth(self.username, self.password)\n        self.desp_access_token = desp_auth.get_token()\n\n        # Get DEDL auth token\n        dedl_auth = DEDLAuth(self.desp_access_token)\n        self.dedl_access_token = dedl_auth.get_token()\n\n        return self.dedl_access_token\n\n","type":"content","url":"/dedl-hook-access#autehentication-function","position":3},{"hierarchy":{"lvl1":"Access to Hook services ","lvl2":"Authetication"},"type":"lvl2","url":"/dedl-hook-access#authetication","position":4},{"hierarchy":{"lvl1":"Access to Hook services ","lvl2":"Authetication"},"content":"\n\n# Enter DESP credentials.\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\ntoken = AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = token.get_token()\n\n# Check the status of the request\nif access_token is not None:\n\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\n\nelse:\n\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\n","type":"content","url":"/dedl-hook-access#authetication","position":5},{"hierarchy":{"lvl1":"Access to Hook services ","lvl2":"Get a list of avilable Hooks"},"type":"lvl2","url":"/dedl-hook-access#get-a-list-of-avilable-hooks","position":6},{"hierarchy":{"lvl1":"Access to Hook services ","lvl2":"Get a list of avilable Hooks"},"content":"\n\napi_headers = {\"Authorization\": \"Bearer \" + access_token}\nservice_root_url = \"https://odp.data.destination-earth.eu/odata/v1/\"\nresult = requests.get(service_root_url + \"Workflows\", headers=api_headers).json()\n\n# Assuming 'result' is a JSON array\nfor item in result[\"value\"]:\n    for key, value in item.items():\n        print(f\"{key}: {value}\")\n    print()  # Print an empty line to separate each item","type":"content","url":"/dedl-hook-access#get-a-list-of-avilable-hooks","position":7},{"hierarchy":{"lvl1":"Hook Service"},"type":"lvl1","url":"/readme-1","position":0},{"hierarchy":{"lvl1":"Hook Service"},"content":" Author: EUMETSAT\n\n\nThis folder contains materials for learning how to use Hook Service.\n\nThe Destination Earth Data Lake (DEDL) ‘Hook service’ provides ready-to-use high level serverless workflows and functions preconfigured to efficiently access and manipulate Destination Earth Data Lake (DEDL) data. A growing number of workflows and functions will provide on-demand capabilities for the diverse data analysis needs.","type":"content","url":"/readme-1","position":1},{"hierarchy":{"lvl1":"Hook Service","lvl2":"Notebooks"},"type":"lvl2","url":"/readme-1#notebooks","position":2},{"hierarchy":{"lvl1":"Hook Service","lvl2":"Notebooks"},"content":"","type":"content","url":"/readme-1#notebooks","position":3},{"hierarchy":{"lvl1":"Hook Service","lvl3":"Tutorial: How to discover available workflows and run one example (data-harvest workflow)","lvl2":"Notebooks"},"type":"lvl3","url":"/readme-1#tutorial-how-to-discover-available-workflows-and-run-one-example-data-harvest-workflow","position":4},{"hierarchy":{"lvl1":"Hook Service","lvl3":"Tutorial: How to discover available workflows and run one example (data-harvest workflow)","lvl2":"Notebooks"},"content":"#The Tutorial.ipynb is executeable as-is, but parameters can be provided from a file .env_tutorial\n#To provide environment parameters from file\n\n# 1. create a file called .env_tutorial in the same folder as the notebook\n# 2. paste in the following contents\n# 3. execute the Tutorial.ipynb notebook which should now pick up the environment variables from file\n\n########## SET HOOK_ORDER_NAME ##########\n# This will be added to the order name. e.g. replace XXXX with your name\n\nHOOK_ORDER_NAME=XXXX-20250212-1\n\n########## SET WORKFLOW, COLLECTION_ID and DATA_ID ##########\n# Uncomment HOOK_WORKFLOW (to identify the HOOK to execute)\n# Uncomment HOOK_COLLECTION_ID (to identify the DEDL Collection where the data comes from)\n# Uncomment HOOK_DATA_ID (to identify the data that is input to the HOOK)\n# Uncomment HOOK_SOURCE_TYPE (to identify the source_type of the HOOK. Possible values are DESP, EXTERNAL...)\n\n# DEDL HDA input possible\n\n#HOOK_WORKFLOW=card_bs\n#HOOK_COLLECTION_ID=EO.ESA.DAT.SENTINEL-1.L1_GRD\n#HOOK_DATA_ID=S1A_IW_GRDH_1SDV_20230910T054256_20230910T054321_050261_060CD9_BF21.SAFE\n#HOOK_SOURCE_TYPE=DESP\n\n#HOOK_WORKFLOW=lai\n#HOOK_COLLECTION_ID=EO.ESA.DAT.SENTINEL-2.MSI.L2A\n#HOOK_DATA_ID=S2A_MSIL2A_20230906T102601_N0509_R108_T32UMA_20230906T164400.SAFE\n#HOOK_SOURCE_TYPE=DESP\n\n# INPUT PRODUCT TYPES: SLC, S1_SLC__1S, S2_SLC__1S, S3_SLC__1S, S4_SLC__1S, S5_SLC__1S, S6_SLC__1S, IW_SLC__1S, EW_SLC__1S, WV_SLC__1S\n#HOOK_WORKFLOW=card_cohinf\n#HOOK_COLLECTION_ID=EO.ESA.DAT.SENTINEL-1.L1_SLC\n#HOOK_DATA_ID=S1A_IW_SLC__1SDV_20240121T155314_20240121T155341_052207_064FA6_3AB9\n#HOOK_ADDITIONAL1=\"NAME=card_producttype;VALUE=CARD_COH;VALUE_TYPE=str\"\n#HOOK_DATA_ID=S1A_IW_SLC__1SDV_20230930T172417_20230930T172444_050560_06170F_59DF\n#HOOK_ADDITIONAL1=\"NAME=card_producttype;VALUE=CARD_INF;VALUE_TYPE=str\"\n#HOOK_ADDITIONAL2=\"NAME=timespan;VALUE=24;VALUE_TYPE=int\"\n#HOOK_SOURCE_TYPE=DESP\n\n#HOOK_WORKFLOW=c2rcc\n#HOOK_COLLECTION_ID=EO.ESA.DAT.SENTINEL-2.MSI.L1C\n#HOOK_DATA_ID=S2B_MSIL1C_20231001T102739_N0509_R108_T31TGL_20231001T123227\n#HOOK_SOURCE_TYPE=DESP\n\nHOOK_WORKFLOW=data-harvest\nHOOK_COLLECTION_ID=EO.ESA.DAT.SENTINEL-2.MSI.L1C\nHOOK_DATA_ID=S2A_MSIL1C_20230910T050701_N0509_R019_T47VLH_20230910T074321.SAFE\nHOOK_SOURCE_TYPE=DESP\n#HOOK_SOURCE_TYPE=EXTERNAL\n#HOOK_EXTERNAL_TOKEN_URL=https://identity.XXXX/auth/realms/XXXX/protocol/openid-connect/token\n#HOOK_EXTERNAL_CLIENT_ID=hda_public\n#HOOK_EXTERNAL_USERNAME=\n#HOOK_EXTERNAL_PASSWORD=\n\n#HOOK_WORKFLOW=sen2cor\n#HOOK_COLLECTION_ID=EO.ESA.DAT.SENTINEL-2.MSI.L1C\n#HOOK_DATA_ID=S2B_MSIL1C_20231001T102739_N0509_R108_T32TMS_20231001T123227\n#HOOK_SOURCE_TYPE=DESP\n\n##### DEDL HDA input NOT possible at the moment. i.e. source_type is not DESP nor EXTERNAL #####\n# These HOOKs have specific configuration. See below\n\n# Accesses Creodias Catalogue in the background\n#HOOK_WORKFLOW=copdem\n#HOOK_COLLECTION_ID=\n#HOOK_DATA_ID=S3A_SL_1_RBT____20161110T022134_20161110T022434_20181003T070309_0179_010_374______LR1_R_NT_003.SEN3\n#HOOK_SOURCE_TYPE=\n\n# Accesses Creodias Catalogue in the background\n#HOOK_WORKFLOW=maja\n#HOOK_COLLECTION_ID=\n#HOOK_DATA_ID=S2B_MSIL1C_20220429T102549_N0400_R108_T31UGQ_20220429T124017.SAFE\n#HOOK_SOURCE_TYPE=\n#HOOK_ADDITIONAL1=\"NAME=input_type;VALUE=TIMESPAN;VALUE_TYPE=str\"\n#HOOK_ADDITIONAL2=\"NAME=timeseries_end_id;VALUE=S2B_MSIL1C_20220429T102549_N0400_R108_T31UGQ_20220429T124017.SAFE;VALUE_TYPE=str\"\n#HOOK_ADDITIONAL3=\"NAME=timespan;VALUE=18;VALUE_TYPE=int\"\n\n\n########## START : Example Triggering CUSTOM HOOOK - dedl_hello_world ##########\n\n# # This 'Custom Hook' demonstrator expects a file called 'example.data' in the 'source_s3_path' bucket/folder (of the configured source_s3_endpoint)\n# # The text contents of example.data will be converted to Upper Case and output to Private or Temporary Storage\n\n# HOOK_WORKFLOW=dedl_hello_world\n\n# # Needs to be empty\n# HOOK_COLLECTION_ID=\n\n# # This normally contains the ID of a product, but for Custom Hooks we will set it to an arbitrary value 'Custom Hook'\n# HOOK_DATA_ID=Custom Hook\n\n# # Needs to be empty\n# HOOK_SOURCE_TYPE=\n\n# # Setting source_s3_path TO s3://XXXX/dedl_hello_world/input_file_location\n# HOOK_ADDITIONAL1=\"NAME=source_s3_path;VALUE=s3://XXXX/dedl_hello_world/input_file_location;VALUE_TYPE=str\"\n# # Setting source_s3_endpoint_url TO https://s3.central.data.destination-earth.eu\n# HOOK_ADDITIONAL2=\"NAME=source_s3_endpoint_url;VALUE=https://s3.central.data.destination-earth.eu;VALUE_TYPE=str\"\n# # Setting source_s3_access_key TO YYYY (change this to your access_key)\n# HOOK_ADDITIONAL3=\"NAME=source_s3_access_key;VALUE=YYYY;VALUE_TYPE=str\"\n# # Setting source_s3_secret_key TO ZZZZ (change this to your access_key)\n# HOOK_ADDITIONAL4=\"NAME=source_s3_secret_key;VALUE=ZZZZ;VALUE_TYPE=str\"\n\n########## END : Example Triggering CUSTOM HOOOK - dedl_hello_world ##########\n\n########## SET STORAGE OPTION ##########\n# If you set HOOK_IS_PRIVATE_STORAGE to True you will need to set the bucket name, access key, and secret key\nHOOK_IS_PRIVATE_STORAGE=False\n#HOOK_OUTPUT_BUCKET=your_bucket-name\n#HOOK_OUTPUT_STORAGE_ACCESS_KEY=your_access_key\n#HOOK_OUTPUT_STORAGE_SECRET_KEY=your_secret_key\n","type":"content","url":"/readme-1#tutorial-how-to-discover-available-workflows-and-run-one-example-data-harvest-workflow","position":5},{"hierarchy":{"lvl1":"Hook Service","lvl3":"DEDL​-Hook​_access​.ipynb: Simple retrieval of token and listing of workflows","lvl2":"Notebooks"},"type":"lvl3","url":"/readme-1#dedl-hook-access-ipynb-simple-retrieval-of-token-and-listing-of-workflows","position":6},{"hierarchy":{"lvl1":"Hook Service","lvl3":"DEDL​-Hook​_access​.ipynb: Simple retrieval of token and listing of workflows","lvl2":"Notebooks"},"content":"Launch this noteboook with your DESP credentials and retrieve a token you can use to interact with OnDemand Processing API (Hook API)","type":"content","url":"/readme-1#dedl-hook-access-ipynb-simple-retrieval-of-token-and-listing-of-workflows","position":7},{"hierarchy":{"lvl1":"Hook Service","lvl2":"Further Information"},"type":"lvl2","url":"/readme-1#further-information","position":8},{"hierarchy":{"lvl1":"Hook Service","lvl2":"Further Information"},"content":"Further information available in DestinE Data Lake documentation: \n\nhttps://​destine​-data​-lake​-docs​.data​.destination​-earth​.eu​/en​/latest​/index​.html\n\nAdditional ressources:\nDestinE Data Portfolio: \n\nhttps://​hda​.data​.destination​-earth​.eu​/ui​/catalog\nDataLake Priority services: \n\nhttps://​hda​.data​.destination​-earth​.eu​/ui​/services\nHDA SWAGGER UI: \n\nhttps://​hda​.data​.destination​-earth​.eu​/docs/","type":"content","url":"/readme-1#further-information","position":9},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)"},"type":"lvl1","url":"/tutorial","position":0},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)"},"content":"\n\nAuthor: EUMETSAT \nCopyright: 2024 EUMETSAT \nLicence: MIT ","type":"content","url":"/tutorial","position":1},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)"},"type":"lvl1","url":"/tutorial#dedl-hook-tutorial-data-harvest-data-harvest","position":2},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)"},"content":"\n\nThis notebook demonstrates how to use the Hook service.\n\nAuthor: EUMETSAT\n\nThe detailed API and definition of each endpoint and parameters is available in the OnDemand Processing API OData v1  OpenAPI documentation found at:\n\n\nhttps://​odp​.data​.destination​-earth​.eu​/odata​/docs\n\nFurther documentation is available at:\n\n\nhttps://​destine​-data​-lake​-docs​.data​.destination​-earth​.eu​/en​/latest​/dedl​-big​-data​-processing​-services​/Hook​-service​/Hook​-service​.html\n\n","type":"content","url":"/tutorial#dedl-hook-tutorial-data-harvest-data-harvest","position":3},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Install python package requirements and import environment variables"},"type":"lvl2","url":"/tutorial#install-python-package-requirements-and-import-environment-variables","position":4},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Install python package requirements and import environment variables"},"content":"\n\n# Note: The destinelab python package (which helps with authentication) is available already if you are using Python DEDL kernel\n# Otherwise, the destinelab python package can be installed by uncommenting the following line\n\n# For the importing of environment variables using the load_dotenv(...) command \n%pip install python-dotenv\n# for example code navigating private S3 compatible storage (PRIVATE bucket storage)\n%pip install boto3\n\n\nimport os\nimport json\nimport requests\nfrom dotenv import load_dotenv\nfrom getpass import getpass\nimport destinelab as destinelab\n\n# Load (optional) notebook specific environment variables from .env_tutorial\nload_dotenv(\"./.env_tutorial\", override=True)\n\n","type":"content","url":"/tutorial#install-python-package-requirements-and-import-environment-variables","position":5},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Authentification - Get token"},"type":"lvl2","url":"/tutorial#authentification-get-token","position":6},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Authentification - Get token"},"content":"\n\n# By default users should use their DESP credentials to get an Access_token\n# This token is added as an Authorisation Header when interacting with the Hook Service API\n\n# Enter DESP credentials.\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\ntoken = destinelab.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\n\naccess_token = token.get_token()\n\n# Check the status of the request\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\n    # Save API headers\n    api_headers = {\"Authorization\": \"Bearer \" + access_token}\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\n","type":"content","url":"/tutorial#authentification-get-token","position":7},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Setup static variables"},"type":"lvl2","url":"/tutorial#setup-static-variables","position":8},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Setup static variables"},"content":"\n\n# Hook service url (ending with odata/v1/ - e.g. https://odp.data.destination-earth.eu/odata/v1/)\nhook_service_root_url = \"https://odp.data.destination-earth.eu/odata/v1/\"\n\n","type":"content","url":"/tutorial#setup-static-variables","position":9},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"List available workflows"},"type":"lvl2","url":"/tutorial#list-available-workflows","position":10},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"List available workflows"},"content":"Next we can check what possible workflows are available to us by using methodhttps://odp.data.destination-earth.eu/odata/v1/Workflows\n\n# Send request and return json object listing all provided workfows, ordered by Id\nresult = requests.get(\n    f\"{hook_service_root_url}Workflows?$orderby=Id asc\", headers=api_headers   \n).json()\n\nprint(\"List of available DEDL provided Hooks\")\nfor i in range(len(result[\"value\"])):\n    print(\n        f\"Name:{str(result['value'][i]['Name']).ljust(20, ' ')}DisplayName:{str(result['value'][i]['DisplayName'])}\"\n    )  # print JSON string\n\n# Print result JSON object: containing provided workflow list\nworkflow_details = json.dumps(result, indent=4)\nprint(workflow_details)\n\n","type":"content","url":"/tutorial#list-available-workflows","position":11},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Select a workflow and see parameters"},"type":"lvl2","url":"/tutorial#select-a-workflow-and-see-parameters","position":12},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Select a workflow and see parameters"},"content":"\n\nIf we want to see the details of a specific workflow, showing us the parameters that can be set for that workflow, we can add a filter to the query as follows:\n\nhttps://odp.data.destination-earth.eu/odata/v1/Workflows?$expand=WorkflowOptions&$filter=(Name eq data-harvest)\n\n\\expand=WorkflowOptions** shows all parameters accepted by workflow   \n**\\\\filter=(Name eq data-harvest) narrows the result to workflow called “data-harvest”\n\n# Select workflow : defaults to data-harvest\nworkflow = os.getenv(\"HOOK_WORKFLOW\", \"data-harvest\")\nprint(f\"workflow: {workflow}\")\n\n# Send request\nresult = requests.get(\n    f\"{hook_service_root_url}Workflows?$expand=WorkflowOptions&$filter=(Name eq '{workflow}')\",\n    headers=api_headers,\n).json()\nworkflow_details = json.dumps(result, indent=4)\nprint(workflow_details)  # print formatted workflow_details, a JSON string\n\n","type":"content","url":"/tutorial#select-a-workflow-and-see-parameters","position":13},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Order selected workflow"},"type":"lvl2","url":"/tutorial#order-selected-workflow","position":14},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Order selected workflow"},"content":"\n\nThe order selected above will now be configured and executed.\n\ne.g. workflow = “data-harvest”.\n\nMake an order to ‘harvest data’ using Harmonised Data Access API.\n\ni.e. data from an input source can be transferred to a Private bucket or a Temporary storage bucket.\n\n","type":"content","url":"/tutorial#order-selected-workflow","position":15},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl3":"Name your order","lvl2":"Order selected workflow"},"type":"lvl3","url":"/tutorial#name-your-order","position":16},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl3":"Name your order","lvl2":"Order selected workflow"},"content":"\n\n# Here we set the variable order_name, this will allow us to:\n# Easily identify the running process (e.g. when checking the status)\n# order_name is added as a suffix to the order 'Name'\norder_name = os.getenv(\"HOOK_ORDER_NAME\") or input(\"Name your order: \")\nprint(f\"order_name:{order_name}\")\n\n","type":"content","url":"/tutorial#name-your-order","position":17},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl3":"Define output storage","lvl2":"Order selected workflow"},"type":"lvl3","url":"/tutorial#define-output-storage","position":18},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl3":"Define output storage","lvl2":"Order selected workflow"},"content":"\n\nIn workflow parameters, among others values, storage to retreive the result has to be provided.Two possibilites:\n\nUse your user storage\n\nUse a temporary storage\n\n","type":"content","url":"/tutorial#define-output-storage","position":19},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl4":"1. - Your user storage (provided by DEDL ISLET service)","lvl3":"Define output storage","lvl2":"Order selected workflow"},"type":"lvl4","url":"/tutorial#id-1-your-user-storage-provided-by-dedl-islet-service","position":20},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl4":"1. - Your user storage (provided by DEDL ISLET service)","lvl3":"Define output storage","lvl2":"Order selected workflow"},"content":"\n\nExample using a S3 bucket created with ISLET Storage service  - result will be available in this bucket\n\nworkflow parameter: {“Name”: “output_storage”, “Value”: “PRIVATE”}\n\n# Output storage - Islet service\n\n# Note: If you want the output to go to your own PRIVATE bucket rather than TEMPORARY storage (expires after 2 weeks),\n#       i) This Configuration will need to be updated with your output_bucket, output_storage_access_key, output_secret_key, output_prefix\n#       ii) You will need to change the output_storage in the order to PRIVATE and add the necessary source_ parameters (see workflow options and commented example)\n\n# URL of the S3 endpoint in the Central Site (or lumi etc.)\noutput_storage_url = \"https://s3.central.data.destination-earth.eu\"\n# output_storage_url = \"https://s3.lumi.data.destination-earth.eu\"\n\n# Name of the object storage bucket where the results will be stored.\noutput_bucket = os.getenv(\"HOOK_OUTPUT_BUCKET\", \"your-bucket-name\")\nprint(f\"output_bucket            : {output_bucket}\")\n\n# Islet object storage credentials (openstack ec2 credentials)\noutput_storage_access_key = os.getenv(\"HOOK_OUTPUT_STORAGE_ACCESS_KEY\", \"your-access-key\")\noutput_storage_secret_key = os.getenv(\"HOOK_OUTPUT_STORAGE_SECRET_KEY\", \"your-secret-key\")\nprint(f\"output_storage_access_key: {output_storage_access_key}\")\nprint(f\"output_storage_secret_key: {output_storage_secret_key}\")\n\n\n# This is the name of the folder in your output_bucket where the output of the hook will be stored.\n# Here we concatenate 'dedl' with the 'workflow' and 'order_name'\noutput_prefix = f\"dedl-{workflow}-{order_name}\"\nprint(f\"output_prefix            : {output_prefix}\")\n\n","type":"content","url":"/tutorial#id-1-your-user-storage-provided-by-dedl-islet-service","position":21},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl4":"2 - Use temporary storage","lvl3":"Define output storage","lvl2":"Order selected workflow"},"type":"lvl4","url":"/tutorial#id-2-use-temporary-storage","position":22},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl4":"2 - Use temporary storage","lvl3":"Define output storage","lvl2":"Order selected workflow"},"content":"\n\nThe result of processing will be stored in shared storage and download link provided in the output product details\n\nworkflow parameter: {“Name”: “output_storage”, “Value”: “TEMPORARY”}\n\n","type":"content","url":"/tutorial#id-2-use-temporary-storage","position":23},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl3":"Define parameters and send order","lvl2":"Order selected workflow"},"type":"lvl3","url":"/tutorial#define-parameters-and-send-order","position":24},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl3":"Define parameters and send order","lvl2":"Order selected workflow"},"content":"\n\n# URL of the STAC server where your collection/item can be downloaded\nstac_hda_api_url = \"https://hda.data.destination-earth.eu/stac\"\n\n# Note: The data (collection_id and data_id) will have been previously discovered and searched for\n\n# Set collection where the item can be found : defaults to example for data-harvest\ncollection_id = os.getenv(\"HOOK_COLLECTION_ID\", \"EO.ESA.DAT.SENTINEL-2.MSI.L1C\")\nprint(f\"STAC collection url: {stac_hda_api_url}/collections/{collection_id}\")\n\n# Set the Item to Retrieve : defaults to example for data-harvest. If Multiple Values, provide comma separated list\ndata_id = os.getenv(\"HOOK_DATA_ID\", \"S2A_MSIL1C_20230910T050701_N0509_R019_T47VLH_20230910T074321.SAFE\")\nprint(f\"data_id: {data_id}\")\nidentifier_list = [data_id_element.strip() for data_id_element in data_id.split(',')]\n\n# Get boolean value from String, default (False)\nis_private_storage = os.getenv(\"HOOK_IS_PRIVATE_STORAGE\", \"False\") == \"True\"\nprint(f\"is_private_storage: {is_private_storage}\")\n\n# we use source_type to add DESP or EXTERNAL specific configuration\nsource_type = os.getenv(\"HOOK_SOURCE_TYPE\", \"DESP\")\nprint(f\"source_type: {source_type}\")\n\nif source_type == \"EXTERNAL\":\n    EXTERNAL_USERNAME = os.getenv(\"HOOK_EXTERNAL_USERNAME\", \"EXTERNAL_USERNAME\")\n    EXTERNAL_PASSWORD = os.getenv(\"HOOK_EXTERNAL_PASSWORD\", \"EXTERNAL_PASSWORD\")\n    EXTERNAL_TOKEN_URL = os.getenv(\"HOOK_EXTERNAL_TOKEN_URL\", \"EXTERNAL_TOKEN_URL\")\n    EXTERNAL_CLIENT_ID = os.getenv(\"HOOK_EXTERNAL_CLIENT_ID\", \"EXTERNAL_CLIENT_ID\")\n\n########## BUILD ORDER BODY : CHOOSE PRIVATE or TEMPORARY output_storage ##########\n\n# Initialise the order_body\norder_body_custom_bucket = {\n    \"Name\": \"Tutorial \" + workflow + \" - \" + order_name,\n    \"WorkflowName\": workflow,\n    \"IdentifierList\": identifier_list,\n    \"WorkflowOptions\": [],\n}\n\n\n##### Configure PRIVATE OR TEMPORARY STORAGE #####\nif is_private_storage:\n\n    print(\"##### Preparing Order Body for PRIVATE STORAGE #####\")\n    order_body_custom_bucket[\"WorkflowOptions\"].extend(\n        [\n            {\"Name\": \"output_storage\", \"Value\": \"PRIVATE\"},\n            {\"Name\": \"output_s3_access_key\", \"Value\": output_storage_access_key},\n            {\"Name\": \"output_s3_secret_key\", \"Value\": output_storage_secret_key},\n            {\"Name\": \"output_s3_path\", \"Value\": f\"s3://{output_bucket}/{output_prefix}\"},\n            {\"Name\": \"output_s3_endpoint_url\", \"Value\": output_storage_url}\n        ]\n    )\n\nelse:\n\n    print(\"##### Preparing Order Body for TEMPORARY STORAGE #####\")\n    order_body_custom_bucket[\"WorkflowOptions\"].extend(\n        [\n            {\"Name\": \"output_storage\", \"Value\": \"TEMPORARY\"},\n        ]\n    )\n\n##### Configure SOURCE_TYPE and associated parameters #####\nif source_type == \"DESP\":\n\n    # Using DESP credentials is standard way of executing Hooks.\n    print(\"##### Preparing Order Body for access to DEDL HDA using DESP Credentials #####\")\n    order_body_custom_bucket[\"WorkflowOptions\"].extend(\n        [\n            {\"Name\": \"source_type\", \"Value\": \"DESP\"},\n            {\"Name\": \"desp_source_username\", \"Value\": DESP_USERNAME},\n            {\"Name\": \"desp_source_password\", \"Value\": DESP_PASSWORD},\n            {\"Name\": \"desp_source_collection\", \"Value\": collection_id}\n        ]\n    )\n\nelif source_type == \"EXTERNAL\":\n\n    # Build your order body : Example using EXTERNAL source type and source_catalogue_api_type STAC.\n    # This would allow you to access products directly from a configured STAC server\n    # Here we show an example configuration of a STAC server with OIDC security, that could be adapted to your needs (change urls, etc)\n    # This is shown for example purposes only. The standard way of configuring is with DESP source_type seen above.\n    print(\"##### Preparing Order Body for access to EXTERNAL STAC Server using EXTERNAL Credentials #####\")\n    order_body_custom_bucket[\"WorkflowOptions\"].extend(\n        [\n            {\"Name\": \"source_type\", \"Value\": \"EXTERNAL\"},\n            {\"Name\": \"source_catalogue_api_url\", \"Value\": stac_hda_api_url},\n            {\"Name\": \"source_catalogue_api_type\", \"Value\": \"STAC\"},\n            {\"Name\": \"source_token_url\", \"Value\": EXTERNAL_TOKEN_URL},\n            {\"Name\": \"source_grant_type\", \"Value\": \"PASSWORD\"},\n            {\"Name\": \"source_auth_header_name\", \"Value\": \"Authorization\"},\n            {\"Name\": \"source_username\", \"Value\": EXTERNAL_USERNAME},\n            {\"Name\": \"source_password\", \"Value\": EXTERNAL_PASSWORD},\n            {\"Name\": \"source_client_id\", \"Value\": EXTERNAL_CLIENT_ID},\n            {\"Name\": \"source_client_secret\", \"Value\": \"\"},\n            {\"Name\": \"source_catalogue_collection\", \"Value\": collection_id}\n        ]\n    )\n\nelse:\n\n    print(\"source_type not equal to DESP or EXTERNAL\")\n\n\n\n########## ADDITIONAL OPTIONS ##########\n\nadditional_options = []\n\n# Checks environment variables for the form HOOK_ADDITIONAL1=\"NAME=12345;VALUE=abcdef\"\nfor env_key, env_value in os.environ.items():\n    if env_key.startswith('HOOK_ADDITIONAL'):\n        #print(f\"{env_key}: {env_value}\")        \n        parts = env_value.split(';')\n        # Extract the name and value\n        name = parts[0].split('=')[1]\n        value = parts[1].split('=')[1]\n        value_type = parts[2].split('=')[1]\n        additional_options.append({\"Name\": name, \"Value\": value if value_type == 'str' else int(value)})\n\nprint(f\"addditional_options:{additional_options}\")\n\nif additional_options:\n    print(\"Adding additional_options\")\n    order_body_custom_bucket[\"WorkflowOptions\"].extend(additional_options)\n\n########## BUILD ORDER BODY : END ##########\n\n# Uncomment this to see the final order body\n# print(json.dumps(order_body_custom_bucket, indent=4))\n\n\n# Send order\norder_request = requests.post(\n    hook_service_root_url + \"BatchOrder/OData.CSC.Order\",\n    json.dumps(order_body_custom_bucket),\n    headers=api_headers,\n).json()\n\n# If code = 201, the order has been successfully sent\n\n# Print order_request JSON object: containing order_request details\norder_reques_details = json.dumps(order_request, indent=4)\nprint(order_reques_details)\n\norder_id = order_request['value']['Id']\nprint(f\"order 'Id' from order_request: {order_id}\")\n\n\nIt is possible to order multiple product using endpoint:\nhttps://odp.data.destination-earth.eu/odata/v1/BatchOrder/OData.CSC.Order\n\n","type":"content","url":"/tutorial#define-parameters-and-send-order","position":25},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Check The status of the order"},"type":"lvl2","url":"/tutorial#check-the-status-of-the-order","position":26},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Check The status of the order"},"content":"Possible status values\n\nqueued (i.e. queued for treatment but not started)\n\nin_progress (i.e. order being treated)\n\ncompleted (i.e. order is complete and data ready)\n\n\n# ProductionOrders endpoint gives status of orders (only with one item attached)\n# Otherwise use BatchOrder(XXXX)/Products endpoint to get status of individual items associated with order\nif len(identifier_list) == 1:\n    order_status_url = f\"{hook_service_root_url}ProductionOrders\"\n    params = {\"$filter\": f\"id eq {order_id}\"}\n    order_status_response = requests.get(order_status_url, params=params, headers=api_headers).json()\n    print(json.dumps(order_status_response, indent=4))\n\n# Get Status of all items of an order in this way\norder_status_response = requests.get(\n    f\"{hook_service_root_url}BatchOrder({order_id})/Products\",\n    headers=api_headers,\n).json()\nprint(json.dumps(order_status_response, indent=4))\n\n\n","type":"content","url":"/tutorial#check-the-status-of-the-order","position":27},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Access workflow output"},"type":"lvl2","url":"/tutorial#access-workflow-output","position":28},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl2":"Access workflow output"},"content":"\n\n","type":"content","url":"/tutorial#access-workflow-output","position":29},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl4":"Private storage","lvl2":"Access workflow output"},"type":"lvl4","url":"/tutorial#private-storage","position":30},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl4":"Private storage","lvl2":"Access workflow output"},"content":"Let us now check our private storage using this boto3 script.\nYou can also go and check this in the Islet service using the Horizon user interface\n\n# PRIVATE STORAGE: Prints contents of Private Bucket\nimport boto3\n\nif is_private_storage:\n\n    s3 = boto3.client(\n        \"s3\",\n        aws_access_key_id=output_storage_access_key,\n        aws_secret_access_key=output_storage_secret_key,\n        endpoint_url=output_storage_url,\n    )\n\n    paginator = s3.get_paginator(\"list_objects_v2\")\n    pages = paginator.paginate(Bucket=output_bucket, Prefix=output_prefix + \"/\")\n\n    for page in pages:\n        try:\n            for obj in page[\"Contents\"]:\n                print(obj[\"Key\"])\n        except KeyError:\n            print(\"No files exist\")\n            exit(1)\n\n","type":"content","url":"/tutorial#private-storage","position":31},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl3":"Temporary storage","lvl2":"Access workflow output"},"type":"lvl3","url":"/tutorial#temporary-storage","position":32},{"hierarchy":{"lvl1":"DEDL - Hook Tutorial - Data Harvest (data-harvest)","lvl3":"Temporary storage","lvl2":"Access workflow output"},"content":"\n\n# List order items within a production order\n# When the output_storage is of type TEMPORARY we can get a DownloadLink from the following code (Can also optionally download items here in code with the flag is_download_products)\n\n# If TEMPORARY storage\nif not is_private_storage:\n\n    # Set to True to download products at the same level as the notebook file. File name will be in format \"output-{workflow}-{order_id}-{product_id}.zip\"\n    is_download_products = False\n\n    # Get Status of all items of an order in this way\n    product_status_response = requests.get(\n        f\"{hook_service_root_url}BatchOrder({order_id})/Products\",\n        headers=api_headers,\n    ).json()\n    print(json.dumps(product_status_response, indent=4))\n\n    if is_download_products:\n\n        is_all_products_completed = True\n        # We only attempt to download products when each of the items is in complete status.\n        for i in range(len(product_status_response[\"value\"])):\n\n            product_id = product_status_response[\"value\"][i][\"Id\"]\n            product_status = product_status_response[\"value\"][i][\"Status\"]\n\n            if product_status != \"completed\":\n                is_all_products_completed = False\n\n        # Can download if all products completed\n        if is_all_products_completed:\n\n            for i in range(len(product_status_response[\"value\"])):\n\n                product_id = product_status_response[\"value\"][i][\"Id\"]\n                product_status = product_status_response[\"value\"][i][\"Status\"]\n\n                # Infer the url of the product\n                url_product = f\"{hook_service_root_url}BatchOrder({order_id})/Product({product_id})/$value\"\n                print(f\"url_product: {url_product}\")\n                # Download the product\n                r = requests.get(\n                    url_product, headers=api_headers, allow_redirects=True\n                )\n                product_file_name = f\"output-{workflow}-{order_id}-{product_id}.zip\"\n                open(product_file_name, \"wb\").write(r.content)\n                print(f\"Download Complete: product_file_name: {product_file_name}\")\n\n        else:\n            print(f\"Status for order:{order_id} - At least one of the products does not have the status of 'completed'.\")\n","type":"content","url":"/tutorial#temporary-storage","position":33},{"hierarchy":{"lvl1":"HOOK Gallery"},"type":"lvl1","url":"/gallery-1","position":0},{"hierarchy":{"lvl1":"HOOK Gallery"},"content":"","type":"content","url":"/gallery-1","position":1},{"hierarchy":{"lvl1":"DestinE-DataLake-Lab"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"DestinE-DataLake-Lab"},"content":"\n\nDestination Earth Data Lake Laboratory, which contains additional information for working with DestinE Data Lake services:\n\nHarmonised Data Access (Juypter notebooks examples + Python Tools)\n\nSTACK service (Juypter Notebook examples on how to use DASK for near data processing)\n\nHOOK service (Juypter Notebook examples on how to use HOOK for workflows)\n\nFurther information available in DestinE Data Lake documentation: \n\nhttps://​destine​-data​-lake​-docs​.data​.destination​-earth​.eu​/en​/latest​/index​.html\n\nAdditional ressources:\n\nDestinE Data Portfolio: \n\nhttps://​hda​.data​.destination​-earth​.eu​/ui​/catalog\n\nDataLake Priority services: \n\nhttps://​hda​.data​.destination​-earth​.eu​/ui​/services\n\nHDA SWAGGER UI: \n\nhttps://​hda​.data​.destination​-earth​.eu​/docs/\n\nDestinE Platform Insula Service Users\n\nPlease perform the following and select my_env kernel when running the provided Notebooks\n\nOpen a terminal window (File, New, Terminal) and run the following commands in sequence:\n\nCreate a virtual environment: python -m venv /home/jovyan/my_env\n\nActivate it: source /home/jovyan/my_env/bin/activate\n\nInstall required dependencies for this example Notebooks: pip install -r /home/jovyan/datalake-lab/requirements.txt\n\nVerify the installation: pip list | grep destinelab\n\nThis should give:\n\ndestinelab         0.9\n\nInstall kernel my_env. Run the command: python -m ipykernel install --name my_env --user\n\nSelect the kernel my_env from the top-right menu of these notebooks.\n\nUsers who already have a previous version of the ‘my_env’ environment installed, should delete the kernel before running the steps above. To delete the my_env kernel please run the following command: ‘jupyter kernelspec uninstall my_env’ from a terminal window.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"DestinE Data Lake - Stack Service Dask"},"type":"lvl1","url":"/dedl-stackservice-dask","position":0},{"hierarchy":{"lvl1":"DestinE Data Lake - Stack Service Dask"},"content":"","type":"content","url":"/dedl-stackservice-dask","position":1},{"hierarchy":{"lvl1":"DestinE Data Lake - Stack Service Dask"},"type":"lvl1","url":"/dedl-stackservice-dask#destine-data-lake-stack-service-dask","position":2},{"hierarchy":{"lvl1":"DestinE Data Lake - Stack Service Dask"},"content":"\n\n","type":"content","url":"/dedl-stackservice-dask#destine-data-lake-stack-service-dask","position":3},{"hierarchy":{"lvl1":"DestinE Data Lake - Stack Service Dask","lvl2":"Authentication via OIDC password grant flow"},"type":"lvl2","url":"/dedl-stackservice-dask#authentication-via-oidc-password-grant-flow","position":4},{"hierarchy":{"lvl1":"DestinE Data Lake - Stack Service Dask","lvl2":"Authentication via OIDC password grant flow"},"content":"The DEDL Stack client library holds the DaskOIDC class as a helper class to authenticate a user against the given identity provider of DestinE Data Lake.\nThe users password is directly handed over to the request object and is not stored.\nRefreshed token is used to request a new access token in case it is expired.\n\nThe DaskMultiCluster class provides an abstraction layer to spawn multiple Dask clusters, one per location, within the data lake. Each cluster will be composed of 2 workers per default, with adaptive scaling enabled towards a maximum of 10 workers. In addition, the workers are configured to have 2 cores and 2 GB RAM per default. This can be changed via the \n\ncluster options exposed up to the given service quota of the individual user role:\n\nWorker cores:\n\nmin: 1\n\nmax: ..::service-quota::..\n\nWorker memory:\n\nmin: 1 GB\n\nmax: ..::service-quota::.. GB\n\nDask Worker and Scheduler nodes are based on a custom build \n\ncontainer image with the aim to match the environment, Jupyter Kernel, of the DEDL JupyterLab instance. Warnings will be displayed if a version missmatch is detected. Feel free to use your custom image to run your workloads by replacing the container image in the cluster options object.\n\nfrom dedl_stack_client.authn import DaskOIDC\nfrom dedl_stack_client.dask import DaskMultiCluster\nfrom rich.prompt import Prompt\n\nmyAuth = DaskOIDC(username=Prompt.ask(prompt=\"Username\"))\nmyDEDLClusters = DaskMultiCluster(auth=myAuth)\nmyDEDLClusters.new_cluster()\n\nPrint the given client object details per location as well as the link to the Dask dashboard.\n\nwith myDEDLClusters.as_current(location=\"central\") as myclient:\n    print(myclient)\n    print(myclient.dashboard_link)\nwith myDEDLClusters.as_current(location=\"lumi\") as myclient:\n    print(myclient)\n    print(myclient.dashboard_link)\n\n","type":"content","url":"/dedl-stackservice-dask#authentication-via-oidc-password-grant-flow","position":5},{"hierarchy":{"lvl1":"DestinE Data Lake - Stack Service Dask","lvl3":"Shutdown the all clusters and free up all resources","lvl2":"Authentication via OIDC password grant flow"},"type":"lvl3","url":"/dedl-stackservice-dask#shutdown-the-all-clusters-and-free-up-all-resources","position":6},{"hierarchy":{"lvl1":"DestinE Data Lake - Stack Service Dask","lvl3":"Shutdown the all clusters and free up all resources","lvl2":"Authentication via OIDC password grant flow"},"content":"\n\nmyDEDLClusters.shutdown()","type":"content","url":"/dedl-stackservice-dask#shutdown-the-all-clusters-and-free-up-all-resources","position":7},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"type":"lvl1","url":"/extremedt-datacube-xviewer","position":0},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"content":"\n\nAuthor: EUMETSATCopyright: 2024 EUMETSATLicence: MIT\n\n","type":"content","url":"/extremedt-datacube-xviewer","position":1},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"type":"lvl1","url":"/extremedt-datacube-xviewer#create-a-dashboard-based-on-data-cube-populated-with-data-obtained-from-weather-and-geophysical-extremes-digital-twin-dt-extremedt","position":2},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"content":"DISCLAIMERIn order to deal with the code provided within this notebook, it is required to run it on user environment (local one or virtual machine).\n\nThis notebook covers:\n\nfind available data cubes and their urls\n\nfilter data area of interest\n\ncreate interactive dashboard using xcube - xviewer\n\nPresequites\n\nXcube\nInstall xcube and create a new environment mamba create --name xcube --channel conda-forge xcubeORInstall xcube in the current environment  mamba install --channel conda-forge xcube\n\nXarraypip pip install xarrayORconda  conda install -c conda-forge xarray dask netCDF4 bottleneck\n\n","type":"content","url":"/extremedt-datacube-xviewer#create-a-dashboard-based-on-data-cube-populated-with-data-obtained-from-weather-and-geophysical-extremes-digital-twin-dt-extremedt","position":3},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Prepre your environment"},"type":"lvl2","url":"/extremedt-datacube-xviewer#prepre-your-environment","position":4},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Prepre your environment"},"content":"\n\nfrom xcube.webapi.viewer import Viewer\nimport xarray as xr\nimport requests\n\n","type":"content","url":"/extremedt-datacube-xviewer#prepre-your-environment","position":5},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Connect with Extreme DT data cube"},"type":"lvl2","url":"/extremedt-datacube-xviewer#connect-with-extreme-dt-data-cube","position":6},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Connect with Extreme DT data cube"},"content":"The data cube provides data:\n\nFour variables\n\n2t - Air temperature at 2 meteres above grond [K]\n\n2d - Dew point temperature at 2 meteres above grond [K]\n\nsp - Surface pressure [Pa]\n\nForecast 2024.04.04-13 + 96 hours for each date\n\nHourly step\n\nWorld\n\n","type":"content","url":"/extremedt-datacube-xviewer#connect-with-extreme-dt-data-cube","position":7},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Select proper data cube"},"type":"lvl2","url":"/extremedt-datacube-xviewer#select-proper-data-cube","position":8},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Select proper data cube"},"content":"Data cubes on s3 bucket are stored under URL \n\nhttps://​s3​.central​.data​.destination​-earth​.eu​/swift​/v1​/dedl​_datacube.Data cubes are stored in two directories:\n\nExtremeDT - the newest one\n\narchive - from preicus days\n\nFile nameing convention:dt_extreme_YYYYMMDD.zarr/YYYYMMDD - is the date when forecast starts (step 0)\n\nResultsAfter exectution of code below, the list of urls linked to available cubes will be printed.\n\n# URL to s3 where  ExtremeDT data cubes are stored\ndatacube_url = 'https://s3.central.data.destination-earth.eu/swift/v1/dedl_datacube'\nresponse = requests.get(datacube_url)\n\nif response.status_code == 200:\n    lines = response.text.splitlines()\n    zarr_items = [line for line in lines if line.endswith('.zarr') or line.endswith('.zarr/')]\n    if zarr_items:\n        for item in zarr_items:\n            print(item)\n            new_url = f\"{datacube_url}/{item}\"\n            print(\"New URL:\", new_url)\n    else:\n        print(\"No .zarr files or directories found.\")\nelse:\n    print(\"Failed to fetch contents. Status code:\", response.status_code)\n\nurl = 'https://s3.waw3-1.cloudferro.com/swift/v1/s5p_l3/ExtremeDT/dt_extreme_20240412.zarr/'\n\n","type":"content","url":"/extremedt-datacube-xviewer#select-proper-data-cube","position":9},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl3":"Area of interest","lvl2":"Select proper data cube"},"type":"lvl3","url":"/extremedt-datacube-xviewer#area-of-interest","position":10},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl3":"Area of interest","lvl2":"Select proper data cube"},"content":"Upload data for selected area and verify what variables are provided. In this case uplaod data for Kenya. List of available variables should be returend.\n\nkenya_bbox = [33.501,   # West\n              -4.677,   # South\n              41.899,   # East\n              5.193]    # North\n\nkenya_dt = xr.open_zarr(url).sel(lon=slice(kenya_bbox[0], \n                                            kenya_bbox[2]), \n                                lat=slice(kenya_bbox[3], \n                                            kenya_bbox[1]),\n                                                         )\n\nlist(kenya_dt.keys())\n\n","type":"content","url":"/extremedt-datacube-xviewer#area-of-interest","position":11},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Prepare Data for visualization"},"type":"lvl2","url":"/extremedt-datacube-xviewer#prepare-data-for-visualization","position":12},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Prepare Data for visualization"},"content":"\n\n","type":"content","url":"/extremedt-datacube-xviewer#prepare-data-for-visualization","position":13},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl3":"Convert units","lvl2":"Prepare Data for visualization"},"type":"lvl3","url":"/extremedt-datacube-xviewer#convert-units","position":14},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl3":"Convert units","lvl2":"Prepare Data for visualization"},"content":"\n\nkenya_dt['2t'] -= 273.15    # Conversion to Celcius degrees\nkenya_dt['2d'] -= 273.15    # Conversion to Celcius degrees\nkenya_dt['sp'] /= 100       # Conversion to hectoPascals\n\nkenya_dt['2t'].attrs['units'] = '°C'\nkenya_dt['2d'].attrs['units'] = '°C'\nkenya_dt['sp'].attrs['units'] = 'hPa'\n\n","type":"content","url":"/extremedt-datacube-xviewer#convert-units","position":15},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Define style of visualization"},"type":"lvl2","url":"/extremedt-datacube-xviewer#define-style-of-visualization","position":16},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Define style of visualization"},"content":"\n\nError message starting with:404 GET /viewer/config/config.json (127.0.0.1): xcube viewer has not been been configured\n404 GET /viewer/config/config.json (127.0.0.1) 2.14mscould occur.It is normal and does not affect on proper functioning of xviewer.\n\nviewer = Viewer(server_config={\n    \"Styles\": [\n        {\n            \"Identifier\": \"dt_legend\",          # Style's name\n            \"ColorMappings\": {\n                \"2t\": {                         # Variable's name\n                    \"ValueRange\": [10, 30],     # Variable's values range\n                    \"ColorBar\": \"coolwarm\"      # colorbar\n                },\n                \"2d\": {                         # Variable's name\n                    \"ValueRange\": [0, 20],\n                    \"ColorBar\": \"coolwarm\",\n                },\n                'sp': {                         # Variable's name\n                    \"ValueRange\": [0, 1000],    # Variable's values range\n                    \"ColorBar\": \"viridis\"       # colorbar\n                },\n            }\n        },\n    ]\n})\n\n","type":"content","url":"/extremedt-datacube-xviewer#define-style-of-visualization","position":17},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl3":"Add style to the data cube","lvl2":"Define style of visualization"},"type":"lvl3","url":"/extremedt-datacube-xviewer#add-style-to-the-data-cube","position":18},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl3":"Add style to the data cube","lvl2":"Define style of visualization"},"content":"\n\nviewer.add_dataset(kenya_dt, \n                   style=\"dt_legend\")\n\n","type":"content","url":"/extremedt-datacube-xviewer#add-style-to-the-data-cube","position":19},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Run the dashboard"},"type":"lvl2","url":"/extremedt-datacube-xviewer#run-the-dashboard","position":20},{"hierarchy":{"lvl1":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Run the dashboard"},"content":"\n\nviewer.info()       # open dashboard in separate tab in browser\n#viewer.show()      # open dashboard in current Jupyter Notebook","type":"content","url":"/extremedt-datacube-xviewer#run-the-dashboard","position":21},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"type":"lvl1","url":"/extremedt-datacube","position":0},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"content":"\n\nLicence: MIT ","type":"content","url":"/extremedt-datacube","position":1},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"type":"lvl1","url":"/extremedt-datacube#data-cube-populated-with-data-obtained-from-weather-and-geophysical-extremes-digital-twin-dt-extremedt","position":2},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"content":"This notebook covers:\n\nfind available data cubes and their urls\n\nupload data cube\n\nplot map for desired area, time and variable\n\nplot time series chart for selected variable in specifc time for specific location\n\ncreate interactive dashboard using xcube - xviewer\n\n","type":"content","url":"/extremedt-datacube#data-cube-populated-with-data-obtained-from-weather-and-geophysical-extremes-digital-twin-dt-extremedt","position":3},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Prepre your environment"},"type":"lvl2","url":"/extremedt-datacube#prepre-your-environment","position":4},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Prepre your environment"},"content":"\n\nimport xarray as xr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport dask\nimport requests\n\n","type":"content","url":"/extremedt-datacube#prepre-your-environment","position":5},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Connect with Extreme DT data cube"},"type":"lvl2","url":"/extremedt-datacube#connect-with-extreme-dt-data-cube","position":6},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Connect with Extreme DT data cube"},"content":"The data cube provides data:\n\nFour variables\n\n2t - Air temperature at 2 meters above grond [K]\n\n2d - Dew point temperature at 2 meters above grond [K]\n\nsp - Surface pressure [Pa]\n\nForecast from 10.04.2024 + 96 hours\n\nHourly step\n\nWorld\n\n","type":"content","url":"/extremedt-datacube#connect-with-extreme-dt-data-cube","position":7},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Select proper data cube"},"type":"lvl2","url":"/extremedt-datacube#select-proper-data-cube","position":8},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Select proper data cube"},"content":"Data cubes on s3 bucket are stored under URL \n\nhttps://​s3​.central​.data​.destination​-earth​.eu​/swift​/v1​/dedl​_datacube.Data cubes are stored in two directories:\n\nExtremeDT - the newest one\n\narchive - from prievous days\n\nFile nameing convention:dt_extreme_YYYYMMDD.zarr/YYYYMMDD - is the date when forecast starts (step 0)\n\nResultsAfter exectution of code below, the list of urls linked to available cubes will be printed.\n\n# URL to s3 where  ExtremeDT data cubes are stored\ndatacube_url = 'https://s3.central.data.destination-earth.eu/swift/v1/dedl_datacube'\nresponse = requests.get(datacube_url)\n\nif response.status_code == 200:\n    lines = response.text.splitlines()\n    zarr_items = [line for line in lines if line.endswith(\".zarr\") or line.endswith(\".zarr/\")]\n    if zarr_items:\n        for item in zarr_items:\n            print(item)\n            new_url = f\"{datacube_url}/{item}\"\n            print(\"New URL:\", new_url)\n    else:\n        print(\"No .zarr files or directories found.\")\nelse:\n    print(\"Failed to fetch contents. Status code:\", response.status_code)\n\n\nGet info about the newset data cube.\n\n# Paste into url variable link to the newest data cube\nurl = 'https://s3.central.data.destination-earth.eu/swift/v1/dedl_datacube/ExtremeDT/dt_extreme_20240410.zarr/'\n\n","type":"content","url":"/extremedt-datacube#select-proper-data-cube","position":9},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Let’s make some test"},"type":"lvl2","url":"/extremedt-datacube#lets-make-some-test","position":10},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Let’s make some test"},"content":"\n\n","type":"content","url":"/extremedt-datacube#lets-make-some-test","position":11},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl3":"Area of interest","lvl2":"Let’s make some test"},"type":"lvl3","url":"/extremedt-datacube#area-of-interest","position":12},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl3":"Area of interest","lvl2":"Let’s make some test"},"content":"Upload data for selected area and verify what variables are provided. In this case uplaod data for Africa. List of available variables should be returend.\n\nafrica_bbox = [-20,     # West\n                -40,    # South\n                60,     # East\n                40]     #North\n\nafrica_dt = xr.open_zarr(url).sel(lon=slice(africa_bbox[0], \n                                            africa_bbox[2]), \n                                lat=slice(africa_bbox[3], \n                                            africa_bbox[1]),\n                                                         )\n\nlist(africa_dt.keys())\n\nPlot map of air temperature for Africa.\n\nlon = africa_dt['lon']\nlat = africa_dt['lat']\ntemperature = africa_dt['2t'][0, 0] - 273.15 # Conversion to Celcius degrees\n\nplt.figure(figsize=(10, 6))\nplt.pcolormesh(lon, lat, temperature, cmap='coolwarm')\nplt.colorbar(label='Temperature (°C)')\nplt.title('Temperature Map')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\n\n","type":"content","url":"/extremedt-datacube#area-of-interest","position":13},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Get data for specific time range"},"type":"lvl2","url":"/extremedt-datacube#get-data-for-specific-time-range","position":14},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Get data for specific time range"},"content":"Get data from 10st of April to 11th of April.\n\nafrica_bbox = [-20,     # West\n                -40,    # South\n                60,     # East\n                40]     #North\n\nafrica_dt = xr.open_zarr(url).sel(lon=slice(africa_bbox[0], \n                                            africa_bbox[2]), \n                                lat=slice(africa_bbox[3], \n                                            africa_bbox[1]),\n                                time=slice('20240410T000000', '20240411T000000')\n                                                         )\n\nprint(africa_dt.time)\n\n","type":"content","url":"/extremedt-datacube#get-data-for-specific-time-range","position":15},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Obtain data for specific variable and time"},"type":"lvl2","url":"/extremedt-datacube#obtain-data-for-specific-variable-and-time","position":16},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Obtain data for specific variable and time"},"content":"Obtain surface pressure data from 10th of April to 11th of April.\n\nafrica_bbox = [-20,     # West\n                -40,    # South\n                60,     # East\n                40]     #North\n\nafrica_dt = xr.open_zarr(url)['sp'].sel(lon=slice(africa_bbox[0], \n                                            africa_bbox[2]), \n                                lat=slice(africa_bbox[3], \n                                            africa_bbox[1]),\n                                time=slice('20240410T000000', '20240411T000000')\n                                                         )\n\nprint(africa_dt.var)\n\nPlot map of surface pressure over Africa.\n\nlon = africa_dt['lon']\nlat = africa_dt['lat']\ntemperature = africa_dt[0] / 100    # Conversion to hectoPascals\n\nplt.figure(figsize=(10, 6))\nplt.pcolormesh(lon, lat, temperature, cmap='viridis')\nplt.colorbar(label='Surface Pressure (hPa)')\nplt.title('Surface pressure Map')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\n\n","type":"content","url":"/extremedt-datacube#obtain-data-for-specific-variable-and-time","position":17},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Time series"},"type":"lvl2","url":"/extremedt-datacube#time-series","position":18},{"hierarchy":{"lvl1":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT","lvl2":"Time series"},"content":"Verify if it is possible to create time series chart (for 96 hours) from DT output - air temperature over Nairobi.\n\nafrica_bbox = [-20,     # West\n                -40,    # South\n                60,     # East\n                40]     #North\n\nafrica_dt = xr.open_zarr(url).sel(lon=slice(africa_bbox[0], \n                                            africa_bbox[2]), \n                                lat=slice(africa_bbox[3], \n                                            africa_bbox[1]),\n                                )\n                                                    \n\nCreate a chart.\n\n# Define Nairobi coordinates\nnairobi_lat = -1.286389\nnairobi_lon = 36.817223\n\nlat = africa_dt['lat']\nlon = africa_dt['lon']\nnearest_lat_idx = np.abs(lat - nairobi_lat).argmin()\nnearest_lon_idx = np.abs(lon - nairobi_lon).argmin()\n\ntemperature_nairobi = africa_dt['2t'][:, :, nearest_lat_idx, nearest_lon_idx] - 273.15\ntime_values = africa_dt.time.values\n\nplt.figure(figsize=(10, 6))\nplt.plot(time_values, temperature_nairobi, marker='o', color='b')\nplt.title('Air Temperature Time Series for Nairobi')\nplt.xlabel('Time')\nplt.ylabel('Temperature (°C)')\nplt.grid(True)\nplt.show()\n","type":"content","url":"/extremedt-datacube#time-series","position":19},{"hierarchy":{"lvl1":"STACK Service"},"type":"lvl1","url":"/readme-2","position":0},{"hierarchy":{"lvl1":"STACK Service"},"content":"","type":"content","url":"/readme-2","position":1},{"hierarchy":{"lvl1":"STACK Service"},"type":"lvl1","url":"/readme-2#stack-service","position":2},{"hierarchy":{"lvl1":"STACK Service"},"content":" Author: EUMETSAT\n\n\nMaterials to learn how to use STACK service and examples\n\nNotebook\n\nUse DASK: Use DASK in DestinE Data Lake\n\nAccess Extreme DT data in a cube\n\nFurther information available in DestinE Data Lake documentation: \n\nhttps://​destine​-data​-lake​-docs​.data​.destination​-earth​.eu​/en​/latest​/index​.html\n\nAdditional ressources:\nDestinE Data Portfolio: \n\nhttps://​hda​.data​.destination​-earth​.eu​/ui​/catalog\nDataLake Priority services: \n\nhttps://​hda​.data​.destination​-earth​.eu​/ui​/services\nHDA SWAGGER UI: \n\nhttps://​hda​.data​.destination​-earth​.eu​/docs/","type":"content","url":"/readme-2#stack-service","position":3},{"hierarchy":{"lvl1":"STACK service - Dask 101"},"type":"lvl1","url":"/stack-dask-101","position":0},{"hierarchy":{"lvl1":"STACK service - Dask 101"},"content":"\n\nLicence: MIT ","type":"content","url":"/stack-dask-101","position":1},{"hierarchy":{"lvl1":"STACK service - Dask 101"},"type":"lvl1","url":"/stack-dask-101#stack-service-dask-101","position":2},{"hierarchy":{"lvl1":"STACK service - Dask 101"},"content":"","type":"content","url":"/stack-dask-101#stack-service-dask-101","position":3},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl4":"Overview"},"type":"lvl4","url":"/stack-dask-101#overview","position":4},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl4":"Overview"},"content":"","type":"content","url":"/stack-dask-101#overview","position":5},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl5":"Content","lvl4":"Overview"},"type":"lvl5","url":"/stack-dask-101#content","position":6},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl5":"Content","lvl4":"Overview"},"content":"Dask API introduction\n\ndask.distributed\n\nDestinE DataLake Dask Cluster","type":"content","url":"/stack-dask-101#content","position":7},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl5":"Duration: 20 min.","lvl4":"Overview"},"type":"lvl5","url":"/stack-dask-101#duration-20-min","position":8},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl5":"Duration: 20 min.","lvl4":"Overview"},"content":"\n\nPlease make sure Python DEDL kernel is used, and run the mamba install command.","type":"content","url":"/stack-dask-101#duration-20-min","position":9},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"What is Dask?"},"type":"lvl2","url":"/stack-dask-101#what-is-dask","position":10},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"What is Dask?"},"content":"Dask is a Python library for parallel and distributed computing.\n\nDask addresses the challenge of scaling Python code from a single machine to large clusters of machines. In the world of data science and scientific computing, Python is a popular language due to its ease of use and extensive libraries like NumPy, Pandas, and scikit-learn. However, these libraries often struggle to handle large datasets that exceed the memory capacity of a single machine or require parallel processing for efficient computation.\n\nDask provides parallel computing capabilities and allows Python developers to work with larger-than-memory datasets by parallelizing computation across multiple cores within a single machine or distributing it across a cluster of machines. It achieves this by providing parallelized versions of familiar data structures like arrays, dataframes, and lists, which seamlessly integrate with existing Python code.\n\nDask can run on your laptop or can be scaled out to full capacity to a cloud cluster or HPC system.\n\nIt offeres a rich ecosystem of Python libraries to support use cases in multiple domains such as:\n\nGeospatial\n\nFinance\n\nAstrophysics\n\nMicrobiology\n\nEnvironmental science\n\nCheck out the Dask \n\nuse cases page that provides a number of sample workflows to see how others are using Dask to solve their problems.\n\nEscpecially the \n\nPangeo community, a community of people working to enable big data geoscience, heavily relies on Dask as the core compute frameworks to be used. Why? Pangeo community early realised the need for scaleable software and tools to handle petabyte-scale datasets on HPC and cloud platforms.\n\nDask provides multi-core and distributed+parallel execution on larger-than-memory datasets","type":"content","url":"/stack-dask-101#what-is-dask","position":11},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"References","lvl2":"What is Dask?"},"type":"lvl3","url":"/stack-dask-101#references","position":12},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"References","lvl2":"What is Dask?"},"content":"This tutorial builds upon the following reference documents supported and maintained by the Dask and Pangeo communities.\n\nDask Tutorial\n\nDask Examples\n\nPangeo Tutorials\n\n","type":"content","url":"/stack-dask-101#references","position":13},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask Core Library (APIs)"},"type":"lvl2","url":"/stack-dask-101#dask-core-library-apis","position":14},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask Core Library (APIs)"},"content":"Dask provides several APIs, also called collections, to enable distributed+parallel execution on larger-than-memory datasets.\nWe can think of Dask’s APIs at a high and a low level:\n\n\n\nHigh-level collections:  Dask provides high-level Array, Bag, and DataFrame\ncollections that mimic NumPy, lists, and pandas but can operate in parallel on\ndatasets that don’t fit into memory.\n\nLow-level collections:  Dask also provides low-level Tasks (Delayed and Futures)\ncollections that give you finer control to build custom parallel and distributed computations.\n\nIn this tutorial we will focus on Dask Arrays and Tasks (Delayed and Futures). Please visit the \n\nDask Examples and \n\nDask Tutorial for additional information.\n\n","type":"content","url":"/stack-dask-101#dask-core-library-apis","position":15},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"dask.array - parallelized numpy","lvl2":"Dask Core Library (APIs)"},"type":"lvl3","url":"/stack-dask-101#dask-array-parallelized-numpy","position":16},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"dask.array - parallelized numpy","lvl2":"Dask Core Library (APIs)"},"content":"Parallel, larger-than-memory, n-dimensional array using blocked algorithms.\n\nParallel: Uses all of the cores on your computer\n\nLarger-than-memory:  Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n\nBlocked Algorithms:  Perform large computations by performing many smaller computations.\n\n\n\nIn other words, Dask Array implements a subset of the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using all of our cores. We coordinate these blocked algorithms using Dask graphs.\n\nIn this notebook, we’ll build some understanding by implementing some blocked algorithms from scratch.\nWe’ll then use Dask Array to analyze large datasets, in parallel, using a familiar NumPy-like API.\n\nRelated Documentation\n\nArray documentation\n\nArray screencast\n\nArray API\n\nArray examples\n\n","type":"content","url":"/stack-dask-101#dask-array-parallelized-numpy","position":17},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl4":"Arrays - Example","lvl3":"dask.array - parallelized numpy","lvl2":"Dask Core Library (APIs)"},"type":"lvl4","url":"/stack-dask-101#arrays-example","position":18},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl4":"Arrays - Example","lvl3":"dask.array - parallelized numpy","lvl2":"Dask Core Library (APIs)"},"content":"\n\nA dask array looks and feels a lot like a numpy array. However, a dask array doesn’t directly hold any data. Instead, it symbolically represents the computations needed to generate the data. Nothing is actually computed until the actual numerical values are needed. This mode of operation is called “lazy”; it allows one to build up complex, large calculations symbolically before turning them over the scheduler for execution.\n\nIf we want to create a numpy array of all ones, we do it like this:\n\nimport numpy as np\nshape = (1000, 4000)\nones_np = np.ones(shape)\nones_np\n\nThis array contains exactly 32 MB of data:\n\nprint('%.1f MB' % (ones_np.nbytes / 1e6))\n\nNow let’s create the same array using dask’s array interface.\n\nimport dask.array as da\nones = da.ones(shape)\nones\n\nThis works, but we didn’t tell Dask how to split up the array, so it is not optimized for distributed computation.\n\nA crucial difference with Dask is that we must specify the chunks argument. “Chunks” describes how the array is split up over many sub-arrays.\n\nThere are several ways to \n\nspecify chunks.\n\nchunk_shape = (1000, 1000)\nones = da.ones(shape, chunks=chunk_shape)\nones\n\nNotice that we just see a symbolic representation of the array, including its shape, dtype, and chunksize. No data has been generated yet. When we call .compute() on a dask array, the computation is trigger and the dask array becomes a numpy array.\n\nones.compute()\n\nIn order to understand what happened when we called .compute(), we can visualize the Dask graph, the symbolic operations, that make up the array.\n\nones.visualize(format='svg')\n\nThe array has four chunks. To generate it, Dask calls np.ones four times and then concatenates this together into one array.\n\nRather than immediately loading a Dask array (which puts all the data into RAM), it is more common to reduce the data somehow. For example:\n\nsum_of_ones = ones.sum()\nsum_of_ones.visualize(format='svg')\n\nHere we see Dask’s strategy for finding the sum. This simple example illustrates the beauty of Dask: it automatically designs an algorithm appropriate for custom operations with big data.\n\nIf we make our operation more complex, the graph gets more complex.\n\nfancy_calculation = (ones * ones[::-1, ::-1]).mean()\nfancy_calculation.visualize(format='svg')\n\n","type":"content","url":"/stack-dask-101#arrays-example","position":19},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"dask.delayed - parallelize generic Python code","lvl2":"Dask Core Library (APIs)"},"type":"lvl3","url":"/stack-dask-101#dask-delayed-parallelize-generic-python-code","position":20},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"dask.delayed - parallelize generic Python code","lvl2":"Dask Core Library (APIs)"},"content":"What if you don’t have an Dask array or Dask dataframe? Instead of having blocks where the function is applied to each block, you can decorate functions with @delayed and have the functions themselves be lazy. Rather than compute its result immediately, it records what needs to be computed as a task into a graph that we’ll run later on parallel hardware.\n\nThis is a simple way to use Dask to parallelize existing codebases or build \n\ncomplex systems.\n\nRelated Documentation\n\nDelayed documentation\n\nDelayed screencast\n\nDelayed API\n\nDelayed examples\n\nDelayed best practices\n\nA typical workfow Read-Transform-Write workflow are most often implemented as outlined hereafter.\nIn general, most workflows containing a for-loop can benefit from dask.delayed.import dask\n    \n@dask.delayed\ndef process_file(filename):\n    data = read_a_file(filename)\n    data = do_a_transformation(data)\n    destination = f\"results/{filename}\"\n    write_out_data(data, destination)\n    return destination\n\nresults = []\nfor filename in filenames:\n    results.append(process_file(filename))\n    \ndask.compute(results)\n\n","type":"content","url":"/stack-dask-101#dask-delayed-parallelize-generic-python-code","position":21},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl5":"dask.delayed - Example","lvl3":"dask.delayed - parallelize generic Python code","lvl2":"Dask Core Library (APIs)"},"type":"lvl5","url":"/stack-dask-101#dask-delayed-example","position":22},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl5":"dask.delayed - Example","lvl3":"dask.delayed - parallelize generic Python code","lvl2":"Dask Core Library (APIs)"},"content":"For demonstration purposes we will create simple functions to perform simple operations like add two numbers together, but they sleep for a random amount of time to simulate real work.\n\nimport time\n\ndef inc(x):\n    time.sleep(0.1)\n    return x + 1\n\ndef dec(x):\n    time.sleep(0.1)\n    return x - 1\n\ndef add(x, y):\n    time.sleep(0.2)\n    return x + y\n\nWe can run them like normal Python functions below\n\n%%time\nx = inc(1)\ny = dec(2)\nz = add(x, y)\nz\n\nThese ran one after the other, in sequence. Note though that the first two lines inc(1) and dec(2) don’t depend on each other, we could have called them in parallel.\n\nWe can call dask.delayed on these funtions to make them lazy. Rather than compute their results immediately, they record what we want to compute as a task into a graph that we’ll run later on parallel hardware.\n\nimport dask\ninc = dask.delayed(inc)\ndec = dask.delayed(dec)\nadd = dask.delayed(add)\n\nCalling these lazy functions is now almost free. We’re just constructing a graph\n\n%%time\nx = inc(1)\ny = dec(2)\nz = add(x, y)\nz\n\nVisualize computation\n\nz.visualize(format='svg', rankdir='LR')\n\nRun in parallel. Call .compute() when you want your result as a normal Python object\n\n%%time\nz.compute()\n\nParallelize Normal Python code\n\nNow we use dask.delayed in a normal for-loop Python code as given in the example above. This generates graphs instead of doing computations directly, but still looks like the code we had before. Dask is a convenient way to add parallelism to existing workflows.\n\n%%time\nzs = []\nfor i in range(256):\n    x = inc(i)\n    y = dec(x)\n    z = add(x, y)\n    zs.append(z)\n\nzs = dask.persist(*zs)   # trigger computation in the background\n\n","type":"content","url":"/stack-dask-101#dask-delayed-example","position":23},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask Cluster (dask.distributed)"},"type":"lvl2","url":"/stack-dask-101#dask-cluster-dask-distributed","position":24},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask Cluster (dask.distributed)"},"content":"Dask has the ability to run work on multiple machines using the distributed scheduler. dask.distributed is a lightweight library for distributed computing in Python. It extends both the concurrent.futures and Dask APIs to run on various clusters technologies such as Kubernetes, Yarn, SLURM, PBS, etc. .\nMost of the times when you are using Dask, you will be using a distributed scheduler, which exists in the context of a Dask cluster. When we talk about Dask Clusters we can think of those as depicted in the following:\n\n\n\n","type":"content","url":"/stack-dask-101#dask-cluster-dask-distributed","position":25},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask @ DEDL"},"type":"lvl2","url":"/stack-dask-101#dask-dedl","position":26},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask @ DEDL"},"content":"DestinE Data Lake utilises a deployment of \n\nDask Gateway on each location (bridge) in the data lake. Dask Gateway provides a secure, multi-tenant server for managing Dask clusters. It allows users to launch and use Dask clusters in a shared, centrally managed cluster environment, without requiring users to have direct access to the underlying cluster backend (e.g. Kubernetes, Hadoop/YARN, HPC Job queues, etc…).\n\nDask Gateway exposes a REST API to spawn clusters on demand. The overall architecture of Dask Gateway is depicted hereafter.\n","type":"content","url":"/stack-dask-101#dask-dedl","position":27},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"How to connect and spawn a cluster?","lvl2":"Dask @ DEDL"},"type":"lvl3","url":"/stack-dask-101#how-to-connect-and-spawn-a-cluster","position":28},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"How to connect and spawn a cluster?","lvl2":"Dask @ DEDL"},"content":"Central Site\n\naddress: \n\nhttp://​dask​.central​.data​.destination​-earth​.eu\n\nproxy_address: tcp://dask.central.data.destination-earth.eu:80\n\nLUMI Bridge\n\naddress: \n\nhttp://​dask​.lumi​.data​.destination​-earth​.eu\n\nproxy_address: tcp://dask.lumi.data.destination-earth.eu:80\n\nfrom dask_gateway.auth import GatewayAuth\nfrom getpass import getpass\nfrom destinelab import AuthHandler as DESP_AuthHandler\n\nclass DESPAuth(GatewayAuth):\n    def __init__(self, username: str):\n        self.auth_handler = DESP_AuthHandler(username, getpass(\"Please input your DESP password: \"))\n        self.access_token = self.auth_handler.get_token()\n    \n    def pre_request(self, _):\n        headers = {\"Authorization\": \"Bearer \" + self.access_token}\n        return headers, None\n\nOnly authenticated access is granted to the DEDL STACK service Dask, therefore a helper class to authenticate a user against the DESP identity management system is implemented. The users password is directly handed over to the request object and is not permanently stored.\n\nIn the following, please enter your DESP username and password. Again, the password will only be saved for the duration of this user session and will be remove as soon as the notebook/kernel is closed.\n\nfrom rich.prompt import Prompt\nmyAuth = DESPAuth(username=Prompt.ask(prompt=\"Username\"))\n\nfrom dask_gateway import Gateway\ngateway = Gateway(address=\"http://dask.central.data.destination-earth.eu\",\n                  proxy_address=\"tcp://dask.central.data.destination-earth.eu:80\",\n                  auth=myAuth)\n\nCluster creation and client instantiation to communicate with the new cluster\n\ncluster = gateway.new_cluster()\nclient = cluster.get_client()\ncluster\n\nUp to now the cluster will only consist of the distributed scheduler. If you want to spawn workers directly via Python adaptively, please use the following method call. With the following the cluster will be scaled to 2 workers initially. Depending on the load, Dask will add addtional workers, up to 5, if needed.\n\ncluster.adapt(minimum=2, maximum=5)\n\n","type":"content","url":"/stack-dask-101#how-to-connect-and-spawn-a-cluster","position":29},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl4":"dask.futures - non-blocking distributed calculations","lvl3":"How to connect and spawn a cluster?","lvl2":"Dask @ DEDL"},"type":"lvl4","url":"/stack-dask-101#dask-futures-non-blocking-distributed-calculations","position":30},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl4":"dask.futures - non-blocking distributed calculations","lvl3":"How to connect and spawn a cluster?","lvl2":"Dask @ DEDL"},"content":"We will now make use of the remote Dask Cluster using the Dask low-level collection dask.futures.\n\nSubmit arbitrary functions for computation in a parallelized, eager, and non-blocking way.\n\nThe futures interface (derived from the built-in concurrent.futures) provide fine-grained real-time execution for custom situations. We can submit individual functions for evaluation with one set of inputs, or evaluated over a sequence of inputs with submit() and map(). The call returns immediately, giving one or more futures, whose status begins as “pending” and later becomes “finished”. There is no blocking of the local Python session.\n\nImportant\n\nThis is the important difference between futures and delayed. Both can be used to support arbitrary task scheduling, but delayed is lazy (it just constructs a graph) whereas futures are eager. With futures, as soon as the inputs are available and there is compute available, the computation starts.\n\nRelated Documentation\n\nFutures documentation\n\nFutures screencast\n\nFutures examples\n\nThis is the same workflow that as given above in the dask.delayed section. It is a for-loop to iterate of certain files to perform a transformation and to write the result.def process_file(filename):\n    data = read_a_file(filename)\n    data = do_a_transformation(data)\n    destination = f\"results/{filename}\"\n    write_out_data(data, destination)\n    return destination\n\nfutures = []\nfor filename in filenames:\n    future = client.submit(process_file, filename)\n    futures.append(future)\n    \nfutures\n\nfrom time import sleep\n\n\ndef inc(x):\n    sleep(1)\n    return x + 1\n\nWe can run these function locally\n\ninc(1)\n\nOr we can submit them to run remotely with Dask. This immediately returns a future that points to the ongoing computation, and eventually to the stored result.\n\nfuture = client.submit(inc, 1)  # returns immediately with pending future\nfuture\n\nIf you wait a second, and then check on the future again, you’ll see that it has finished.\n\nfuture\n\nYou can block on the computation and gather the result with the .result() method.\n\nfuture.result()\n\nOther ways to wait for a futurefrom dask.distributed import wait, progress\nprogress(future)\n\nshows a progress bar in the notebook. This progress bar is also asynchronous, and doesn’t block the execution of other code in the meanwhile.wait(future)\n\nblocks and forces the notebook to wait until the computation pointed to by future is done. However, note that if the result of inc() is sitting in the cluster, it would take no time to execute the computation now, because Dask notices that we are asking for the result of a computation it already knows about. More on this later.\n\nOther ways to gather resultsclient.gather(futures)\n\ngathers results from more than one future.\n\nfrom dask.distributed import wait, progress\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\nfuture_x = client.submit(inc, 1)\nfuture_y = client.submit(inc, 2)\nfuture_z = client.submit(sum, [future_x, future_y])\nprogress(future_z)\n\nRemove your cluster again to free up resources when you are done.\n\ncluster.close(shutdown=True)","type":"content","url":"/stack-dask-101#dask-futures-non-blocking-distributed-calculations","position":31},{"hierarchy":{"lvl1":"STACK service - Python Client Dask"},"type":"lvl1","url":"/stack-python-client-dask","position":0},{"hierarchy":{"lvl1":"STACK service - Python Client Dask"},"content":"\n\nLicence: MIT ","type":"content","url":"/stack-python-client-dask","position":1},{"hierarchy":{"lvl1":"STACK service - Python Client Dask"},"type":"lvl1","url":"/stack-python-client-dask#stack-service-python-client-dask","position":2},{"hierarchy":{"lvl1":"STACK service - Python Client Dask"},"content":"","type":"content","url":"/stack-python-client-dask#stack-service-python-client-dask","position":3},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask"},"type":"lvl1","url":"/stack-python-client-dask#multi-cloud-processing-with-dask","position":4},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask"},"content":"","type":"content","url":"/stack-python-client-dask#multi-cloud-processing-with-dask","position":5},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl3":"Overview"},"type":"lvl3","url":"/stack-python-client-dask#overview","position":6},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl3":"Overview"},"content":"","type":"content","url":"/stack-python-client-dask#overview","position":7},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl4":"Content","lvl3":"Overview"},"type":"lvl4","url":"/stack-python-client-dask#content","position":8},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl4":"Content","lvl3":"Overview"},"content":"DestinE Data Lake (DEDL) Stack Client\n\nMaking use of clients context manager\n\nUse Case: Pakistan Flood 2022","type":"content","url":"/stack-python-client-dask#content","position":9},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl4":"Duration: 15 min.","lvl3":"Overview"},"type":"lvl4","url":"/stack-python-client-dask#duration-15-min","position":10},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl4":"Duration: 15 min.","lvl3":"Overview"},"content":"\n\nPlease make sure Python DEDL kernel is used.\n\nDestinE Data Lake utilises a deployment of \n\nDask Gateway on each location (bridge) in the data lake. Dask Gateway provides a secure, multi-tenant server for managing Dask clusters. It allows users to launch and use Dask clusters in a shared, centrally managed cluster environment, without requiring users to have direct access to the underlying cluster backend (e.g. Kubernetes, Hadoop/YARN, HPC Job queues, etc…).\n\nDask Gateway exposes a REST API to spawn clusters on demand. The overall architecture of Dask Gateway is depicted hereafter.","type":"content","url":"/stack-python-client-dask#duration-15-min","position":11},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl2":"DEDL Dask Gateway"},"type":"lvl2","url":"/stack-python-client-dask#dedl-dask-gateway","position":12},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl2":"DEDL Dask Gateway"},"content":"Central Site\n\naddress: \n\nhttp://​dask​.central​.data​.destination​-earth​.eu\n\nproxy_address: tcp://dask.central.data.destination-earth.eu:80\n\nLUMI Bridge\n\naddress: \n\nhttp://​dask​.lumi​.data​.destination​-earth​.eu\n\nproxy_address: tcp://dask.lumi.data.destination-earth.eu:80\n\nOnly authenticated access is granted to the DEDL STACK service Dask, therefore a helper class to authenticate a user against the DESP identity management system is implemented. The users password is directly handed over to the request object and is not permanently stored.\n\nIn the following, please enter your DESP username and password. Again, the password will only be saved for the duration of this user session and will be remove as soon as the notebook/kernel is closed.\n\nfrom dask_gateway.auth import GatewayAuth\nfrom getpass import getpass\nfrom destinelab import AuthHandler as DESP_AuthHandler\n\nclass DESPAuth(GatewayAuth):\n    def __init__(self, username: str):\n        self.auth_handler = DESP_AuthHandler(username, getpass(\"Please input your DESP password: \"))\n        self.access_token = self.auth_handler.get_token()\n    \n    def pre_request(self, _):\n        headers = {\"Authorization\": \"Bearer \" + self.access_token}\n        return headers, None\n\nfrom rich.prompt import Prompt\nmyAuth = DESPAuth(username=Prompt.ask(prompt=\"Username\"))\n\n","type":"content","url":"/stack-python-client-dask#dedl-dask-gateway","position":13},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl2":"DestinE Data Lake (DEDL) Stack Client"},"type":"lvl2","url":"/stack-python-client-dask#destine-data-lake-dedl-stack-client","position":14},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl2":"DestinE Data Lake (DEDL) Stack Client"},"content":"The \n\nDEDL Stack Client is a Python library to facilitate the use of Stack Service Dask. The main objective is to provide an abstraction layer to interact with the various clusters on each DEDL bridge. Computations can be directed to the different Dask clusters by making use of a context manager as given in the following.\n\nfrom dedl_stack_client.dask import DaskMultiCluster\n\nmyDEDLClusters = DaskMultiCluster(auth=myAuth)\nmyDEDLClusters.new_cluster()\n\nmyDEDLClusters.get_cluster_url()\n\nWe can again showcase the execution of standard Python functions on the remote clusters.\nIn the following we will make use of dask.futures, non-blocking distributed calculations, utilising the map() method for task distribution. Detailed information about dask.futures can be found on the \n\nDask documention.\n\nThis approach allows for embarrassingly parallel task scheduling, which is very similar to Function as a Service capabilities.\n\nfrom time import sleep as wait\n\ndef apply_myfunc(x):\n    wait(1)\n    return x+1\n\nWe want to run apply_myfunc() on Central Site and wait for all results to be ready. my_filelist_central represents a filelist to be processed by apply_myfunc().\n\nmy_filelist_central = range(20)\nwith myDEDLClusters.as_current(location=\"central\") as myclient:\n    central_future = myclient.map(apply_myfunc, my_filelist_central)\n    results_central = myclient.gather(central_future)\n\nresults_central\n\nRun computation at LUMI bridge.\n\nmy_filelist_lumi = range(32)\nwith myDEDLClusters.as_current(location=\"lumi\") as myclient:\n    lumi_future = myclient.map(apply_myfunc, my_filelist_lumi)\n    results_lumi = myclient.gather(lumi_future)\n\nresults_lumi\n\n","type":"content","url":"/stack-python-client-dask#destine-data-lake-dedl-stack-client","position":15},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl2":"Limitations"},"type":"lvl2","url":"/stack-python-client-dask#limitations","position":16},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl2":"Limitations"},"content":"Python libraries use in the local environment need to match, same version, with those available in the Dask Cluster. If this is not the case, you will get a warning, code might work but not guaranteed.\n\nNo direct data exchange between Dask Workers across cloud locations possible. Each location acts as atmoic unit, however data can be easily exchanged via storage services such as S3.\n\n","type":"content","url":"/stack-python-client-dask#limitations","position":17},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl2":"Use Case example: Pakistan Flood 2022"},"type":"lvl2","url":"/stack-python-client-dask#use-case-example-pakistan-flood-2022","position":18},{"hierarchy":{"lvl1":"Multi-cloud processing with Dask","lvl2":"Use Case example: Pakistan Flood 2022"},"content":"The complete use case is available on GitHub via \n\nhttps://​github​.com​/destination​-earth​/DestinE​_EUMETSAT​_PakistanFlood​_2022.\n\nThe use case demonstrates the multi-cloud capabilities of DEDL following the paradigm of data proximate computing. Data of the Global Flood Monitoring (GFM) service as well as Climate DT outputs, simulated by utilising ERA5 data have been use for flood risk assessment.\n\nData is stored as datacubes (zarr format) at Central Site and at LUMI bridge in object storage.\n\nimport s3fs\nimport xarray as xr\n\nxr.set_options(keep_attrs=True)\n\ns3fs_central = s3fs.S3FileSystem(\n    anon=True,\n    use_ssl=True,\n    client_kwargs={\"endpoint_url\": \"https://s3.central.data.destination-earth.eu\"})\n\ns3fs_lumi = s3fs.S3FileSystem(\n    anon=True,\n    use_ssl=True,\n    client_kwargs={\"endpoint_url\": \"https://s3.lumi.data.destination-earth.eu\"})\n\nWe can list the data available at Central Site.\n\ns3fs_central.ls(\"increment1-testdata\")\n\nRead data stored in S3 bucket at Central Site. The data we want to read is a single Zarr data store representing the GFM flood data over Pakistan for 2022-08-30.\n\nflood_map = xr.open_zarr(store=s3fs.S3Map(root=f\"increment1-testdata/2022-08-30.zarr\", s3=s3fs_central, check=False),\n                         decode_coords=\"all\",)[\"flood\"].assign_attrs(location=\"central\", resolution=20)\n#flood_map\n\nWe now want to run simple computation and compute the flooded area for the this day in August 2022.\n\nflooded_area_ = flood_map.sum()*20*20/1000.\n#flooded_area_\n\nSo far we haven’t computed anything, so lets do the computation now on the Dask cluster.\n\nfrom rich.console import Console\nfrom rich.prompt import Prompt\nconsole = Console()\n\nflooded_area = myDEDLClusters.compute(flooded_area_, sync=True)\nconsole.print(f\"Flooded area: {flooded_area.data} km2\")\n\nHow was that processing routed to Dask Gateway at Central Site?\n\nmyDEDLClusters.compute(flooded_area_, sync=True) checks for annotations (attributes) of array and maps that to available Dask Clusters.\n\nPreprocess GFM data at Central Site for visualiation\n\ndef preprocess_dataset(data_array: xr.DataArray, method: str):\n    data_array = data_array.squeeze()\n    steps = 500 // data_array.attrs[\"resolution\"]\n    coarsened = data_array.coarsen({'y': steps, 'x': steps}, boundary='trim')\n    if method == 'median':\n        data_array = (coarsened.median() > 0).astype('float32')\n    elif method == 'mean':\n        data_array = coarsened.mean()\n    elif method == 'max':\n        data_array = coarsened.max()\n    else:\n        raise NotImplementedError(method)\n    return data_array\n\nflood_prep_ = preprocess_dataset(flood_map, 'median')\n\nimport numpy as np\nflood_prep = myDEDLClusters.compute(flood_prep_, sync=True)\nflood_prep.rio.write_crs(\"epsg:4326\", inplace=True)\nflood_prep = flood_prep.rio.reproject(f\"EPSG:3857\", nodata=np.nan)\n\nVisualise flood data on map.\n\nimport leafmap\nfrom attr import dataclass\n\n@dataclass\nclass Extent:\n    min_x: float\n    min_y: float\n    max_x: float\n    max_y: float\n    crs: str\n    def get_center(self):\n        return (np.mean([self.min_y, self.max_y]),\n                np.mean([self.min_x,self.max_x]))\n\n\nroi_extent = Extent(65, 21, 71, 31, crs='EPSG:4326')\n\nm = leafmap.Map(center=roi_extent.get_center(),\n                zoom=8, height=600)\nm.add_raster(flood_prep, colormap=\"Blues\", layer_name=\"Flood\", nodata=0.)\n\nm\n\nRead data stored in S3 bucket at LUMI bridge (Finland). Data we want to read is a datacube generated from ERA-5 representing predicted rainfall data.\n\nrainfall = xr.open_zarr(store=s3fs.S3Map(root=f\"increment1-testdata/predicted_rainfall.zarr\",\n                                         s3=s3fs_lumi,\n                                         check=False),\n                        decode_coords=\"all\",)[\"tp\"].assign_attrs(location=\"lumi\", resolution=20)\n\nAnd again run the computation close to the data, therefore at LUMI bridge.\n\nFirst we compute the accumulated rainfall over Pakistan.\nSecondly we compute the average rainfall for August 2022 (monthly mean) at global scale.\n\nfrom datetime import datetime\n\ndef accum_rain_predictions(rain_data, startdate, enddate, extent):\n    rain_ = rain_data.sel(time=slice(startdate, enddate),\n                          latitude=slice(extent.max_y, extent.min_y),\n                          longitude=slice(extent.min_x, extent.max_x))\n    return rain_.cumsum(dim=\"time\", keep_attrs=True)*1000\n\n# compute accumulated rainfall over Pakistan\nacc_rain_ = accum_rain_predictions(rainfall, startdate=datetime(2022, 8, 18),\n                                                  enddate=datetime(2022, 8, 30),\n                                                  extent=roi_extent)\nacc_rain_ = acc_rain_.rename({\"longitude\":\"x\", \"latitude\":\"y\"})\n\nacc_rain = myDEDLClusters.compute(acc_rain_, sync=True)\n\ndef acc_rain_reproject(rain):\n    from rasterio.enums import Resampling\n    rain.rio.write_nodata(0, inplace=True)\n    rain.rio.write_crs('EPSG:4326', inplace=True)\n    return rain.rio.reproject('EPSG:3857', resolution=500, resampling=Resampling.bilinear)\n\nacc_rain = acc_rain_reproject(acc_rain)\n\nVisualise forecast data provided by the Digital Twin which could have been used for flood risk assessment or even alerting.\n\ntime_dim_len = acc_rain.shape[0]\nfor day in range(0, time_dim_len):\n    fpath_str = f\"./{day}.tif\"\n    acc_rain[day,:].rio.to_raster(fpath_str,\n                                  driver=\"COG\",\n                                  overview_count=10)\n\nimport leafmap\nfrom localtileserver import get_leaflet_tile_layer\n\nm = leafmap.Map(center=roi_extent.get_center(),\n                zoom=6, height=600)\n\nlayer_dict = {}\ndate_vals = np.datetime_as_string(acc_rain[\"time\"].values, unit='D')\nfor day in range(0, time_dim_len):\n    layer_dict[date_vals[day]]= get_leaflet_tile_layer(f\"./{day}.tif\",\n                                                       colormap=\"Blues\",\n                                                       indexes=[1],\n                                                       nodata=0.,\n                                                       vmin=acc_rain.min().values,\n                                                       vmax=acc_rain.max().values,\n                                                       opacity=0.85)\n\nm.add_local_tile(flood_prep,\n                 colormap=\"Blues\",\n                 nodata=0.)\nm.add_time_slider(layer_dict,\n                  layer=\"Accumluated Rainfall\",\n                  time_interval=1.)\nm\n\nmyDEDLClusters.shutdown()","type":"content","url":"/stack-python-client-dask#use-case-example-pakistan-flood-2022","position":19},{"hierarchy":{"lvl1":"STACK Gallery"},"type":"lvl1","url":"/gallery-2","position":0},{"hierarchy":{"lvl1":"STACK Gallery"},"content":"","type":"content","url":"/gallery-2","position":1}]}